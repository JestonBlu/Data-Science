<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science Tasks</title>
  <meta name="description" content="Data Science and Statistical Analysis I want to remember">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science Tasks" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/jestonblu/datascience" />
  
  <meta property="og:description" content="Data Science and Statistical Analysis I want to remember" />
  <meta name="github-repo" content="jestonblu/datascience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science Tasks" />
  
  <meta name="twitter:description" content="Data Science and Statistical Analysis I want to remember" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="04-Forecasting.html">
<link rel="next" href="06-Simulation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Science Tasks</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html"><i class="fa fa-check"></i>Statistical Methods</a><ul>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#fitting-distributions"><i class="fa fa-check"></i>Fitting Distributions</a><ul>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#continuous-distributions"><i class="fa fa-check"></i>Continuous Distributions</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#discrete-distributions"><i class="fa fa-check"></i>Discrete Distributions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#hypothesis-testing"><i class="fa fa-check"></i>Hypothesis Testing</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#parameter-estimation"><i class="fa fa-check"></i>Parameter Estimation</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#sample-size-and-power"><i class="fa fa-check"></i>Sample Size and Power</a><ul>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#proportions"><i class="fa fa-check"></i>Proportions</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#t-test"><i class="fa fa-check"></i>T-test</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#chi-square"><i class="fa fa-check"></i>Chi-square</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#anova"><i class="fa fa-check"></i>ANOVA</a></li>
</ul></li>
<li><a href="01-Statistical-Methods.html#survival-analysis"><strong>Survival Analysis</strong></a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#experimental-design"><i class="fa fa-check"></i>Experimental Design</a><ul>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#completely-random-design"><i class="fa fa-check"></i>Completely Random Design</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#random-complete-block-design"><i class="fa fa-check"></i>Random Complete Block Design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#contingency-tables"><i class="fa fa-check"></i>Contingency Tables</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#principal-components"><i class="fa fa-check"></i>Principal Components</a></li>
<li class="chapter" data-level="" data-path="01-Statistical-Methods.html"><a href="01-Statistical-Methods.html#eigen-values-and-statistical-distance"><i class="fa fa-check"></i>Eigen Values and Statistical Distance</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="02-Regression.html"><a href="02-Regression.html"><i class="fa fa-check"></i>Regression Methods</a><ul>
<li class="chapter" data-level="" data-path="02-Regression.html"><a href="02-Regression.html#matrix-regression"><i class="fa fa-check"></i>Matrix Regression</a></li>
<li class="chapter" data-level="" data-path="02-Regression.html"><a href="02-Regression.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="02-Regression.html"><a href="02-Regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i>Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="02-Regression.html"><a href="02-Regression.html#poisson-regression"><i class="fa fa-check"></i>Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="03-Clustering.html"><a href="03-Clustering.html"><i class="fa fa-check"></i>Clustering Methods</a><ul>
<li class="chapter" data-level="" data-path="03-Clustering.html"><a href="03-Clustering.html#kmeans-clustering"><i class="fa fa-check"></i>Kmeans Clustering</a></li>
<li class="chapter" data-level="" data-path="03-Clustering.html"><a href="03-Clustering.html#hierarchical-clustering"><i class="fa fa-check"></i>Hierarchical Clustering</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="04-Forecasting.html"><a href="04-Forecasting.html"><i class="fa fa-check"></i>Forecasting</a><ul>
<li class="chapter" data-level="" data-path="04-Forecasting.html"><a href="04-Forecasting.html#basic-graphical-methods-for-time-series"><i class="fa fa-check"></i>Basic Graphical Methods for Time Series</a></li>
<li class="chapter" data-level="" data-path="04-Forecasting.html"><a href="04-Forecasting.html#generate-time-series-data"><i class="fa fa-check"></i>Generate Time Series Data</a><ul>
<li class="chapter" data-level="0.0.1" data-path="04-Forecasting.html"><a href="04-Forecasting.html#manually-generated-series"><i class="fa fa-check"></i><b>0.0.1</b> Manually Generated Series</a></li>
<li class="chapter" data-level="" data-path="04-Forecasting.html"><a href="04-Forecasting.html#auto-generated-series"><i class="fa fa-check"></i>Auto Generated Series</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="05-Machine-Learning.html"><a href="05-Machine-Learning.html"><i class="fa fa-check"></i>Machine Learning</a><ul>
<li class="chapter" data-level="" data-path="05-Machine-Learning.html"><a href="05-Machine-Learning.html#basic-random-forest"><i class="fa fa-check"></i>Basic Random Forest</a></li>
<li class="chapter" data-level="" data-path="05-Machine-Learning.html"><a href="05-Machine-Learning.html#gradient-boosting"><i class="fa fa-check"></i>Gradient Boosting</a></li>
<li class="chapter" data-level="" data-path="05-Machine-Learning.html"><a href="05-Machine-Learning.html#ensemble-model"><i class="fa fa-check"></i>Ensemble Model</a></li>
<li class="chapter" data-level="" data-path="05-Machine-Learning.html"><a href="05-Machine-Learning.html#overfitting"><i class="fa fa-check"></i>Overfitting</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="06-Simulation.html"><a href="06-Simulation.html"><i class="fa fa-check"></i>Simulation</a><ul>
<li class="chapter" data-level="" data-path="06-Simulation.html"><a href="06-Simulation.html#bootstrap-simulation"><i class="fa fa-check"></i>Bootstrap Simulation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="07-Visualization.html"><a href="07-Visualization.html"><i class="fa fa-check"></i>Visualization</a><ul>
<li class="chapter" data-level="" data-path="07-Visualization.html"><a href="07-Visualization.html#building-maps-with-choropleth-and-ggplot"><i class="fa fa-check"></i>Building Maps with choropleth and ggplot</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Tasks</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1 unnumbered">
<h1>Machine Learning</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Required Packages
<span class="kw">library</span>(neuralnet)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(nnet)
<span class="kw">library</span>(gridExtra)

## Set up parameters for the NN
n =<span class="st"> </span><span class="dv">10000</span>
size =<span class="st"> </span><span class="dv">50</span>
maxit =<span class="st"> </span><span class="dv">500</span>

## Generate Random Data fron the Uniform Distribution
<span class="kw">set.seed</span>(<span class="dv">10</span>)
x =<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min =</span> <span class="op">-</span><span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">1</span>)
y =<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min =</span> <span class="op">-</span><span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">1</span>)
dt =<span class="st"> </span><span class="kw">data.frame</span>(x, y)

## Quick Plot of Data
<span class="kw">qplot</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="dt">data =</span> dt, <span class="dt">main =</span> <span class="st">&quot;Data Set&quot;</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-1.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Calculate the distance to the origin
dt<span class="op">$</span>dto =<span class="st"> </span><span class="kw">with</span>(dt, <span class="kw">sqrt</span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>))

## Create a bullseye basid on distance to origin
dt<span class="op">$</span>code =<span class="st"> </span><span class="dv">0</span>
dt<span class="op">$</span>code[dt<span class="op">$</span>dto <span class="op">&lt;</span><span class="st"> </span>.<span class="dv">8</span>] =<span class="st"> </span><span class="dv">1</span>
dt<span class="op">$</span>code[dt<span class="op">$</span>dto <span class="op">&lt;</span><span class="st"> </span>.<span class="dv">5</span>] =<span class="st"> </span><span class="dv">0</span>
dt<span class="op">$</span>code[dt<span class="op">$</span>dto <span class="op">&lt;</span><span class="st"> </span>.<span class="dv">15</span>] =<span class="st"> </span><span class="dv">1</span>

## Code the indicator
dt<span class="op">$</span>code2 =<span class="st"> </span><span class="kw">class.ind</span>(dt<span class="op">$</span>code)



## Create a custom plot theme
theme_jeb =<span class="st"> </span><span class="cf">function</span>() {
  <span class="kw">theme</span>(<span class="dt">panel.background =</span> <span class="kw">element_blank</span>(),
        <span class="dt">panel.border =</span> <span class="kw">element_rect</span>(<span class="dt">color =</span> <span class="st">&quot;gray&quot;</span>, <span class="dt">fill =</span> <span class="ot">NA</span>),
        <span class="dt">panel.grid =</span> <span class="kw">element_blank</span>(),
        <span class="dt">axis.text =</span> <span class="kw">element_blank</span>(),
        <span class="dt">axis.ticks =</span> <span class="kw">element_blank</span>(),
        <span class="dt">axis.title =</span> <span class="kw">element_blank</span>(),
        <span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">face =</span> <span class="st">&quot;bold&quot;</span>))
}

## Main plot of the testing set
g1 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Coded Data Set&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g1</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-2.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Train the NN from 50 random samples of the data set
samp =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dv">50</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)
mdl =<span class="st"> </span><span class="kw">nnet</span>(code2 <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y, <span class="dt">data =</span> dt[samp,], <span class="dt">size =</span> size, <span class="dt">softmax =</span> <span class="ot">TRUE</span>,
           <span class="dt">maxit =</span> maxit, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
dt<span class="op">$</span>out =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">predict</span>(mdl, dt, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>))


g2 =<span class="st"> </span><span class="kw">ggplot</span>(dt[samp, ], <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Training Set N=50&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g3 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> out)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Model: N=50,&quot;</span>, <span class="st">&quot;Error=&quot;</span>,
                <span class="kw">length</span>(<span class="kw">which</span>(<span class="kw">as.numeric</span>(dt<span class="op">$</span>out)<span class="op">-</span><span class="kw">as.numeric</span>(dt<span class="op">$</span>code) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))<span class="op">/</span>n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

<span class="kw">grid.arrange</span>(g2, g3, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-3.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## 100 random samples
samp =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)
mdl =<span class="st"> </span><span class="kw">nnet</span>(code2 <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y, <span class="dt">data =</span> dt[samp,], <span class="dt">size =</span> size, <span class="dt">softmax =</span> <span class="ot">TRUE</span>,
           <span class="dt">maxit =</span> maxit, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
dt<span class="op">$</span>out.<span class="dv">100</span> =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">predict</span>(mdl, dt, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>))

g4 =<span class="st"> </span><span class="kw">ggplot</span>(dt[samp, ], <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Training Set N=100&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g5 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> out.<span class="dv">100</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Model: N=100,&quot;</span>, <span class="st">&quot;Error=&quot;</span>,
                <span class="kw">length</span>(<span class="kw">which</span>(<span class="kw">as.numeric</span>(dt<span class="op">$</span>out.<span class="dv">100</span>)<span class="op">-</span><span class="kw">as.numeric</span>(dt<span class="op">$</span>code) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))<span class="op">/</span>n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

<span class="kw">grid.arrange</span>(g4, g5, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-4.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## 500 random samples
samp =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dv">500</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)
mdl =<span class="st"> </span><span class="kw">nnet</span>(code2 <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y, <span class="dt">data =</span> dt[samp,], <span class="dt">size =</span> size, <span class="dt">softmax =</span> <span class="ot">TRUE</span>,
           <span class="dt">maxit =</span> maxit, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
dt<span class="op">$</span>out.<span class="dv">500</span> =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">predict</span>(mdl, dt, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>))

g6 =<span class="st"> </span><span class="kw">ggplot</span>(dt[samp, ], <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Training Set N=500&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g7 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> out.<span class="dv">500</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Model: N=500,&quot;</span>, <span class="st">&quot;Error=&quot;</span>,
                <span class="kw">length</span>(<span class="kw">which</span>(<span class="kw">as.numeric</span>(dt<span class="op">$</span>out.<span class="dv">500</span>)<span class="op">-</span><span class="kw">as.numeric</span>(dt<span class="op">$</span>code) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))<span class="op">/</span>n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

<span class="kw">grid.arrange</span>(g6, g7, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-5.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## 1000 random samples
samp =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dv">1000</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)
mdl =<span class="st"> </span><span class="kw">nnet</span>(code2 <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y, <span class="dt">data =</span> dt[samp,], <span class="dt">size =</span> size, <span class="dt">softmax =</span> <span class="ot">TRUE</span>,
           <span class="dt">maxit =</span> maxit, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
dt<span class="op">$</span>out.<span class="dv">1000</span> =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">predict</span>(mdl, dt, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>))

g8 =<span class="st"> </span><span class="kw">ggplot</span>(dt[samp, ], <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Training Set N=1000&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g9 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> out.<span class="dv">1000</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Model: N=1000,&quot;</span>, <span class="st">&quot;Error=&quot;</span>,
                <span class="kw">length</span>(<span class="kw">which</span>(<span class="kw">as.numeric</span>(dt<span class="op">$</span>out.<span class="dv">1000</span>)<span class="op">-</span><span class="kw">as.numeric</span>(dt<span class="op">$</span>code) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))<span class="op">/</span>n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

<span class="kw">grid.arrange</span>(g8, g9, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-6.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## 2500 random samples
samp =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dv">2500</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)
mdl =<span class="st"> </span><span class="kw">nnet</span>(code2 <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y, <span class="dt">data =</span> dt[samp,], <span class="dt">size =</span> size, <span class="dt">softmax =</span> <span class="ot">TRUE</span>,
           <span class="dt">maxit =</span> maxit, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
dt<span class="op">$</span>out.<span class="dv">2500</span> =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">predict</span>(mdl, dt, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>))

g10 =<span class="st"> </span><span class="kw">ggplot</span>(dt[samp, ], <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Training Set N=2500&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g11 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> out.<span class="dv">2500</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Model: N=2500,&quot;</span>, <span class="st">&quot;Error=&quot;</span>,
                <span class="kw">length</span>(<span class="kw">which</span>(<span class="kw">as.numeric</span>(dt<span class="op">$</span>out.<span class="dv">2500</span>)<span class="op">-</span><span class="kw">as.numeric</span>(dt<span class="op">$</span>code) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))<span class="op">/</span>n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

<span class="kw">grid.arrange</span>(g10, g11, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-7.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## 5000 random samples
samp =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dv">5000</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)
mdl =<span class="st"> </span><span class="kw">nnet</span>(code2 <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y, <span class="dt">data =</span> dt[samp,], <span class="dt">size =</span> size, <span class="dt">softmax =</span> <span class="ot">TRUE</span>,
           <span class="dt">maxit =</span> maxit, <span class="dt">trace =</span> <span class="ot">FALSE</span>)
dt<span class="op">$</span>out.<span class="dv">5000</span> =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">predict</span>(mdl, dt, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>))

g12 =<span class="st"> </span><span class="kw">ggplot</span>(dt[samp, ], <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> <span class="kw">factor</span>(code))) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Training Set N=5000&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

g13 =<span class="st"> </span><span class="kw">ggplot</span>(dt, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">col =</span> out.<span class="dv">5000</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="kw">paste</span>(<span class="st">&quot;Model: N=5000,&quot;</span>, <span class="st">&quot;Error=&quot;</span>,
                <span class="kw">length</span>(<span class="kw">which</span>(<span class="kw">as.numeric</span>(dt<span class="op">$</span>out.<span class="dv">5000</span>)<span class="op">-</span><span class="kw">as.numeric</span>(dt<span class="op">$</span>code) <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))<span class="op">/</span>n)) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme_jeb</span>()

<span class="kw">grid.arrange</span>(g12, g13, <span class="dt">nrow =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/a1-8.png" width="864" /></p>
<div id="basic-random-forest" class="section level2 unnumbered">
<h2>Basic Random Forest</h2>
<hr />
<div id="background" class="section level5 unnumbered">
<h5>Background</h5>
<p>The following dataset are career stats for over 1000 MLB baseball players. The data consists of the position of each player along with 19 numeric variables measure offense. The training set consists of 677 observations and the testing set has 339 observations. The objective is to build a model that will predict whether a player is in the Hall of Fame based on his career statistics. Since so few players make it to the Hall of Fame, the methodology for scoring the accuracy of models is based on the following calculation: <span class="math inline">\((sensitivity + 3*specificity) / 4\)</span>. The objective is to get as few incorrect predictions as possible, but having fewer false positives will affect the accuracy measure more than false negatives.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## required packages
<span class="kw">library</span>(randomForest)

## Training and Testing Data
hof.train =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/HOF_tr.csv&quot;</span>); hof.test =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/HOF_te.csv&quot;</span>)

## remove unwanted columns
hof.train =<span class="st"> </span>hof.train[, <span class="op">-</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>)]; hof.test =<span class="st"> </span>hof.test[, <span class="op">-</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>)]

<span class="kw">head</span>(hof.train)</code></pre></div>
<pre><code>  HOF POS  ASG    G    AB    R    H  DB  TP  HR  RBI  SB  CS   BB   SO AVG
1   Y  OF 0.75 2440  9288 1383 3141 543  85 135 1138 319 125  790  434 338
2   Y  SS 0.56 2601 10230 1335 2677 394  92  83  791 506 136  736  742 262
3   Y  OF 0.83 1783  7244 1071 2304 414  57 207 1085 134  76  450  965 318
4   Y  1B 0.38 3026 11336 1627 3255 560  35 504 1917 110  43 1333 1516 287
5   Y  1B 0.95 2469  9315 1424 3053 445 112  92 1015 353 187 1018 1028 328
6   Y  1B 0.41 2124  7927 1131 2351 417  27 379 1365 142  80  588 1169 297
  SLG OBP
1 459 388
2 343 311
3 477 360
4 476 359
5 429 393
6 499 350</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(hof.train)</code></pre></div>
<pre><code> HOF     POS           ASG               G              AB       
 N:644   1B: 77   Min.   :0.0000   Min.   : 253   Min.   :  559  
 Y: 33   2B: 83   1st Qu.:0.0000   1st Qu.: 924   1st Qu.: 2626  
         3B: 71   Median :0.0000   Median :1294   Median : 4101  
         C :122   Mean   :0.1022   Mean   :1336   Mean   : 4323  
         OF:244   3rd Qu.:0.1500   3rd Qu.:1666   3rd Qu.: 5536  
         SS: 80   Max.   :0.9500   Max.   :3081   Max.   :11551  
       R                H              DB            TP        
 Min.   :  37.0   Min.   :  90   Min.   : 14   Min.   :  0.00  
 1st Qu.: 308.0   1st Qu.: 656   1st Qu.:109   1st Qu.: 12.00  
 Median : 522.0   Median :1064   Median :183   Median : 24.00  
 Mean   : 574.9   Mean   :1157   Mean   :199   Mean   : 30.19  
 3rd Qu.: 760.0   3rd Qu.:1502   3rd Qu.:260   3rd Qu.: 41.00  
 Max.   :2295.0   Max.   :3283   Max.   :668   Max.   :166.00  
       HR             RBI               SB                CS        
 Min.   :  1.0   Min.   :  44.0   Min.   :   0.00   Min.   :  0.00  
 1st Qu.: 34.0   1st Qu.: 268.0   1st Qu.:  14.00   1st Qu.: 15.00  
 Median : 80.0   Median : 443.0   Median :  42.00   Median : 31.00  
 Mean   :113.9   Mean   : 538.1   Mean   :  84.64   Mean   : 42.07  
 3rd Qu.:155.0   3rd Qu.: 708.0   3rd Qu.:  98.00   3rd Qu.: 57.00  
 Max.   :660.0   Max.   :1917.0   Max.   :1406.00   Max.   :335.00  
       BB             SO            AVG             SLG       
 Min.   :  29   Min.   :  71   Min.   :161.0   Min.   :222.0  
 1st Qu.: 221   1st Qu.: 362   1st Qu.:249.0   1st Qu.:350.0  
 Median : 364   Median : 565   Median :261.0   Median :392.0  
 Mean   : 436   Mean   : 643   Mean   :261.5   Mean   :392.7  
 3rd Qu.: 583   3rd Qu.: 842   3rd Qu.:273.0   3rd Qu.:433.0  
 Max.   :2190   Max.   :2597   Max.   :338.0   Max.   :557.0  
      OBP       
 Min.   :203.0  
 1st Qu.:311.0  
 Median :328.0  
 Mean   :328.8  
 3rd Qu.:347.0  
 Max.   :415.0  </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Simple Random Forest
(<span class="dt">mdl =</span> <span class="kw">randomForest</span>(HOF <span class="op">~</span><span class="st"> </span>., <span class="dt">ntree =</span> <span class="dv">1000</span>, <span class="dt">data =</span> hof.train))</code></pre></div>
<pre><code>
Call:
 randomForest(formula = HOF ~ ., data = hof.train, ntree = 1000) 
               Type of random forest: classification
                     Number of trees: 1000
No. of variables tried at each split: 4

        OOB estimate of  error rate: 1.62%
Confusion matrix:
    N  Y class.error
N 640  4  0.00621118
Y   7 26  0.21212121</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Function for testing accuracy
metric =<span class="st"> </span><span class="cf">function</span>(confusion) {
  sensitivity =<span class="st"> </span>confusion[<span class="dv">4</span>] <span class="op">/</span><span class="st"> </span>(confusion[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>confusion[<span class="dv">4</span>])
  specificity =<span class="st"> </span>confusion[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>(confusion[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>confusion[<span class="dv">3</span>])
  score =<span class="st"> </span>(sensitivity <span class="op">+</span><span class="st"> </span>(<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>specificity)) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>
  <span class="kw">return</span>(score)
}

## Plot of the model performance
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>, <span class="dt">y =</span> mdl<span class="op">$</span>err.rate[,<span class="dv">1</span>], <span class="dt">xlab =</span> <span class="st">&quot;Trees&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Error&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Out of Sample Error Rate&quot;</span>)
<span class="kw">varImpPlot</span>(mdl, <span class="dt">main =</span> <span class="st">&quot;Variable Importance Plot&quot;</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/b1-1.png" width="864" /></p>
</div>
<div id="testing-model-accuracy-on-new-data" class="section level5 unnumbered">
<h5>Testing Model Accuracy on New Data</h5>
<p>Now that we have a trained model, we will apply the model to data that was not used in the training set. We will calculate the same accuracy score and compare the two. If they are wildly different we may have a problem with overfitting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## predict the probability of HOF
estimate =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(mdl, hof.test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))
estimate<span class="op">$</span>predict =<span class="st"> </span><span class="kw">predict</span>(mdl, hof.test)
estimate<span class="op">$</span>actual =<span class="st"> </span>hof.test<span class="op">$</span>HOF

## Generate a confusion matrix
(<span class="dt">confusion =</span> <span class="kw">table</span>(estimate[, <span class="dv">3</span><span class="op">:</span><span class="dv">4</span>]))</code></pre></div>
<pre><code>       actual
predict   N   Y
      N 322   2
      Y   1  14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Final Accuracy Measure
(<span class="dt">test.metric =</span> <span class="kw">metric</span>(confusion))</code></pre></div>
<pre><code>[1] 0.9787037</code></pre>
<p>The random forest method is fairly robust to overfitting because it reserves some of the training data to use as test data which is called Out of Bag (OOB error). Because of this internal mechanism we could probably ues a larger portion of the overall data to train. The next sections tests this to see if accuracy is improved.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hof =<span class="st"> </span><span class="kw">rbind</span>(hof.train, hof.test)

## create a training and testing set by randomly sampling from all of the data
<span class="kw">set.seed</span>(<span class="dv">1002</span>)
x =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(hof), <span class="dt">replace =</span> <span class="ot">FALSE</span>)

## lets train the model on about 90% of the data
train =<span class="st"> </span>hof[x[<span class="dv">1</span><span class="op">:</span><span class="dv">900</span>], ]
test =<span class="st"> </span>hof[<span class="op">-</span>x[<span class="dv">1</span><span class="op">:</span><span class="dv">900</span>], ]

## build the model
(<span class="dt">mdl =</span> <span class="kw">randomForest</span>(HOF <span class="op">~</span><span class="st"> </span>., <span class="dt">ntree =</span> <span class="dv">1000</span>, <span class="dt">data =</span> train))</code></pre></div>
<pre><code>
Call:
 randomForest(formula = HOF ~ ., data = train, ntree = 1000) 
               Type of random forest: classification
                     Number of trees: 1000
No. of variables tried at each split: 4

        OOB estimate of  error rate: 1.56%
Confusion matrix:
    N  Y class.error
N 858  3 0.003484321
Y  11 28 0.282051282</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## predict the probability of HOF
estimate =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(mdl, test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))
estimate<span class="op">$</span>predict =<span class="st"> </span><span class="kw">predict</span>(mdl, test)
estimate<span class="op">$</span>actual =<span class="st"> </span>test<span class="op">$</span>HOF

## confusion matrix
(<span class="dt">confusion =</span> <span class="kw">table</span>(estimate[, <span class="dv">3</span><span class="op">:</span><span class="dv">4</span>]))</code></pre></div>
<pre><code>       actual
predict   N   Y
      N 106   2
      Y   0   8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">test.metric =</span> <span class="kw">metric</span>(confusion))</code></pre></div>
<pre><code>[1] 0.9861111</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## model plots
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>, <span class="dt">y =</span> mdl<span class="op">$</span>err.rate[,<span class="dv">1</span>], <span class="dt">xlab =</span> <span class="st">&quot;Trees&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Error&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
     <span class="dt">main =</span> <span class="st">&quot;Out of Sample Error Rate&quot;</span>)
<span class="kw">varImpPlot</span>(mdl, <span class="dt">main =</span> <span class="st">&quot;Variable Importance Plot&quot;</span>)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/b3-1.png" width="864" /></p>
<p>We get slightly better results from increasing the training set. The Random Forest model is predicting Yes to Hall of Fame if it measures the probability &gt; .5. Since it is is so rare that a player gets voted to the Hall of Fame how accurate is the model if we lower the threshold? Based on a review of some of the false negatives I will make the minimum threshold for predicting yes .33</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## predict the probability of HOF
estimate =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(mdl, test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))
estimate<span class="op">$</span>predict =<span class="st"> &quot;N&quot;</span>
estimate<span class="op">$</span>predict[<span class="kw">which</span>(estimate<span class="op">$</span>Y <span class="op">&gt;</span><span class="st"> </span>.<span class="dv">33</span>)] =<span class="st"> &quot;Y&quot;</span>
estimate<span class="op">$</span>actual =<span class="st"> </span>test<span class="op">$</span>HOF

## confusion matrix
(<span class="dt">confusion =</span> <span class="kw">table</span>(estimate[, <span class="dv">3</span><span class="op">:</span><span class="dv">4</span>]))</code></pre></div>
<pre><code>       actual
predict   N   Y
      N 106   0
      Y   0  10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="dt">test.metric =</span> <span class="kw">metric</span>(confusion))</code></pre></div>
<pre><code>[1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#</span>
<span class="co"># ## required packages</span>
<span class="co"># library(e1071)</span>
<span class="co">#</span>
<span class="co"># ## Training and Testing Data</span>
<span class="co"># hof.train = read.csv(&quot;data/HOF_tr.csv&quot;);</span>
<span class="co"># hof.test = read.csv(&quot;data/HOF_te.csv&quot;)</span>
<span class="co">#</span>
<span class="co"># hof = rbind(hof.train, hof.test)</span>
<span class="co">#</span>
<span class="co"># ## create a training and testing set by randomly sampling from all of the data</span>
<span class="co"># ## using the same set as in the random forest example</span>
<span class="co"># set.seed(1002)</span>
<span class="co"># x = sample(nrow(hof), replace = FALSE)</span>
<span class="co">#</span>
<span class="co"># ## remove unwanted columns</span>
<span class="co"># hof = hof[, -c(2:4)]</span>
<span class="co">#</span>
<span class="co"># ## lets train the model on about 90% of the data</span>
<span class="co"># train = hof[x[1:900], ]</span>
<span class="co"># test = hof[-x[1:900], ]</span>
<span class="co">#</span>
<span class="co"># head(train)</span>
<span class="co"># summary(train)</span>
<span class="co">#</span>
<span class="co"># ## train the model</span>
<span class="co"># (tuneModel = tune(svm, HOF ~ ., data = train,</span>
<span class="co">#                   ranges = list(</span>
<span class="co">#                     cost = seq(.1, 5, .25),</span>
<span class="co">#                     gamma = seq(0, .5, .01))))</span>
<span class="co">#</span>
<span class="co"># ## darker colors are better models</span>
<span class="co"># plot(tuneModel, main = &quot;Model Performance&quot;)</span>
<span class="co">#</span>
<span class="co"># ## store best parameters</span>
<span class="co"># cst = as.numeric(tuneModel$best.parameters[1])</span>
<span class="co"># gma = as.numeric(tuneModel$best.parameters[2])</span>
<span class="co">#</span>
<span class="co"># ## do an indepth search of the darker grid</span>
<span class="co"># (tuneModel = tune(svm, HOF ~ ., data = train,</span>
<span class="co">#                   ranges = list(</span>
<span class="co">#                     cost = seq(cst-1, cst+1, .01),</span>
<span class="co">#                     gamma = seq(0, gma+.1, .001))))</span>
<span class="co">#</span>
<span class="co"># ## darker colors are better models</span>
<span class="co"># plot(tuneModel, main = &quot;Model Performance&quot;)</span>
<span class="co">#</span>
<span class="co"># ## store best parameters</span>
<span class="co"># cst = as.numeric(tuneModel$best.parameters[1])</span>
<span class="co"># gma = as.numeric(tuneModel$best.parameters[2])</span>
<span class="co">#</span>
<span class="co"># ## build model based on the tuned parameters</span>
<span class="co"># mdl = svm(HOF ~ ., data = train, probability = TRUE, cost = cst, gamma = gma)</span>
<span class="co"># x = predict(mdl, test, probability = TRUE)</span>
<span class="co">#</span>
<span class="co"># ## compile results</span>
<span class="co"># results = data.frame(</span>
<span class="co">#   Prediction = x,</span>
<span class="co">#   Actual = test$HOF,</span>
<span class="co">#   Prob.N = attr(x, &quot;probabilities&quot;)[, 1],</span>
<span class="co">#   Prob.Y = attr(x, &quot;probabilities&quot;)[, 2]</span>
<span class="co"># )</span>
<span class="co">#</span>
<span class="co"># ## accuracy calculation from the random forest example</span>
<span class="co"># metric = function(confusion) {</span>
<span class="co">#   sensitivity = confusion[4] / (confusion[2] + confusion[4])</span>
<span class="co">#   specificity = confusion[1] / (confusion[1] + confusion[3])</span>
<span class="co">#   score = (sensitivity + (3 * specificity)) / 4</span>
<span class="co">#   return(score)</span>
<span class="co"># }</span>
<span class="co">#</span>
<span class="co"># ## confusion matrix and accuracy score</span>
<span class="co"># (confusion = table(Prediction = results$Prediction, Actual = results$Actual))</span>
<span class="co">#</span>
<span class="co"># ## accuracy score for training set</span>
<span class="co"># metric(confusion)</span>
<span class="co">#</span>
<span class="co"># ## look at the probability prediction for all of CV results to see if we should lower the</span>
<span class="co"># ## probability threshold for predicing Y to HOF</span>
<span class="co"># subset(results, Prediction != Actual)</span>
<span class="co"># min = min(subset(results, Prediction != Actual, &quot;Prob.Y&quot;))</span>
<span class="co">#</span>
<span class="co"># ## lower the threshold</span>
<span class="co"># results$Prediction.new = &quot;N&quot;</span>
<span class="co"># results$Prediction.new[results$Prob.Y &gt;= min] = &quot;Y&quot;</span>
<span class="co">#</span>
<span class="co"># ## confusion matrix and accuracy score</span>
<span class="co"># (confusion = table(Prediction = results$Prediction.new, Actual = results$Actual))</span>
<span class="co">#</span>
<span class="co"># ## accuracy score for training set</span>
<span class="co"># metric(confusion)</span></code></pre></div>
</div>
</div>
<div id="gradient-boosting" class="section level2 unnumbered">
<h2>Gradient Boosting</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## required packages
<span class="kw">library</span>(caret)
<span class="kw">library</span>(gbm)

## Training and Testing Data
hof.train =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/HOF_tr.csv&quot;</span>);
hof.test =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;data/HOF_te.csv&quot;</span>)

hof =<span class="st"> </span><span class="kw">rbind</span>(hof.train, hof.test)
hof<span class="op">$</span>HOF =<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">as.numeric</span>(hof<span class="op">$</span>HOF) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)

## create a training and testing set by randomly sampling from all of the data
## using the same set as in the random forest example
<span class="kw">set.seed</span>(<span class="dv">1002</span>)
x =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(hof), <span class="dt">replace =</span> <span class="ot">FALSE</span>)

## remove unwanted columns
hof =<span class="st"> </span>hof[, <span class="op">-</span><span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>)]

## lets train the model on about 90% of the data
train =<span class="st"> </span>hof[x[<span class="dv">1</span><span class="op">:</span><span class="dv">900</span>], ]
test =<span class="st"> </span>hof[<span class="op">-</span>x[<span class="dv">1</span><span class="op">:</span><span class="dv">900</span>], ]

<span class="kw">head</span>(train)</code></pre></div>
<pre><code>    HOF POS  ASG    G   AB    R    H  DB TP  HR  RBI  SB  CS  BB   SO AVG
453   0  1B 0.44 2071 7030 1105 1921 295 48 370 1274  63  31 943 1137 273
803   0  SS 0.00  568 1104  142  260  43 10  37  109   7   5  94  220 236
621   0   C 0.00  476 1125   89  267  41  5  18  108   1   0  43  159 237
230   0  OF 0.12 1912 6787  926 1884 334 69 164  824 312 134 468 1266 278
379   0  OF 0.17 1457 4843  737 1399 212 60 142  661  89  68 644  591 289
720   0  OF 0.09 1221 3895  540 1020 175 37 112  485  45  30 351  574 262
    SLG OBP
453 487 359
803 393 304
621 331 268
230 420 325
379 445 371
720 412 323</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(train)</code></pre></div>
<pre><code> HOF     POS           ASG               G                AB       
 0:861   1B: 96   Min.   :0.0000   Min.   : 140.0   Min.   :  252  
 1: 39   2B:105   1st Qu.:0.0000   1st Qu.: 937.5   1st Qu.: 2628  
         3B:100   Median :0.0000   Median :1286.0   Median : 4034  
         C :158   Mean   :0.1028   Mean   :1338.3   Mean   : 4335  
         OF:331   3rd Qu.:0.1500   3rd Qu.:1666.2   3rd Qu.: 5607  
         SS:110   Max.   :0.9500   Max.   :3308.0   Max.   :12364  
       R                H                DB              TP        
 Min.   :  20.0   Min.   :  48.0   Min.   :  6.0   Min.   :  0.00  
 1st Qu.: 308.5   1st Qu.: 660.2   1st Qu.:110.8   1st Qu.: 12.00  
 Median : 511.5   Median :1061.0   Median :180.0   Median : 24.00  
 Mean   : 574.8   Mean   :1159.5   Mean   :199.4   Mean   : 30.44  
 3rd Qu.: 756.0   3rd Qu.:1531.2   3rd Qu.:264.0   3rd Qu.: 41.00  
 Max.   :2295.0   Max.   :3771.0   Max.   :725.0   Max.   :177.00  
       HR              RBI               SB                CS        
 Min.   :  0.00   Min.   :  21.0   Min.   :   0.00   Min.   :  0.00  
 1st Qu.: 37.75   1st Qu.: 280.5   1st Qu.:  15.00   1st Qu.: 15.00  
 Median : 81.00   Median : 447.0   Median :  40.50   Median : 30.00  
 Mean   :115.15   Mean   : 540.9   Mean   :  84.33   Mean   : 41.81  
 3rd Qu.:155.00   3rd Qu.: 708.0   3rd Qu.: 101.25   3rd Qu.: 57.00  
 Max.   :755.00   Max.   :2297.0   Max.   :1406.00   Max.   :335.00  
       BB               SO              AVG             SLG       
 Min.   :  17.0   Min.   :  35.0   Min.   :161.0   Min.   :222.0  
 1st Qu.: 226.8   1st Qu.: 375.5   1st Qu.:248.0   1st Qu.:351.0  
 Median : 363.0   Median : 569.0   Median :262.0   Median :392.5  
 Mean   : 435.3   Mean   : 643.5   Mean   :261.3   Mean   :393.2  
 3rd Qu.: 567.0   3rd Qu.: 841.2   3rd Qu.:274.0   3rd Qu.:432.0  
 Max.   :2190.0   Max.   :2597.0   Max.   :338.0   Max.   :565.0  
      OBP       
 Min.   :203.0  
 1st Qu.:310.0  
 Median :327.0  
 Mean   :328.2  
 3rd Qu.:347.0  
 Max.   :417.0  </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## build model
fitControl =<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;repeatedCV&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>, <span class="dt">repeats =</span> <span class="dv">5</span>)
mdl =<span class="st"> </span><span class="kw">train</span>(HOF <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>, <span class="dt">trControl =</span> fitControl,
            <span class="dt">verbose =</span> <span class="ot">FALSE</span>)

## Model Summary
mdl; <span class="kw">plot</span>(mdl)</code></pre></div>
<pre><code>Stochastic Gradient Boosting 

900 samples
 17 predictor
  2 classes: &#39;0&#39;, &#39;1&#39; 

No pre-processing
Resampling: Cross-Validated (5 fold, repeated 5 times) 
Summary of sample sizes: 720, 721, 719, 720, 720, 720, ... 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.9826764  0.7663030
  1                  100      0.9828986  0.7750536
  1                  150      0.9826788  0.7761948
  2                   50      0.9835603  0.7752180
  2                  100      0.9833406  0.7791355
  2                  150      0.9826727  0.7703613
  3                   50      0.9831184  0.7699114
  3                  100      0.9831171  0.7729111
  3                  150      0.9833394  0.7789173

Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1

Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were n.trees = 50, interaction.depth
 = 2, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p><img src="05-Machine-Learning_files/figure-html/d1-1.png" width="864" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x =<span class="st"> </span><span class="kw">predict</span>(mdl, test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>)

## compile results
results =<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">Actual =</span> test<span class="op">$</span>HOF,
  <span class="dt">Prob.N =</span> x[, <span class="dv">1</span>],
  <span class="dt">Prob.Y =</span> x[, <span class="dv">2</span>]
)

## code 0/1 back to N/Y
results<span class="op">$</span>Actual =<span class="st"> </span><span class="kw">as.character</span>(results<span class="op">$</span>Actual)
results<span class="op">$</span>Actual[results<span class="op">$</span>Actual <span class="op">==</span><span class="st"> &#39;0&#39;</span>] =<span class="st"> &#39;N&#39;</span>
results<span class="op">$</span>Actual[results<span class="op">$</span>Actual <span class="op">==</span><span class="st"> &#39;1&#39;</span>] =<span class="st"> &#39;Y&#39;</span>
results<span class="op">$</span>Actual =<span class="st"> </span><span class="kw">factor</span>(results<span class="op">$</span>Actual)

## if probability of HOF is &gt; .5 then score a Y
results<span class="op">$</span>Prediction =<span class="st"> &quot;N&quot;</span>
results<span class="op">$</span>Prediction[results<span class="op">$</span>Prob.Y <span class="op">&gt;=</span><span class="st"> </span>.<span class="dv">5</span>] =<span class="st"> &quot;Y&quot;</span>
results<span class="op">$</span>Prediction =<span class="st"> </span><span class="kw">factor</span>(results<span class="op">$</span>Prediction)

## accuracy calculation from the random forest example
metric =<span class="st"> </span><span class="cf">function</span>(confusion) {
  sensitivity =<span class="st"> </span>confusion[<span class="dv">4</span>] <span class="op">/</span><span class="st"> </span>(confusion[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>confusion[<span class="dv">4</span>])
  specificity =<span class="st"> </span>confusion[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>(confusion[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>confusion[<span class="dv">3</span>])
  score =<span class="st"> </span>(sensitivity <span class="op">+</span><span class="st"> </span>(<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>specificity)) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>
  <span class="kw">return</span>(score)
}

## confusion matrix and accuracy score
(<span class="dt">confusion =</span> <span class="kw">table</span>(<span class="dt">Prediction =</span> results<span class="op">$</span>Prediction, <span class="dt">Actual =</span> results<span class="op">$</span>Actual))</code></pre></div>
<pre><code>          Actual
Prediction   N   Y
         N 106   4
         Y   0   6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## accuracy score for training set
<span class="kw">metric</span>(confusion)</code></pre></div>
<pre><code>[1] 0.9727273</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## look at the incorrect responses and see if we can lower the threshold with creating
## false positives
<span class="kw">summary</span>(results); <span class="kw">subset</span>(results, Actual <span class="op">!=</span><span class="st"> </span>Prediction)</code></pre></div>
<pre><code> Actual      Prob.N             Prob.Y         Prediction
 N:106   Min.   :0.006085   Min.   :0.000995   N:110     
 Y: 10   1st Qu.:0.998981   1st Qu.:0.001019   Y:  6     
         Median :0.998981   Median :0.001019             
         Mean   :0.932913   Mean   :0.067087             
         3rd Qu.:0.998981   3rd Qu.:0.001019             
         Max.   :0.999005   Max.   :0.993915             </code></pre>
<pre><code>   Actual    Prob.N    Prob.Y Prediction
1       Y 0.8317815 0.1682185          N
3       Y 0.5878968 0.4121032          N
77      Y 0.5756887 0.4243113          N
78      Y 0.6324244 0.3675756          N</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">min.pred =<span class="st"> </span><span class="kw">min</span>(<span class="kw">subset</span>(results, Actual <span class="op">!=</span><span class="st"> </span>Prediction, <span class="st">&quot;Prob.Y&quot;</span>))

## it looks like there is no danger of lowering the threshold
results<span class="op">$</span>Prediction.new =<span class="st"> &quot;N&quot;</span>
results<span class="op">$</span>Prediction.new[results<span class="op">$</span>Prob.Y <span class="op">&gt;=</span><span class="st"> </span>min.pred] =<span class="st"> &quot;Y&quot;</span>

## confusion matrix and accuracy score
(<span class="dt">confusion =</span> <span class="kw">table</span>(<span class="dt">Prediction =</span> results<span class="op">$</span>Prediction.new, <span class="dt">Actual =</span> results<span class="op">$</span>Actual))</code></pre></div>
<pre><code>          Actual
Prediction   N   Y
         N 105   0
         Y   1  10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## accuracy score for training set
<span class="kw">metric</span>(confusion)</code></pre></div>
<pre><code>[1] 0.9772727</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## there are fewer incorrect answers, but the penalty for false positives are greater
## than false negatives so the accuracy score is actually lower</code></pre></div>
</div>
<div id="ensemble-model" class="section level2 unnumbered">
<h2>Ensemble Model</h2>
<hr />
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># hof = read.csv(&quot;data/HOF_tr.csv&quot;)</span>
<span class="co">#</span>
<span class="co"># ## required packages</span>
<span class="co"># library(randomForest)</span>
<span class="co"># library(e1071)</span>
<span class="co">#</span>
<span class="co"># ## remove years and position</span>
<span class="co"># hof = hof[, -c(2, 3, 5)]</span>
<span class="co">#</span>
<span class="co"># ## create some additional metrics</span>
<span class="co"># hof$SO.P.AB = round(with(hof, SO / AB) * 100, 1)</span>
<span class="co"># hof$BB.P.AB = round(with(hof, BB / AB) * 100, 1)</span>
<span class="co"># hof$HR.P.AB = round(with(hof, HR / AB) * 100, 1)</span>
<span class="co"># hof$TP.P.H = round(with(hof, TP / H) * 100, 1)</span>
<span class="co"># hof$DB.P.H = round(with(hof, DB / H) * 100, 1)</span>
<span class="co"># hof$SB.P.H = round(with(hof, SB / (H - HR)) * 100, 1)</span>
<span class="co">#</span>
<span class="co"># ## scale the data</span>
<span class="co"># hof.scale = data.frame(scale(hof[, 2:24], center = TRUE, scale = TRUE))</span>
<span class="co"># hof.scale$HOF = hof$HOF</span>
<span class="co">#</span>
<span class="co"># ## Setup data frame for the leave one out cross validation</span>
<span class="co"># LOO = data.frame()</span>
<span class="co">#</span>
<span class="co"># ## execute the cross validation loop</span>
<span class="co"># for (i in 1:677) {</span>
<span class="co">#</span>
<span class="co">#   ## create the training set</span>
<span class="co">#   train = hof.scale[-i, ]</span>
<span class="co">#   test = hof.scale[i, ]</span>
<span class="co">#</span>
<span class="co">#   ## principal compnents</span>
<span class="co">#   pr = prcomp(train[, 1:23])</span>
<span class="co">#</span>
<span class="co">#   ## calculate PCA for training and testing sets</span>
<span class="co">#   pca.train = data.frame(as.matrix(train[, 1:23]) %*% pr$rotation)</span>
<span class="co">#   train = cbind(HOF = train[, 24], pca.train[, 1:10])</span>
<span class="co">#</span>
<span class="co">#   pca.test = data.frame(as.matrix(test[, 1:23]) %*% pr$rotation)</span>
<span class="co">#   test = cbind(HOF = test[, 24], pca.test[, 1:10])</span>
<span class="co">#</span>
<span class="co">#   ## Model Building</span>
<span class="co">#   ## random forest</span>
<span class="co">#   mdl.rf = randomForest(HOF ~ ., data = train, ntree = 50)</span>
<span class="co">#</span>
<span class="co">#   ## SVM</span>
<span class="co">#   mdl.svm = svm(HOF ~ ., data = train, probability = TRUE)</span>
<span class="co">#   x = predict(mdl.svm, test, probability = TRUE)</span>
<span class="co">#</span>
<span class="co">#   ## Logistic Regression</span>
<span class="co">#   mdl.glm = glm(HOF ~ .,</span>
<span class="co">#                 data = train,</span>
<span class="co">#                 family=binomial(link=&#39;logit&#39;),</span>
<span class="co">#                 control = list(maxit = 500))</span>
<span class="co">#</span>
<span class="co">#   ## data.frame of results</span>
<span class="co">#   results = data.frame(</span>
<span class="co">#     RF = predict(mdl.rf, test, type = &quot;prob&quot;)[, 2],</span>
<span class="co">#     SVM = round(attr(x,&quot;probabilities&quot;)[,&quot;Y&quot;], 3),</span>
<span class="co">#     GLM = round(predict(mdl.glm, test, type = &quot;response&quot;), 3)</span>
<span class="co">#   )</span>
<span class="co">#</span>
<span class="co">#   ## add an average score and decision for all predictions</span>
<span class="co">#   results$AVG = rowMeans(results[, 1:3])</span>
<span class="co">#</span>
<span class="co">#   ## set up empty vectors</span>
<span class="co">#   results$AVG_DEC = 0</span>
<span class="co">#   results$RF_DEC = 0</span>
<span class="co">#   results$SVM_DEC = 0</span>
<span class="co">#   results$GLM_DEC = 0</span>
<span class="co">#</span>
<span class="co">#   ## if the probability is higher than the pretermined value then the model</span>
<span class="co">#   ## predicts HOF entrance</span>
<span class="co">#   results$AVG_DEC[which(results$AVG &gt; .35)] = 1</span>
<span class="co">#   results$RF_DEC[which(results$RF   &gt; .2)] = 1</span>
<span class="co">#   results$SVM_DEC[which(results$SVM &gt; .1)] = 1</span>
<span class="co">#   results$GLM_DEC[which(results$GLM &gt; .2)] = 1</span>
<span class="co">#</span>
<span class="co">#   ## ensemble variable, if any of the individual models predict entrance</span>
<span class="co">#   results$ANY_DEC = 0</span>
<span class="co">#   results$ANY_DEC[which(results$RF_DEC == 1 | results$SVM_DEC == 1 |</span>
<span class="co">#                           results$GLM_DEC == 1)] = 1</span>
<span class="co">#</span>
<span class="co">#   ## Add the actual data</span>
<span class="co">#   results$ACTUAL = test$HOF</span>
<span class="co">#   results$ACTUAL = as.character(results$ACTUAL)</span>
<span class="co">#   results$ACTUAL[which(results$ACTUAL == &quot;Y&quot;)] = 1</span>
<span class="co">#   results$ACTUAL[which(results$ACTUAL == &quot;N&quot;)] = 0</span>
<span class="co">#   results$ACTUAL = as.numeric(results$ACTUAL)</span>
<span class="co">#</span>
<span class="co">#   ## rbind the results</span>
<span class="co">#   LOO = rbind(LOO, results)</span>
<span class="co">#</span>
<span class="co"># }</span>
<span class="co">#</span>
<span class="co"># ## calcualte the accuracy of the individual, average, and any predictions</span>
<span class="co"># ## using the weighted accuracy measure in the homework</span>
<span class="co"># ## (sensitivity + (3 x specificity)) / 4,</span>
<span class="co"># models = data.frame(</span>
<span class="co">#   RF = ((length(which(LOO$RF_DEC == 1 &amp; LOO$ACTUAL == 1))  /</span>
<span class="co">#            sum(LOO$ACTUAL)) + 3 * (length(which(LOO$RF_DEC ==  0 &amp; LOO$ACTUAL == 0)) /</span>
<span class="co">#                                      length(which(LOO$ACTUAL == 0))))/4,</span>
<span class="co">#   SVM = ((length(which(LOO$SVM_DEC == 1 &amp; LOO$ACTUAL == 1))/</span>
<span class="co">#             sum(LOO$ACTUAL)) + 3 * (length(which(LOO$SVM_DEC == 0 &amp; LOO$ACTUAL == 0)) /</span>
<span class="co">#                                       length(which(LOO$ACTUAL == 0))))/4,</span>
<span class="co">#   GLM = ((length(which(LOO$GLM_DEC == 1 &amp; LOO$ACTUAL == 1))/</span>
<span class="co">#             sum(LOO$ACTUAL)) + 3 * (length(which(LOO$GLM_DEC == 0 &amp; LOO$ACTUAL == 0)) /</span>
<span class="co">#                                       length(which(LOO$ACTUAL == 0))))/4,</span>
<span class="co">#   AVG = ((length(which(LOO$AVG_DEC == 1 &amp; LOO$ACTUAL == 1))/</span>
<span class="co">#             sum(LOO$ACTUAL)) + 3 * (length(which(LOO$AVG_DEC == 0 &amp; LOO$ACTUAL == 0)) /</span>
<span class="co">#                                       length(which(LOO$ACTUAL == 0))))/4,</span>
<span class="co">#   ANY = ((length(which(LOO$ANY_DEC == 1 &amp; LOO$ACTUAL == 1))/</span>
<span class="co">#             sum(LOO$ACTUAL)) + 3 * (length(which(LOO$ANY_DEC == 0 &amp; LOO$ACTUAL == 0)) /</span>
<span class="co">#                                       length(which(LOO$ACTUAL == 0))))/4</span>
<span class="co">#   )</span>
<span class="co">#</span>
<span class="co"># models = round(models, 3); print(colMeans(models))</span>
<span class="co">#</span>
<span class="co"># ## Build final models to be used in the prediction</span>
<span class="co"># train = rbind(train, test)</span>
<span class="co">#</span>
<span class="co"># ## Model Building</span>
<span class="co"># ## random forest</span>
<span class="co"># mdl.rf = randomForest(HOF ~ ., data = train, ntree = 50)</span>
<span class="co">#</span>
<span class="co"># ## SVM</span>
<span class="co"># mdl.svm = svm(HOF ~ ., data = train, probability = TRUE)</span>
<span class="co">#</span>
<span class="co"># ## Logistic Regression</span>
<span class="co"># mdl.glm = glm(HOF ~ .,</span>
<span class="co">#               data = train,</span>
<span class="co">#               family=binomial(link=&#39;logit&#39;),</span>
<span class="co">#               control = list(maxit = 500))</span>
<span class="co">#</span>
<span class="co">#</span>
<span class="co"># ## cleanup workspace</span>
<span class="co"># rm(i, x, results, pca.test, pca.train, test, train)</span>
<span class="co">#</span>
<span class="co"># ## The final models have been trained, now it is time to test the accuracy based on data that the models have not seen.  We need to create a function that uses the trained models and predicts the outcome to compare against the actual outcome.</span>
<span class="co">#</span>
<span class="co"># ## Bring in the test data</span>
<span class="co"># dta = read.csv(&quot;data/HOF_te.csv&quot;)</span>
<span class="co">#</span>
<span class="co"># ## Ensemble prediction function</span>
<span class="co"># ens_prediction = function(dta) {</span>
<span class="co">#</span>
<span class="co">#   ## expecting a dataframe with same column names as the training set</span>
<span class="co">#   ## including the response variable</span>
<span class="co">#   hof = dta</span>
<span class="co">#</span>
<span class="co">#   ## remove years and position</span>
<span class="co">#   hof = hof[, -c(2, 3, 5)]</span>
<span class="co">#</span>
<span class="co">#   ## create some additional metrics</span>
<span class="co">#   hof$SO.P.AB = round(with(hof, SO / AB) * 100, 1)</span>
<span class="co">#   hof$BB.P.AB = round(with(hof, BB / AB) * 100, 1)</span>
<span class="co">#   hof$HR.P.AB = round(with(hof, HR / AB) * 100, 1)</span>
<span class="co">#   hof$TP.P.H = round(with(hof, TP / H) * 100, 1)</span>
<span class="co">#   hof$DB.P.H = round(with(hof, DB / H) * 100, 1)</span>
<span class="co">#   hof$SB.P.H = round(with(hof, SB / (H - HR)) * 100, 1)</span>
<span class="co">#</span>
<span class="co">#   ## scale the data</span>
<span class="co">#   hof.scale = data.frame(scale(hof[, 2:24], center = TRUE, scale = TRUE))</span>
<span class="co">#   hof.scale$HOF = hof$HOF</span>
<span class="co">#</span>
<span class="co">#   ## use principal components object to calculate PCA</span>
<span class="co">#   hof.pca = data.frame(as.matrix(hof.scale[, 1:23]) %*% pr$rotation)</span>
<span class="co">#   test = cbind(HOF = hof.scale[, 24], hof.pca[, 1:10])</span>
<span class="co">#</span>
<span class="co">#   ## create predictions</span>
<span class="co">#   x = predict(mdl.svm, test, probability = TRUE)</span>
<span class="co">#</span>
<span class="co">#   ## data.frame of results</span>
<span class="co">#   results = data.frame(</span>
<span class="co">#    RF = predict(mdl.rf, test, type = &quot;prob&quot;)[, 2],</span>
<span class="co">#    SVM = round(attr(x,&quot;probabilities&quot;)[,&quot;Y&quot;], 3),</span>
<span class="co">#    GLM = round(predict(mdl.glm, test, type = &quot;response&quot;), 3))</span>
<span class="co">#</span>
<span class="co">#   ## add an average score and decision for all predictions</span>
<span class="co">#   results$AVG = rowMeans(results[, 1:3])</span>
<span class="co">#</span>
<span class="co">#   ## set up empty vectors</span>
<span class="co">#   results$AVG_DEC = 0</span>
<span class="co">#   results$RF_DEC = 0</span>
<span class="co">#   results$SVM_DEC = 0</span>
<span class="co">#   results$GLM_DEC = 0</span>
<span class="co">#</span>
<span class="co">#   ## if the probability is higher than the pretermined value then the model</span>
<span class="co">#   ## predicts HOF entrance</span>
<span class="co">#   results$AVG_DEC[which(results$AVG &gt; .35)] = 1</span>
<span class="co">#   results$RF_DEC[which(results$RF   &gt; .2)] = 1</span>
<span class="co">#   results$SVM_DEC[which(results$SVM &gt; .1)] = 1</span>
<span class="co">#   results$GLM_DEC[which(results$GLM &gt; .2)] = 1</span>
<span class="co">#</span>
<span class="co">#   ## ensemble variable, if any of the individual models predict entrance</span>
<span class="co">#   results$ANY_DEC = 0</span>
<span class="co">#   results$ANY_DEC[which(results$RF_DEC == 1 |</span>
<span class="co">#                           results$SVM_DEC == 1 |</span>
<span class="co">#                           results$GLM_DEC == 1)] = 1</span>
<span class="co">#</span>
<span class="co">#   results$FINAL = &#39;N&#39;</span>
<span class="co">#   results$FINAL[which(results$ANY_DEC == 1)] = &#39;Y&#39;</span>
<span class="co">#</span>
<span class="co">#   return(results$FINAL)</span>
<span class="co">#</span>
<span class="co">#   ##    RF   SVM   GLM   AVG   ANY</span>
<span class="co">#   ##  0.909 0.898 0.942 0.937 0.948</span>
<span class="co">#</span>
<span class="co">#   ## Final prediction using an ensemble method that predicts HOF if any</span>
<span class="co">#   ## of the individual models predicts HOF probability &gt; .6</span>
<span class="co">#   ##</span>
<span class="co">#   ## Expected Weighted Accuracy: .948</span>
<span class="co"># }</span>
<span class="co">#</span>
<span class="co"># ## Create a table for comparison</span>
<span class="co"># final = data.frame(HOF = dta$HOF)</span>
<span class="co">#</span>
<span class="co"># ## Prediction</span>
<span class="co"># final$prediction = ens_prediction(dta)</span>
<span class="co">#</span>
<span class="co"># ## Compare the results</span>
<span class="co"># table(final)</span></code></pre></div>
</div>
<div id="overfitting" class="section level2 unnumbered">
<h2>Overfitting</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pander)
<span class="kw">library</span>(e1071)

## First 500 digits in pi
dta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">8</span>,
        <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">2</span>,
        <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">9</span>,
        <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">0</span>, <span class="dv">8</span>,
        <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">8</span>,
        <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">8</span>,
        <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">9</span>, <span class="dv">4</span>,
        <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">3</span>,
        <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">7</span>,
        <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">1</span>,
        <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">4</span>,
        <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">8</span>,
        <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">6</span>,
        <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">6</span>,
        <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">5</span>,
        <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">8</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">8</span>,
        <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">0</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">7</span>,
        <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">4</span>, <span class="dv">9</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>,
        <span class="dv">7</span>, <span class="dv">9</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">1</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">9</span>, <span class="dv">4</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">2</span>)

## Create 5 variables to based on the lagged value of the ith digit
dta =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> dta)

dta<span class="op">$</span>x1 =<span class="st"> </span><span class="ot">NA</span>; dta<span class="op">$</span>x2 =<span class="st"> </span><span class="ot">NA</span>; dta<span class="op">$</span>x3 =<span class="st"> </span><span class="ot">NA</span>; dta<span class="op">$</span>x4 =<span class="st"> </span><span class="ot">NA</span>; dta<span class="op">$</span>x5 =<span class="st"> </span><span class="ot">NA</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">500</span>) {
  dta<span class="op">$</span>x1[i] =<span class="st"> </span>dta<span class="op">$</span>y[i<span class="op">-</span><span class="dv">1</span>]
}

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">3</span><span class="op">:</span><span class="dv">500</span>) {
  dta<span class="op">$</span>x2[i] =<span class="st"> </span>dta<span class="op">$</span>y[i<span class="op">-</span><span class="dv">2</span>]
}

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">4</span><span class="op">:</span><span class="dv">500</span>) {
  dta<span class="op">$</span>x3[i] =<span class="st"> </span>dta<span class="op">$</span>y[i<span class="op">-</span><span class="dv">3</span>]
}

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">5</span><span class="op">:</span><span class="dv">500</span>) {
  dta<span class="op">$</span>x4[i] =<span class="st"> </span>dta<span class="op">$</span>y[i<span class="op">-</span><span class="dv">4</span>]
}

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">6</span><span class="op">:</span><span class="dv">500</span>) {
  dta<span class="op">$</span>x5[i] =<span class="st"> </span>dta<span class="op">$</span>y[i<span class="op">-</span><span class="dv">5</span>]
}

<span class="kw">head</span>(dta)</code></pre></div>
<pre><code>  y x1 x2 x3 x4 x5
1 3 NA NA NA NA NA
2 1  3 NA NA NA NA
3 4  1  3 NA NA NA
4 1  4  1  3 NA NA
5 5  1  4  1  3 NA
6 9  5  1  4  1  3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Remove NA
dta =<span class="st"> </span>dta[<span class="dv">6</span><span class="op">:</span><span class="dv">500</span>,]

## Create Factors out of the variables
dta[] =<span class="st"> </span><span class="kw">lapply</span>(dta, factor)

## Break up the data into the training and testing sets
train =<span class="st"> </span>dta[<span class="dv">1</span><span class="op">:</span><span class="dv">475</span>, ]
test =<span class="st"> </span>dta[<span class="dv">476</span><span class="op">:</span><span class="dv">495</span>, ]


## Tune an SVM Model
mdl.svm =<span class="st"> </span><span class="kw">tune</span>(svm, y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train,
               <span class="dt">ranges =</span> <span class="kw">list</span>(
                 <span class="dt">cost =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">2</span>),
                 <span class="dt">gamma =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, .<span class="dv">1</span>))
)

<span class="kw">plot</span>(mdl.svm)</code></pre></div>
<p><img src="05-Machine-Learning_files/figure-html/f1-1.png" width="768" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">best.cost =<span class="st"> </span>mdl.svm<span class="op">$</span>best.parameters[<span class="dv">1</span>]
best.gamma =<span class="st"> </span>mdl.svm<span class="op">$</span>best.parameters[<span class="dv">2</span>]

mdl.svm =<span class="st"> </span><span class="kw">svm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">cost =</span> best.cost, <span class="dt">gamma =</span> best.gamma,
              <span class="dt">probability =</span> <span class="ot">TRUE</span>)

## Predict the testing set
tmp =<span class="st"> </span><span class="kw">predict</span>(mdl.svm, test, <span class="dt">probability =</span> <span class="ot">TRUE</span>)

results.svm =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">actual =</span> test<span class="op">$</span>y, <span class="dt">predicted =</span> tmp)
results.svm<span class="op">$</span>Result =<span class="st"> </span><span class="ot">FALSE</span>
results.svm<span class="op">$</span>Result[<span class="kw">which</span>(results.svm<span class="op">$</span>actual <span class="op">==</span><span class="st"> </span>results.svm<span class="op">$</span>predicted)] =<span class="st"> </span><span class="ot">TRUE</span>


## create Predict the training set
train.results.svm =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">actual =</span> train<span class="op">$</span>y, <span class="dt">pred =</span> <span class="kw">predict</span>(mdl.svm, train))
train.results.svm<span class="op">$</span>Result =<span class="st"> </span><span class="ot">FALSE</span>
train.results.svm<span class="op">$</span>Result[train.results.svm<span class="op">$</span>actual <span class="op">==</span><span class="st"> </span>train.results.svm<span class="op">$</span>pred] =<span class="st"> </span><span class="ot">TRUE</span>


## Aggregate results
results =<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">Train.Incorrect =</span> <span class="kw">round</span>(<span class="kw">table</span>(train.results.svm<span class="op">$</span>Result)[[<span class="dv">1</span>]]<span class="op">/</span><span class="dv">475</span>, <span class="dv">3</span>),
  <span class="dt">Train.Correct =</span> <span class="kw">round</span>(<span class="kw">table</span>(train.results.svm<span class="op">$</span>Result)[[<span class="dv">2</span>]]<span class="op">/</span><span class="dv">475</span>, <span class="dv">3</span>),
  <span class="dt">Test.Incorrect =</span> <span class="kw">round</span>(<span class="kw">table</span>(results.svm<span class="op">$</span>Result)[[<span class="dv">1</span>]]<span class="op">/</span><span class="dv">20</span>, <span class="dv">3</span>),
  <span class="dt">Test.Correct =</span> <span class="kw">round</span>(<span class="kw">table</span>(results.svm<span class="op">$</span>Result)[[<span class="dv">2</span>]]<span class="op">/</span><span class="dv">20</span>, <span class="dv">3</span>)
)


<span class="kw">pandoc.table</span>(results, <span class="dt">split.tables =</span> <span class="ot">Inf</span>)</code></pre></div>
<pre><code>
-----------------------------------------------------------------
 Train.Incorrect   Train.Correct   Test.Incorrect   Test.Correct 
----------------- --------------- ---------------- --------------
      0.147            0.853            0.8             0.2      
-----------------------------------------------------------------</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="04-Forecasting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="06-Simulation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
