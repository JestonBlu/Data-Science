[
["index.html", "Data Science Tasks Intro", " Data Science Tasks Intro This website is a work in progress and intended to be a personal reference for data science related tasks. It consists of reproducible programming examples for various analytic and visualization techniques. All code is available here. "],
["statistical-methods.html", "Statistical Methods ", " Statistical Methods "],
["fitting-distributions.html", "Fitting Distributions", " Fitting Distributions Continuous Distributions Assessing Distributions Visually Formal Tests for Distribution Fit Maximum Likelihood calculation Normal Distribution library(nortest) library(MASS) ## Draw some random data set.seed(10) x1 = rnorm(20) ## Distribution plots par(mfrow = c(2, 2)) qqnorm(x1) qqline(x1) boxplot(x1, main = &quot;Boxplot&quot;) hist(x1, freq = FALSE, main = &quot;Histogram with Density Curve&quot;) lines(density(x1)) plot(ecdf(x1), main = &quot;Empiracle CDF&quot;) QQ plot indicates the data might be normal by remaining close to the line. The Box plot, histogram, and density curve all support this assumption. Formal tests all agree that the data are from the normal distribution. Shapiro Wilk is considered the best for test for testing normality. ## Are the data from a normal distribution? ## Shapiro-Wilk Test shapiro.test(x1) Shapiro-Wilk normality test data: x1 W = 0.95643, p-value = 0.4753 ## Anderson Darling Test ad.test(x1) Anderson-Darling normality test data: x1 A = 0.27523, p-value = 0.6216 ## Kolmogorov-Smirnoff Test ks.test(x1, &#39;pnorm&#39;) One-sample Kolmogorov-Smirnov test data: x1 D = 0.13528, p-value = 0.8109 alternative hypothesis: two-sided Chi-squared Distribution ## Draw some random data set.seed(10) x2 = rchisq(n = 20, 2) ## Estimate the DF parameter by maximum likelihood fitdistr(x = x2, dchisq, start = list(df = 2)) df 1.8140625 (0.3244201) ## Input the estimate from MLE ks.test(x = x2, y = pchisq, df = 1.8140625) One-sample Kolmogorov-Smirnov test data: x2 D = 0.15608, p-value = 0.6584 alternative hypothesis: two-sided ## Distribution plots par(mfrow = c(2, 2)) qqplot(qchisq(ppoints(20), df = 1.8140625), x2, main = &quot;QQ Plot&quot;) qqline(x2, distribution = function(p) qchisq(p, df = 1.8140625)) boxplot(x2, main = &quot;Boxplot&quot;) hist(x2, freq = FALSE, main = &quot;Histogram with Density Curve&quot;) lines(density(x2)) plot(ecdf(x2), main = &quot;Empiracle CDF&quot;) Calculating the MLE manually ## Generate data from the exponential distribution with mean = 1/5 set.seed(1000) X = rexp(n = 20, rate = 5) ## sample size and range of betas to test n = 20; beta = seq(.01, .5, by = .01) ## Liklihood function Likelihood = (1/beta)^n * exp(-1/beta * sum(X)) ## Maximum Likelihood (mle = max(Likelihood)) [1] 276565.8 (mle.beta = beta[which(Likelihood == mle)]) [1] 0.2 ## Statistical test for how well the specified distribution fits the data ks.test(x = X, y = &quot;pexp&quot;, rate = 1/mle.beta) One-sample Kolmogorov-Smirnov test data: X D = 0.1475, p-value = 0.7232 alternative hypothesis: two-sided par(mfrow = c(1, 2)) ## Plot the maximum likelihood plot(x = beta, y = Likelihood, type = &quot;l&quot;, main = &quot;Maximum Likelihood&quot;, lwd = 2) abline(h = mle, v = mle.beta, lty = 2) ## QQplot for assessing distribution fit visually qqplot(qexp(ppoints(10), rate = 1/mle.beta), X, xlab = &quot;QQ&quot;, main = &quot;QQ Plot&quot;) qqline(X, distribution = function(p) qexp(p, rate = 1/mle.beta)) Discrete Distributions Fitting a Binomial model Chi-squared goodness of fit test Producing a markdown table A rare but fatal disease of genetic origin occurring chiefly in infants and children is under investigation. An experiment was conducted on a 100 couples who are both carriers of the disease and have 5 children. A researcher recorded the number of children having the disease for each couple. library(knitr) ## Dataset (dta = data.frame( Diseased = c(0, 1, 2, 3, 4, 5), Count = c(21, 42, 24, 8, 4, 1) )) Diseased Count 1 0 21 2 1 42 3 2 24 4 3 8 5 4 4 6 5 1 ## Number of diseased children (d) and the total number of children (c) (d = sum(apply(X = dta, MARGIN = 1, FUN = function(p) dta$Diseased * dta$Count)[,1])) [1] 135 (c = 5 * sum(dta$Count)) [1] 500 ## MLE (mle = d / c) [1] 0.27 ## Calculate the expected probabilities and expected diseased children dta$Exp.Prob = round(dbinom(x = 0:5, size = 5, prob = mle), 4) dta$Exp.Diseased = with(dta, sum(Count) * Exp.Prob) dta Diseased Count Exp.Prob Exp.Diseased 1 0 21 0.2073 20.73 2 1 42 0.3834 38.34 3 2 24 0.2836 28.36 4 3 8 0.1049 10.49 5 4 4 0.0194 1.94 6 5 1 0.0014 0.14 ## Chi-square test requirements: ## 1) all Exp must be &gt; 1 ## 2) at most 20% of Exp may be less than 5 ## ## So we need to combine counts 4 and 5 and meet these requirements dta[5, 2:4] = dta[5, 2:4] + dta[6, 2:4] dta = dta[-6, ] dta$Diseased = as.character(dta$Diseased) dta$Diseased[5] = &#39;4 or 5&#39; ## Compute the Chi-Squared Statistic dta$X.2 = round(with(dta, (Count - Exp.Diseased)^2 / Exp.Diseased), 4) dta$Diseased = factor(dta$Diseased) dta Diseased Count Exp.Prob Exp.Diseased X.2 1 0 21 0.2073 20.73 0.0035 2 1 42 0.3834 38.34 0.3494 3 2 24 0.2836 28.36 0.6703 4 3 8 0.1049 10.49 0.5910 5 4 or 5 5 0.0208 2.08 4.0992 ## Compute the test statistic pvalue (TS = sum(dta$X.2)); 1 - pchisq(TS, df = 4) [1] 5.7134 [1] 0.2215985 ## Based on the table below we conclude the binomial model is a good fit for the data ## ## Assessment of Chi-squared GOF P-value ## • p-value &gt; .25 ⇒ Excellent fit ## • .15 ≤ p−value &lt; .25 ⇒ Good fit ## • .05 ≤ p−value &lt; .15 ⇒ Moderately Good fit ## • .01 ≤ p−value &lt; .05 ⇒ Poor fit ## • p-value &lt; .01 ⇒ Unacceptable fit ## ## Plot of the data vs model and create a markdown table from the data frame plot(x = dta$Diseased, y = NULL, xlab = &quot;Diseased Children per Couple&quot;, ylim = c(0, 50), ylab = &quot;Frequency&quot;, axes = FALSE, type = &quot;n&quot;, main = &quot;Binomial Model vs Data&quot;) axis(1, labels = dta$Diseased, at = dta$Diseased) axis(2, labels = seq(0, 50, 10), at = seq(0, 50, 10)) legend(&quot;topright&quot;, c(&quot;Data&quot;, &quot;Model&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 19, bty = &quot;n&quot;) points(x = dta$Diseased, y = dta$Count, col = &quot;blue&quot;, pch = 19, type = &quot;b&quot;) points(x = dta$Diseased, y = dta$Exp.Diseased, col = &quot;red&quot;, pch = 19, type = &quot;b&quot;) kable(dta) Diseased Count Exp.Prob Exp.Diseased X.2 0 21 0.2073 20.73 0.0035 1 42 0.3834 38.34 0.3494 2 24 0.2836 28.36 0.6703 3 8 0.1049 10.49 0.5910 4 or 5 5 0.0208 2.08 4.0992 ## Some Conclusions ## Since N is large we can use asymptotic confidence intervals ## A 95% confidence interval for whether a child will have the disease mle + c(-1, 1) * 1.96 * sqrt(.27 * (1 - .27))/sqrt(100) [1] 0.1829839 0.3570161 ## What is the probability that a couple will have at least 1 child with the disease? 1 - pbinom(q = 0, size = 5, prob = .27) [1] 0.7926928 "],
["hypothesis-testing.html", "Hypothesis Testing", " Hypothesis Testing Shapiro Wilks One Sample T-Test Calculating Power Hypothesis Testing Sample Size Determination ## Data sample on chick weights data(chickwts) head(chickwts) weight feed 1 179 horsebean 2 160 horsebean 3 136 horsebean 4 227 horsebean 5 217 horsebean 6 168 horsebean ## Normal reference plot for height qqnorm(chickwts$weight) qqline(chickwts$weight) ## Are the data from a normal distribution? shapiro.test(chickwts$weight) Shapiro-Wilk normality test data: chickwts$weight W = 0.97674, p-value = 0.2101 mean(chickwts$weight) [1] 261.3099 How does the sample mean compare to a hypothesis test that the true mean is &lt; 260? What is the power of the test? \\[H_0: \\mu \\ge 260, H_a: \\mu \\lt 260\\] Population Fail to Reject Reject \\(H_0\\) \\(H_0\\) is True Correct Type I Error \\(H_a\\) is True Type II Error Correct ## What is the probability of a Type I error if we say the true mean is less than 250? t.test(chickwts$weight, mu = 250, alternative = &quot;less&quot;) One Sample t-test data: chickwts$weight t = 1.2206, df = 70, p-value = 0.8868 alternative hypothesis: true mean is less than 250 95 percent confidence interval: -Inf 276.7549 sample estimates: mean of x 261.3099 ## Verify the t statistic and p-value (ts = (mean(chickwts$weight) - 250) / (sd(chickwts$weight) / sqrt(length(chickwts$weight)))) [1] 1.220623 pt(ts, df = 70) [1] 0.8868377 ## What is the probability of a Type I error if we say the true mean is &gt; 245? t.test(chickwts$weight, mu = 245, alternative = &quot;greater&quot;) One Sample t-test data: chickwts$weight t = 1.7603, df = 70, p-value = 0.04137 alternative hypothesis: true mean is greater than 245 95 percent confidence interval: 245.8648 Inf sample estimates: mean of x 261.3099 ## Verify the t statistic and p-value (ts = (mean(chickwts$weight) - 245) / (sd(chickwts$weight) / sqrt(length(chickwts$weight)))) [1] 1.760251 1 - pt(ts, df = 70) [1] 0.04136678 ## We have rejected the null hypothesis and said under an alpha of .05 there is enough evidence ## to suppor that the true mean of Chick Weights is &gt; 245 ## What is the power of our test? power.t.test(n = length(chickwts$weight), delta = abs(mean(chickwts$weight) - 245), sd = sd(chickwts$weight), sig.level = .05, type = &quot;one.sample&quot;, alternative = &quot;one.sided&quot;, strict = TRUE) One-sample t test power calculation n = 71 delta = 16.30986 sd = 78.0737 sig.level = 0.05 power = 0.5391727 alternative = one.sided ## What sample size would we need to have a power of .8? power.t.test(delta = abs(mean(chickwts$weight) - 245), sd = sd(chickwts$weight), sig.level = .05, power = .8, type = &quot;one.sample&quot;, alternative = &quot;one.sided&quot;, strict = TRUE) One-sample t test power calculation n = 143.0323 delta = 16.30986 sd = 78.0737 sig.level = 0.05 power = 0.8 alternative = one.sided ## Verify manually (sd(chickwts$weight)^2 * (qnorm(p = .95) + qnorm(p = .8))^2) / abs(mean(chickwts$weight) - 245)^2 [1] 141.6698 "],
["parameter-estimation.html", "Parameter Estimation", " Parameter Estimation Bootstrap Sampling ## Create a mixed Distribution set.seed(10) y1 = rnorm(10, 10, 2); y2 = rnorm(10, 15, 1); y3 = rnorm(10, 20, 2) Y = c(y1, y2, y3) par(mfrow = c(1, 2)) hist(Y) plot(density(Y)) ## Is Y part of a normal distribution? shapiro.test(Y) Shapiro-Wilk normality test data: Y W = 0.90712, p-value = 0.0126 ## Bootstrap simulation to estimate the mean X = c() ## Draw 10k random samples from Y and calculate the mean for (i in 1:10000) { x = sample(Y, length(Y), replace = TRUE) mu.x = mean(x) X = c(X, mu.x) } par(mfrow = c(1,1)) ## Draw a histogram of the bootstrap samples for the sample means hist(X, breaks = 50, main = &quot;Histogram of X_Bar&quot;) ## What is a 95% confidence interval for mu? sort(X)[c(250, 9750)] [1] 12.72430 15.58161 "],
["sample-size-and-power.html", "Sample Size and Power", " Sample Size and Power Proportions library(pwr) ## Univariate Proportion pwr.p.test(h = .25, sig.level = .05, power = .8, alternative = &quot;greater&quot;) proportion power calculation for binomial distribution (arcsine transformation) h = 0.25 n = 98.92092 sig.level = 0.05 power = 0.8 alternative = greater ## Two Proportions (equal n) pwr.2p.test(h = .25, sig.level = .05, power = .8, alternative = &quot;greater&quot;) Difference of proportion power calculation for binomial distribution (arcsine transformation) h = 0.25 n = 197.8418 sig.level = 0.05 power = 0.8 alternative = greater NOTE: same sample sizes ## Two Proportions (different n) pwr.2p2n.test(h = .25, n1 = 100, n2 = 120, sig.level = .05, alternative = &quot;greater&quot;) difference of proportion power calculation for binomial distribution (arcsine transformation) h = 0.25 n1 = 100 n2 = 120 sig.level = 0.05 power = 0.5798535 alternative = greater NOTE: different sample sizes T-test ## Equal n pwr.t.test(d = .5, sig.level = .05, power = .8, type = &quot;one.sample&quot;) One-sample t test power calculation n = 33.36713 d = 0.5 sig.level = 0.05 power = 0.8 alternative = two.sided pwr.t.test(d = .5, sig.level = .05, power = .8, type = &quot;two.sample&quot;) Two-sample t test power calculation n = 63.76561 d = 0.5 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number in *each* group pwr.t.test(d = .5, sig.level = .05, power = .8, type = &quot;paired&quot;) Paired t test power calculation n = 33.36713 d = 0.5 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number of *pairs* ## Different n pwr.t2n.test(n1 = 10, n2 = 15, d = 1, sig.level = .05) t test power calculation n1 = 10 n2 = 15 d = 1 sig.level = 0.05 power = 0.6503918 alternative = two.sided Chi-square pwr.chisq.test(w = .25, df = 4, sig.level = .05, power = .8) Chi squared power calculation w = 0.25 N = 190.9646 df = 4 sig.level = 0.05 power = 0.8 NOTE: N is the number of observations ANOVA pwr.anova.test(k = 5, n = 10, f = .5, sig.level = .05) Balanced one-way analysis of variance power calculation k = 5 n = 10 f = 0.5 sig.level = 0.05 power = 0.7730915 NOTE: n is number in each group "],
["survival-analysis.html", "Survival Analysis", " Survival Analysis library(survival) summary(lung) inst time status age Min. : 1.00 Min. : 5.0 Min. :1.000 Min. :39.00 1st Qu.: 3.00 1st Qu.: 166.8 1st Qu.:1.000 1st Qu.:56.00 Median :11.00 Median : 255.5 Median :2.000 Median :63.00 Mean :11.09 Mean : 305.2 Mean :1.724 Mean :62.45 3rd Qu.:16.00 3rd Qu.: 396.5 3rd Qu.:2.000 3rd Qu.:69.00 Max. :33.00 Max. :1022.0 Max. :2.000 Max. :82.00 NA&#39;s :1 sex ph.ecog ph.karno pat.karno Min. :1.000 Min. :0.0000 Min. : 50.00 Min. : 30.00 1st Qu.:1.000 1st Qu.:0.0000 1st Qu.: 75.00 1st Qu.: 70.00 Median :1.000 Median :1.0000 Median : 80.00 Median : 80.00 Mean :1.395 Mean :0.9515 Mean : 81.94 Mean : 79.96 3rd Qu.:2.000 3rd Qu.:1.0000 3rd Qu.: 90.00 3rd Qu.: 90.00 Max. :2.000 Max. :3.0000 Max. :100.00 Max. :100.00 NA&#39;s :1 NA&#39;s :1 NA&#39;s :3 meal.cal wt.loss Min. : 96.0 Min. :-24.000 1st Qu.: 635.0 1st Qu.: 0.000 Median : 975.0 Median : 7.000 Mean : 928.8 Mean : 9.832 3rd Qu.:1150.0 3rd Qu.: 15.750 Max. :2600.0 Max. : 68.000 NA&#39;s :47 NA&#39;s :14 mdl = survfit(Surv(time, status) ~ 1, data = lung) plot(mdl, conf.int = FALSE, main=&quot;Kaplan-Meier Estimator of Survival Function&quot;, xlab=&quot;Survival Time&quot;, ylab=&quot;Survival Function&quot;) summary(mdl) Call: survfit(formula = Surv(time, status) ~ 1, data = lung) time n.risk n.event survival std.err lower 95% CI upper 95% CI 5 228 1 0.9956 0.00438 0.9871 1.000 11 227 3 0.9825 0.00869 0.9656 1.000 12 224 1 0.9781 0.00970 0.9592 0.997 13 223 2 0.9693 0.01142 0.9472 0.992 15 221 1 0.9649 0.01219 0.9413 0.989 26 220 1 0.9605 0.01290 0.9356 0.986 30 219 1 0.9561 0.01356 0.9299 0.983 31 218 1 0.9518 0.01419 0.9243 0.980 53 217 2 0.9430 0.01536 0.9134 0.974 54 215 1 0.9386 0.01590 0.9079 0.970 59 214 1 0.9342 0.01642 0.9026 0.967 60 213 2 0.9254 0.01740 0.8920 0.960 61 211 1 0.9211 0.01786 0.8867 0.957 62 210 1 0.9167 0.01830 0.8815 0.953 65 209 2 0.9079 0.01915 0.8711 0.946 71 207 1 0.9035 0.01955 0.8660 0.943 79 206 1 0.8991 0.01995 0.8609 0.939 81 205 2 0.8904 0.02069 0.8507 0.932 88 203 2 0.8816 0.02140 0.8406 0.925 92 201 1 0.8772 0.02174 0.8356 0.921 93 199 1 0.8728 0.02207 0.8306 0.917 95 198 2 0.8640 0.02271 0.8206 0.910 105 196 1 0.8596 0.02302 0.8156 0.906 107 194 2 0.8507 0.02362 0.8056 0.898 110 192 1 0.8463 0.02391 0.8007 0.894 116 191 1 0.8418 0.02419 0.7957 0.891 118 190 1 0.8374 0.02446 0.7908 0.887 122 189 1 0.8330 0.02473 0.7859 0.883 131 188 1 0.8285 0.02500 0.7810 0.879 132 187 2 0.8197 0.02550 0.7712 0.871 135 185 1 0.8153 0.02575 0.7663 0.867 142 184 1 0.8108 0.02598 0.7615 0.863 144 183 1 0.8064 0.02622 0.7566 0.859 145 182 2 0.7975 0.02667 0.7469 0.852 147 180 1 0.7931 0.02688 0.7421 0.848 153 179 1 0.7887 0.02710 0.7373 0.844 156 178 2 0.7798 0.02751 0.7277 0.836 163 176 3 0.7665 0.02809 0.7134 0.824 166 173 2 0.7577 0.02845 0.7039 0.816 167 171 1 0.7532 0.02863 0.6991 0.811 170 170 1 0.7488 0.02880 0.6944 0.807 175 167 1 0.7443 0.02898 0.6896 0.803 176 165 1 0.7398 0.02915 0.6848 0.799 177 164 1 0.7353 0.02932 0.6800 0.795 179 162 2 0.7262 0.02965 0.6704 0.787 180 160 1 0.7217 0.02981 0.6655 0.783 181 159 2 0.7126 0.03012 0.6559 0.774 182 157 1 0.7081 0.03027 0.6511 0.770 183 156 1 0.7035 0.03041 0.6464 0.766 186 154 1 0.6989 0.03056 0.6416 0.761 189 152 1 0.6943 0.03070 0.6367 0.757 194 149 1 0.6897 0.03085 0.6318 0.753 197 147 1 0.6850 0.03099 0.6269 0.749 199 145 1 0.6803 0.03113 0.6219 0.744 201 144 2 0.6708 0.03141 0.6120 0.735 202 142 1 0.6661 0.03154 0.6071 0.731 207 139 1 0.6613 0.03168 0.6020 0.726 208 138 1 0.6565 0.03181 0.5970 0.722 210 137 1 0.6517 0.03194 0.5920 0.717 212 135 1 0.6469 0.03206 0.5870 0.713 218 134 1 0.6421 0.03218 0.5820 0.708 222 132 1 0.6372 0.03231 0.5769 0.704 223 130 1 0.6323 0.03243 0.5718 0.699 226 126 1 0.6273 0.03256 0.5666 0.694 229 125 1 0.6223 0.03268 0.5614 0.690 230 124 1 0.6172 0.03280 0.5562 0.685 239 121 2 0.6070 0.03304 0.5456 0.675 245 117 1 0.6019 0.03316 0.5402 0.670 246 116 1 0.5967 0.03328 0.5349 0.666 267 112 1 0.5913 0.03341 0.5294 0.661 268 111 1 0.5860 0.03353 0.5239 0.656 269 110 1 0.5807 0.03364 0.5184 0.651 270 108 1 0.5753 0.03376 0.5128 0.645 283 104 1 0.5698 0.03388 0.5071 0.640 284 103 1 0.5642 0.03400 0.5014 0.635 285 101 2 0.5531 0.03424 0.4899 0.624 286 99 1 0.5475 0.03434 0.4841 0.619 288 98 1 0.5419 0.03444 0.4784 0.614 291 97 1 0.5363 0.03454 0.4727 0.608 293 94 1 0.5306 0.03464 0.4669 0.603 301 91 1 0.5248 0.03475 0.4609 0.597 303 89 1 0.5189 0.03485 0.4549 0.592 305 87 1 0.5129 0.03496 0.4488 0.586 306 86 1 0.5070 0.03506 0.4427 0.581 310 85 2 0.4950 0.03523 0.4306 0.569 320 82 1 0.4890 0.03532 0.4244 0.563 329 81 1 0.4830 0.03539 0.4183 0.558 337 79 1 0.4768 0.03547 0.4121 0.552 340 78 1 0.4707 0.03554 0.4060 0.546 345 77 1 0.4646 0.03560 0.3998 0.540 348 76 1 0.4585 0.03565 0.3937 0.534 350 75 1 0.4524 0.03569 0.3876 0.528 351 74 1 0.4463 0.03573 0.3815 0.522 353 73 2 0.4340 0.03578 0.3693 0.510 361 70 1 0.4278 0.03581 0.3631 0.504 363 69 2 0.4154 0.03583 0.3508 0.492 364 67 1 0.4092 0.03582 0.3447 0.486 371 65 2 0.3966 0.03581 0.3323 0.473 387 60 1 0.3900 0.03582 0.3258 0.467 390 59 1 0.3834 0.03582 0.3193 0.460 394 58 1 0.3768 0.03580 0.3128 0.454 426 55 1 0.3700 0.03580 0.3060 0.447 428 54 1 0.3631 0.03579 0.2993 0.440 429 53 1 0.3563 0.03576 0.2926 0.434 433 52 1 0.3494 0.03573 0.2860 0.427 442 51 1 0.3426 0.03568 0.2793 0.420 444 50 1 0.3357 0.03561 0.2727 0.413 450 48 1 0.3287 0.03555 0.2659 0.406 455 47 1 0.3217 0.03548 0.2592 0.399 457 46 1 0.3147 0.03539 0.2525 0.392 460 44 1 0.3076 0.03530 0.2456 0.385 473 43 1 0.3004 0.03520 0.2388 0.378 477 42 1 0.2933 0.03508 0.2320 0.371 519 39 1 0.2857 0.03498 0.2248 0.363 520 38 1 0.2782 0.03485 0.2177 0.356 524 37 2 0.2632 0.03455 0.2035 0.340 533 34 1 0.2554 0.03439 0.1962 0.333 550 32 1 0.2475 0.03423 0.1887 0.325 558 30 1 0.2392 0.03407 0.1810 0.316 567 28 1 0.2307 0.03391 0.1729 0.308 574 27 1 0.2221 0.03371 0.1650 0.299 583 26 1 0.2136 0.03348 0.1571 0.290 613 24 1 0.2047 0.03325 0.1489 0.281 624 23 1 0.1958 0.03297 0.1407 0.272 641 22 1 0.1869 0.03265 0.1327 0.263 643 21 1 0.1780 0.03229 0.1247 0.254 654 20 1 0.1691 0.03188 0.1169 0.245 655 19 1 0.1602 0.03142 0.1091 0.235 687 18 1 0.1513 0.03090 0.1014 0.226 689 17 1 0.1424 0.03034 0.0938 0.216 705 16 1 0.1335 0.02972 0.0863 0.207 707 15 1 0.1246 0.02904 0.0789 0.197 728 14 1 0.1157 0.02830 0.0716 0.187 731 13 1 0.1068 0.02749 0.0645 0.177 735 12 1 0.0979 0.02660 0.0575 0.167 765 10 1 0.0881 0.02568 0.0498 0.156 791 9 1 0.0783 0.02462 0.0423 0.145 814 7 1 0.0671 0.02351 0.0338 0.133 883 4 1 0.0503 0.02285 0.0207 0.123 mdl = survfit(Surv(time, status) ~ strata(sex), data = lung) summary(mdl) Call: survfit(formula = Surv(time, status) ~ strata(sex), data = lung) strata(sex)=sex=1 time n.risk n.event survival std.err lower 95% CI upper 95% CI 11 138 3 0.9783 0.0124 0.9542 1.000 12 135 1 0.9710 0.0143 0.9434 0.999 13 134 2 0.9565 0.0174 0.9231 0.991 15 132 1 0.9493 0.0187 0.9134 0.987 26 131 1 0.9420 0.0199 0.9038 0.982 30 130 1 0.9348 0.0210 0.8945 0.977 31 129 1 0.9275 0.0221 0.8853 0.972 53 128 2 0.9130 0.0240 0.8672 0.961 54 126 1 0.9058 0.0249 0.8583 0.956 59 125 1 0.8986 0.0257 0.8496 0.950 60 124 1 0.8913 0.0265 0.8409 0.945 65 123 2 0.8768 0.0280 0.8237 0.933 71 121 1 0.8696 0.0287 0.8152 0.928 81 120 1 0.8623 0.0293 0.8067 0.922 88 119 2 0.8478 0.0306 0.7900 0.910 92 117 1 0.8406 0.0312 0.7817 0.904 93 116 1 0.8333 0.0317 0.7734 0.898 95 115 1 0.8261 0.0323 0.7652 0.892 105 114 1 0.8188 0.0328 0.7570 0.886 107 113 1 0.8116 0.0333 0.7489 0.880 110 112 1 0.8043 0.0338 0.7408 0.873 116 111 1 0.7971 0.0342 0.7328 0.867 118 110 1 0.7899 0.0347 0.7247 0.861 131 109 1 0.7826 0.0351 0.7167 0.855 132 108 2 0.7681 0.0359 0.7008 0.842 135 106 1 0.7609 0.0363 0.6929 0.835 142 105 1 0.7536 0.0367 0.6851 0.829 144 104 1 0.7464 0.0370 0.6772 0.823 147 103 1 0.7391 0.0374 0.6694 0.816 156 102 2 0.7246 0.0380 0.6538 0.803 163 100 3 0.7029 0.0389 0.6306 0.783 166 97 1 0.6957 0.0392 0.6230 0.777 170 96 1 0.6884 0.0394 0.6153 0.770 175 94 1 0.6811 0.0397 0.6076 0.763 176 93 1 0.6738 0.0399 0.5999 0.757 177 92 1 0.6664 0.0402 0.5922 0.750 179 91 2 0.6518 0.0406 0.5769 0.736 180 89 1 0.6445 0.0408 0.5693 0.730 181 88 2 0.6298 0.0412 0.5541 0.716 183 86 1 0.6225 0.0413 0.5466 0.709 189 83 1 0.6150 0.0415 0.5388 0.702 197 80 1 0.6073 0.0417 0.5309 0.695 202 78 1 0.5995 0.0419 0.5228 0.687 207 77 1 0.5917 0.0420 0.5148 0.680 210 76 1 0.5839 0.0422 0.5068 0.673 212 75 1 0.5762 0.0424 0.4988 0.665 218 74 1 0.5684 0.0425 0.4909 0.658 222 72 1 0.5605 0.0426 0.4829 0.651 223 70 1 0.5525 0.0428 0.4747 0.643 229 67 1 0.5442 0.0429 0.4663 0.635 230 66 1 0.5360 0.0431 0.4579 0.627 239 64 1 0.5276 0.0432 0.4494 0.619 246 63 1 0.5192 0.0433 0.4409 0.611 267 61 1 0.5107 0.0434 0.4323 0.603 269 60 1 0.5022 0.0435 0.4238 0.595 270 59 1 0.4937 0.0436 0.4152 0.587 283 57 1 0.4850 0.0437 0.4065 0.579 284 56 1 0.4764 0.0438 0.3979 0.570 285 54 1 0.4676 0.0438 0.3891 0.562 286 53 1 0.4587 0.0439 0.3803 0.553 288 52 1 0.4499 0.0439 0.3716 0.545 291 51 1 0.4411 0.0439 0.3629 0.536 301 48 1 0.4319 0.0440 0.3538 0.527 303 46 1 0.4225 0.0440 0.3445 0.518 306 44 1 0.4129 0.0440 0.3350 0.509 310 43 1 0.4033 0.0441 0.3256 0.500 320 42 1 0.3937 0.0440 0.3162 0.490 329 41 1 0.3841 0.0440 0.3069 0.481 337 40 1 0.3745 0.0439 0.2976 0.471 353 39 2 0.3553 0.0437 0.2791 0.452 363 37 1 0.3457 0.0436 0.2700 0.443 364 36 1 0.3361 0.0434 0.2609 0.433 371 35 1 0.3265 0.0432 0.2519 0.423 387 34 1 0.3169 0.0430 0.2429 0.413 390 33 1 0.3073 0.0428 0.2339 0.404 394 32 1 0.2977 0.0425 0.2250 0.394 428 29 1 0.2874 0.0423 0.2155 0.383 429 28 1 0.2771 0.0420 0.2060 0.373 442 27 1 0.2669 0.0417 0.1965 0.362 455 25 1 0.2562 0.0413 0.1868 0.351 457 24 1 0.2455 0.0410 0.1770 0.341 460 22 1 0.2344 0.0406 0.1669 0.329 477 21 1 0.2232 0.0402 0.1569 0.318 519 20 1 0.2121 0.0397 0.1469 0.306 524 19 1 0.2009 0.0391 0.1371 0.294 533 18 1 0.1897 0.0385 0.1275 0.282 558 17 1 0.1786 0.0378 0.1179 0.270 567 16 1 0.1674 0.0371 0.1085 0.258 574 15 1 0.1562 0.0362 0.0992 0.246 583 14 1 0.1451 0.0353 0.0900 0.234 613 13 1 0.1339 0.0343 0.0810 0.221 624 12 1 0.1228 0.0332 0.0722 0.209 643 11 1 0.1116 0.0320 0.0636 0.196 655 10 1 0.1004 0.0307 0.0552 0.183 689 9 1 0.0893 0.0293 0.0470 0.170 707 8 1 0.0781 0.0276 0.0390 0.156 791 7 1 0.0670 0.0259 0.0314 0.143 814 5 1 0.0536 0.0239 0.0223 0.128 883 3 1 0.0357 0.0216 0.0109 0.117 strata(sex)=sex=2 time n.risk n.event survival std.err lower 95% CI upper 95% CI 5 90 1 0.9889 0.0110 0.9675 1.000 60 89 1 0.9778 0.0155 0.9478 1.000 61 88 1 0.9667 0.0189 0.9303 1.000 62 87 1 0.9556 0.0217 0.9139 0.999 79 86 1 0.9444 0.0241 0.8983 0.993 81 85 1 0.9333 0.0263 0.8832 0.986 95 83 1 0.9221 0.0283 0.8683 0.979 107 81 1 0.9107 0.0301 0.8535 0.972 122 80 1 0.8993 0.0318 0.8390 0.964 145 79 2 0.8766 0.0349 0.8108 0.948 153 77 1 0.8652 0.0362 0.7970 0.939 166 76 1 0.8538 0.0375 0.7834 0.931 167 75 1 0.8424 0.0387 0.7699 0.922 182 71 1 0.8305 0.0399 0.7559 0.913 186 70 1 0.8187 0.0411 0.7420 0.903 194 68 1 0.8066 0.0422 0.7280 0.894 199 67 1 0.7946 0.0432 0.7142 0.884 201 66 2 0.7705 0.0452 0.6869 0.864 208 62 1 0.7581 0.0461 0.6729 0.854 226 59 1 0.7452 0.0471 0.6584 0.843 239 57 1 0.7322 0.0480 0.6438 0.833 245 54 1 0.7186 0.0490 0.6287 0.821 268 51 1 0.7045 0.0501 0.6129 0.810 285 47 1 0.6895 0.0512 0.5962 0.798 293 45 1 0.6742 0.0523 0.5791 0.785 305 43 1 0.6585 0.0534 0.5618 0.772 310 42 1 0.6428 0.0544 0.5447 0.759 340 39 1 0.6264 0.0554 0.5267 0.745 345 38 1 0.6099 0.0563 0.5089 0.731 348 37 1 0.5934 0.0572 0.4913 0.717 350 36 1 0.5769 0.0579 0.4739 0.702 351 35 1 0.5604 0.0586 0.4566 0.688 361 33 1 0.5434 0.0592 0.4390 0.673 363 32 1 0.5265 0.0597 0.4215 0.658 371 30 1 0.5089 0.0603 0.4035 0.642 426 26 1 0.4893 0.0610 0.3832 0.625 433 25 1 0.4698 0.0617 0.3632 0.608 444 24 1 0.4502 0.0621 0.3435 0.590 450 23 1 0.4306 0.0624 0.3241 0.572 473 22 1 0.4110 0.0626 0.3050 0.554 520 19 1 0.3894 0.0629 0.2837 0.534 524 18 1 0.3678 0.0630 0.2628 0.515 550 15 1 0.3433 0.0634 0.2390 0.493 641 11 1 0.3121 0.0649 0.2076 0.469 654 10 1 0.2808 0.0655 0.1778 0.443 687 9 1 0.2496 0.0652 0.1496 0.417 705 8 1 0.2184 0.0641 0.1229 0.388 728 7 1 0.1872 0.0621 0.0978 0.359 731 6 1 0.1560 0.0590 0.0743 0.328 735 5 1 0.1248 0.0549 0.0527 0.295 765 3 1 0.0832 0.0499 0.0257 0.270 plot(mdl, conf.int = FALSE, col = c(2,4), main=&quot;Survival Function&quot;, xlab=&quot;Survival Time&quot;, ylab=&quot;Survival Function&quot;) legend(800, 1, c(&quot;men&quot;, &quot;women&quot;), lty = c(1, 1), col = c(2, 4)) summary(survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), lung)) Call: survreg(formula = Surv(time, status) ~ ph.ecog + age + strata(sex), data = lung) Value Std. Error z p (Intercept) 6.73235 0.42396 15.880 8.75e-57 ph.ecog -0.32443 0.08649 -3.751 1.76e-04 age -0.00581 0.00693 -0.838 4.02e-01 sex=1 -0.24408 0.07920 -3.082 2.06e-03 sex=2 -0.42345 0.10669 -3.969 7.22e-05 Scale: sex=1 sex=2 0.783 0.655 Weibull distribution Loglik(model)= -1137.3 Loglik(intercept only)= -1146.2 Chisq= 17.8 on 2 degrees of freedom, p= 0.00014 Number of Newton-Raphson Iterations: 5 n=227 (1 observation deleted due to missingness) "],
["experimental-design.html", "Experimental Design", " Experimental Design Completely Random Design The response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC). Treatment Structure: 2 x 3 Factorial Treatment, both Fixed Model: \\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha \\beta_{ij} + e_{ijk}\\) Treatments: \\(\\alpha_i = \\text{supp, } \\beta_j = \\text{dose}\\) Fixed Effects: \\(\\alpha_1 = \\beta_1 = \\alpha \\beta_{1j} = \\alpha \\beta_{i1} = 0\\) Random Effects: \\(e_{ijk} = N(0, \\sigma^2_e)\\) library(lsmeans) library(reshape2) library(car) library(plyr) data(&quot;ToothGrowth&quot;) ## Data is numeric, but we need to force it to be a factor for the model ToothGrowth$dose.factor = as.factor(ToothGrowth$dose) summary(ToothGrowth) len supp dose dose.factor Min. : 4.20 OJ:30 Min. :0.500 0.5:20 1st Qu.:13.07 VC:30 1st Qu.:0.500 1 :20 Median :19.25 Median :1.000 2 :20 Mean :18.81 Mean :1.167 3rd Qu.:25.27 3rd Qu.:2.000 Max. :33.90 Max. :2.000 ## Even observations per treatment group table(ToothGrowth$supp, ToothGrowth$dose) 0.5 1 2 OJ 10 10 10 VC 10 10 10 ## ANOVA model mdl = lm(len ~ supp * dose.factor, data = ToothGrowth) anova(mdl) Analysis of Variance Table Response: len Df Sum Sq Mean Sq F value Pr(&gt;F) supp 1 205.35 205.35 15.572 0.0002312 *** dose.factor 2 2426.43 1213.22 92.000 &lt; 2.2e-16 *** supp:dose.factor 2 108.32 54.16 4.107 0.0218603 * Residuals 54 712.11 13.19 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Least Squares Means lsmeans(mdl, specs = c(&quot;supp&quot;, &quot;dose.factor&quot;)) supp dose.factor lsmean SE df lower.CL upper.CL OJ 0.5 13.23 1.148353 54 10.927691 15.53231 VC 0.5 7.98 1.148353 54 5.677691 10.28231 OJ 1 22.70 1.148353 54 20.397691 25.00231 VC 1 16.77 1.148353 54 14.467691 19.07231 OJ 2 26.06 1.148353 54 23.757691 28.36231 VC 2 26.14 1.148353 54 23.837691 28.44231 Confidence level used: 0.95 ## Differences in Means TukeyHSD(aov(mdl)) Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = mdl) $supp diff lwr upr p adj VC-OJ -3.7 -5.579828 -1.820172 0.0002312 $dose.factor diff lwr upr p adj 1-0.5 9.130 6.362488 11.897512 0.0e+00 2-0.5 15.495 12.727488 18.262512 0.0e+00 2-1 6.365 3.597488 9.132512 2.7e-06 $`supp:dose.factor` diff lwr upr p adj VC:0.5-OJ:0.5 -5.25 -10.048124 -0.4518762 0.0242521 OJ:1-OJ:0.5 9.47 4.671876 14.2681238 0.0000046 VC:1-OJ:0.5 3.54 -1.258124 8.3381238 0.2640208 OJ:2-OJ:0.5 12.83 8.031876 17.6281238 0.0000000 VC:2-OJ:0.5 12.91 8.111876 17.7081238 0.0000000 OJ:1-VC:0.5 14.72 9.921876 19.5181238 0.0000000 VC:1-VC:0.5 8.79 3.991876 13.5881238 0.0000210 OJ:2-VC:0.5 18.08 13.281876 22.8781238 0.0000000 VC:2-VC:0.5 18.16 13.361876 22.9581238 0.0000000 VC:1-OJ:1 -5.93 -10.728124 -1.1318762 0.0073930 OJ:2-OJ:1 3.36 -1.438124 8.1581238 0.3187361 VC:2-OJ:1 3.44 -1.358124 8.2381238 0.2936430 OJ:2-VC:1 9.29 4.491876 14.0881238 0.0000069 VC:2-VC:1 9.37 4.571876 14.1681238 0.0000058 VC:2-OJ:2 0.08 -4.718124 4.8781238 1.0000000 Are the necessary conditions for hypothesis testing present? Normality: Residuals appear normally distributed per the residual normal reference plot and shapiro-wilks test Equal Variance: Brown-Forsythe-Levene test and residual plot supports equal variance Independence: No correlation in the residuals per the Durbin Watson test and plots of variables against residuals Conditions for hypothesis testing appears to be satisfied ## Normality of Residuals shapiro.test(mdl$residuals); qqnorm(mdl$residuals); qqline(mdl$residuals) Shapiro-Wilk normality test data: mdl$residuals W = 0.98499, p-value = 0.6694 ## Equal Variance of Residuals leveneTest(mdl) Levene&#39;s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 5 1.7086 0.1484 54 plot(x = mdl$fitted.values, y = (mdl$residual - mean(mdl$residuals))/sd(mdl$residuals), xlab = &quot;Fitted Values&quot;, ylab = &quot;Standardized Residuals&quot;, main = &quot;Equal Variance Residual Plot&quot;) ## Independence of Residuals par(mfrow = c(1, 2)) plot(ToothGrowth$dose, mdl$residuals, xlab = &quot;Dose&quot;, ylab = &quot;Residuals&quot;) plot(ToothGrowth$supp, mdl$residuals, xlab = &quot;Supp&quot;, ylab = &quot;Residuals&quot;) ## Test for correlation in the residuals durbinWatsonTest(mdl) lag Autocorrelation D-W Statistic p-value 1 -0.02932541 2.025437 0.596 Alternative hypothesis: rho != 0 Group doses so that each dose is not statistically different than any other dose in the group: The interaction between dose and supp are significant so we need to assess the differences in dose per each level of supp. OJ: {.5}, {1, 2} VC: {.5}, {1}, {2} Group supps so that each supp is not statistically different than any other supp in the group: The interaction is significant so we need to assess the supps at each level of dose .5: {OJ}, {VC} 1: {OJ}, {VC} 2: {OJ, VC} Random Complete Block Design An experiment was conducted to compare four different pre-planting treatments for soybeen seeds. A fifth treatment, consisting of not treating the seeds was used as a control. The experimental area consisted of four fields. There are notable differences in the fields. Each field was divided into five plots and one of the treatments was randomly assigned to a plot within each field. Treatment Structure: 1 Single Treatment with 5 levels Response: The number of plants that failed to emerge out of 100 seeds planted per plot. Model: \\(y_{ij} = \\mu + \\alpha_i + \\beta_j + e_{ij}\\) Treatments: \\(\\alpha_i = \\text{Seed, } \\beta_j = \\text{Field, } \\alpha_5 = \\text{Control}\\) Fixed Effects: \\(\\alpha_5 = \\beta_1 = 0\\) Random Effects: \\(e_{ij} = N(0, \\sigma^2_e)\\) Soy Bean Data Treatment Field.1 Field.2 Field.3 Field.4 Avasan 2 5 7 11 Spergon 4 10 9 8 Semaesan 3 6 9 10 Fermate 9 3 5 5 Control 8 11 12 13 ## Make the control treatment the default level soy$Treatment = relevel(soy$Treatment, ref = &quot;Control&quot;) ## We only have one rep per treatment so there are not enough DF to measure the interaction mdl = lm(Count ~ Field + Treatment, data = soy) anova(mdl) Analysis of Variance Table Response: Count Df Sum Sq Mean Sq F value Pr(&gt;F) Field 3 49.8 16.6000 2.5971 0.10070 Treatment 4 72.5 18.1250 2.8357 0.07227 . Residuals 12 76.7 6.3917 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 0.0.0.1 Comparison of means vs control Since we have a control variable we want to know if any of the treatment means are significantly lower than the control mean. library(multcomp) Dunnet = glht(mdl, linfct = mcp(Treatment = &quot;Dunnet&quot;), alternative = &quot;less&quot;) summary(Dunnet) Simultaneous Tests for General Linear Hypotheses Multiple Comparisons of Means: Dunnett Contrasts Fit: lm(formula = Count ~ Field + Treatment, data = soy) Linear Hypotheses: Estimate Std. Error t value Pr(&lt;t) Avasan - Control &gt;= 0 -4.750 1.788 -2.657 0.0323 * Fermate - Control &gt;= 0 -5.500 1.788 -3.077 0.0156 * Semaesan - Control &gt;= 0 -4.000 1.788 -2.238 0.0669 . Spergon - Control &gt;= 0 -3.250 1.788 -1.818 0.1312 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Adjusted p values reported -- single-step method) We have significant evidence that only Avasan and Fermate are significantly lower than the control. Are they significantly different from each other? TukeyHSD(aov(mdl)) Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = mdl) $Field diff lwr upr p adj Field.2-Field.1 1.8 -2.9471482 6.547148 0.6814523 Field.3-Field.1 3.2 -1.5471482 7.947148 0.2406905 Field.4-Field.1 4.2 -0.5471482 8.947148 0.0895218 Field.3-Field.2 1.4 -3.3471482 6.147148 0.8173180 Field.4-Field.2 2.4 -2.3471482 7.147148 0.4666374 Field.4-Field.3 1.0 -3.7471482 5.747148 0.9219188 $Treatment diff lwr upr p adj Avasan-Control -4.75 -10.448139 0.9481388 0.1206718 Fermate-Control -5.50 -11.198139 0.1981388 0.0603205 Semaesan-Control -4.00 -9.698139 1.6981388 0.2305921 Spergon-Control -3.25 -8.948139 2.4481388 0.4074833 Fermate-Avasan -0.75 -6.448139 4.9481388 0.9926478 Semaesan-Avasan 0.75 -4.948139 6.4481388 0.9926478 Spergon-Avasan 1.50 -4.198139 7.1981388 0.9131542 Semaesan-Fermate 1.50 -4.198139 7.1981388 0.9131542 Spergon-Fermate 2.25 -3.448139 7.9481388 0.7194742 Spergon-Semaesan 0.75 -4.948139 6.4481388 0.9926478 There is not significant evidence between the difference in means between any of the treatments. "],
["contingency-tables.html", "Contingency Tables", " Contingency Tables Table 1: Data for applicant entrance for 6 departments Department Male.Yes Male.No Female.Yes Female.No 1 512 313 89 19 2 353 207 17 8 3 120 205 202 391 4 138 279 131 244 5 53 138 94 299 6 22 351 24 317 dta$OR = with(dta, (Male.Yes * Female.No) / (Male.No * Female.Yes)) dta$se = with(dta, (sqrt(1/Male.Yes + 1/Female.No + 1/Male.No + 1/Female.Yes))) dta$Conf.lwr = with(dta, OR - (1.96 * se)) dta$Conf.Upr = with(dta, OR + (1.96 * se)) kable(dta, split.tables = Inf) Department Male.Yes Male.No Female.Yes Female.No OR se Conf.lwr Conf.Upr 1 512 313 89 19 0.3492120 0.2627081 -0.1656958 0.8641199 2 353 207 17 8 0.8025007 0.4375926 -0.0551808 1.6601823 3 120 205 202 391 1.1330596 0.1439424 0.8509325 1.4151868 4 138 279 131 244 0.9212838 0.1502084 0.6268753 1.2156922 5 53 138 94 299 1.2216312 0.2002426 0.8291558 1.6141066 6 22 351 24 317 0.8278727 0.3051635 0.2297522 1.4259933 marginal = colSums(dta[, 2:5]) OR = (marginal[1] * marginal[4]) / (marginal[2] * marginal[3]) se = sqrt(1/marginal[1] + 1/marginal[4] + 1/marginal[2] + 1/marginal[3]) ## Confidence Interval for the Marginal OR OR + c(-1, 1) * 1.96 * se [1] 1.71585 1.96631 library(DescTools) library(lawstat) dta = xtabs(freq ~ ., cbind(expand.grid(Gender = c(&quot;Male&quot;, &quot;Female&quot;), Entrace = c(&quot;Yes&quot;, &quot;No&quot;), Department = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;)), freq = c(512, 89, 313, 19, 353, 17, 207, 8, 120, 202, 205, 391, 138, 131, 279, 244, 53, 94, 138, 299, 22, 24, 351, 317) ) ) ## Ho: OR = 1, Ha: OR &gt; 1 BreslowDayTest(dta, OR = 1) Breslow-Day test on Homogeneity of Odds Ratios data: dta X-squared = 19.938, df = 5, p-value = 0.001283 ## Ho: OR_1 = OR_2 = OR_3 = OR_4 = OR_5 = OR_6, Ha: At least one set of OR are not equal cmh.test(dta) Cochran-Mantel-Haenszel Chi-square Test data: dta CMH statistic = 1.52460, df = 1.00000, p-value = 0.21692, MH Estimate = 0.90470, Pooled Odd Ratio = 1.84110, Odd Ratio of level 1 = 0.34921, Odd Ratio of level 2 = 0.80250, Odd Ratio of level 3 = 1.13310, Odd Ratio of level 4 = 0.92128, Odd Ratio of level 5 = 1.22160, Odd Ratio of level 6 = 0.82787 Based on the Breslow Day test we reject the null hypothesis that the odds ratios are equal to 1. The CMH test fails to reject that gender and entrance are independent. "],
["principal-components.html", "Principal Components", " Principal Components ## Generate some data library(mvtnorm) mu = rep(10, 6) cov = matrix(nrow = 6, byrow = TRUE, c(1, 0, 1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 0, 2, 1, 3, 2, 1, 1, 1, 1, 2, 4, 1, 1, 1, 0, 1, 1, 10) ) set.seed(1000) X = rmvnorm(100, mu, cov) colnames(X) = paste(&quot;x&quot;, 1:6, sep = &quot;&quot;) ## How do the data correlate? round(cor(X), 3) x1 x2 x3 x4 x5 x6 x1 1.000 -0.100 0.671 0.013 0.544 0.162 x2 -0.100 1.000 0.464 0.816 0.314 0.179 x3 0.671 0.464 1.000 0.507 0.462 -0.160 x4 0.013 0.816 0.507 1.000 0.582 0.051 x5 0.544 0.314 0.462 0.582 1.000 -0.044 x6 0.162 0.179 -0.160 0.051 -0.044 1.000 pairs(X) summary(X) x1 x2 x3 x4 Min. : 8.200 Min. : 6.259 Min. : 7.135 Min. : 6.210 1st Qu.: 9.313 1st Qu.: 9.209 1st Qu.: 9.045 1st Qu.: 8.927 Median : 9.828 Median :10.253 Median :10.169 Median :10.089 Mean : 9.975 Mean :10.109 Mean :10.087 Mean :10.084 3rd Qu.:10.614 3rd Qu.:11.006 3rd Qu.:11.007 3rd Qu.:11.262 Max. :12.297 Max. :13.834 Max. :13.563 Max. :14.018 x5 x6 Min. : 6.399 Min. : 3.260 1st Qu.: 8.421 1st Qu.: 8.009 Median :10.153 Median : 9.771 Mean : 9.941 Mean : 9.869 3rd Qu.:11.230 3rd Qu.:11.926 Max. :14.687 Max. :16.130 ## Build the principal components. We do not need to standardize the data ## since all of the variables are roughly the same scale. (pr = prcomp(X)) Standard deviations (1, .., p=6): [1] 3.017891e+00 2.563572e+00 1.517959e+00 1.123358e+00 5.274371e-01 [6] 1.472533e-08 Rotation (n x k) = (6 x 6): PC1 PC2 PC3 PC4 PC5 PC6 x1 0.04204824 0.13561610 -0.43777882 -0.40016623 -0.02461578 -0.79211803 x2 0.10306755 0.37620383 0.53544098 -0.09946666 0.71557069 -0.19802951 x3 -0.06111086 0.35286917 -0.07161650 -0.78263310 -0.09490671 0.49507377 x4 0.05321194 0.56786999 0.42326271 0.16724994 -0.65448653 -0.19802951 x5 -0.01790754 0.62553452 -0.57601203 0.43473997 0.21995796 0.19802951 x6 0.99031424 -0.04233845 -0.07471658 -0.02207772 -0.04014029 0.09901475 ## Principal components rotation matrix is actually the same as the eigen vectors eigen(cov(X))$vectors [,1] [,2] [,3] [,4] [,5] [1,] 0.04204824 0.13561610 0.43777882 0.40016623 0.02461578 [2,] 0.10306755 0.37620383 -0.53544098 0.09946666 -0.71557069 [3,] -0.06111086 0.35286917 0.07161650 0.78263310 0.09490671 [4,] 0.05321194 0.56786999 -0.42326271 -0.16724994 0.65448653 [5,] -0.01790754 0.62553452 0.57601203 -0.43473997 -0.21995796 [6,] 0.99031424 -0.04233845 0.07471658 0.02207772 0.04014029 [,6] [1,] 0.79211803 [2,] 0.19802951 [3,] -0.49507377 [4,] 0.19802951 [5,] -0.19802951 [6,] -0.09901475 ## Build the principal components from the coefficients X.pca = X %*% pr$rotation ## The variance of the principal components is equal to the eigen values eigen(cov(X))$values [1] 9.107665e+00 6.571900e+00 2.304199e+00 1.261933e+00 2.781899e-01 [6] -5.334563e-16 diag(var(X.pca)) PC1 PC2 PC3 PC4 PC5 9.107665e+00 6.571900e+00 2.304199e+00 1.261933e+00 2.781899e-01 PC6 2.168355e-16 ## PCA Summary ## The first two PCA account for 80% of all variation in the data summary(pr) Importance of components: PC1 PC2 PC3 PC4 PC5 PC6 Standard deviation 3.0179 2.5636 1.5180 1.12336 0.52744 1.473e-08 Proportion of Variance 0.4665 0.3366 0.1180 0.06464 0.01425 0.000e+00 Cumulative Proportion 0.4665 0.8031 0.9211 0.98575 1.00000 1.000e+00 X.pca = data.frame(X.pca) ## plot of the first two PCA plot(X.pca$PC1, X.pca$PC2) "],
["eigen-values-and-statistical-distance.html", "Eigen Values and Statistical Distance", " Eigen Values and Statistical Distance ## Eigen values and vectors are used to describe a positie definate covariance matrix. (Cov = matrix(c(1, 0, 0, 2), nrow = 2)) [,1] [,2] [1,] 1 0 [2,] 0 2 eig = eigen(Cov) ## eigen values (lambda = eig$values) [1] 2 1 ## eigen vectors (ee = eig$vectors) [,1] [,2] [1,] 0 -1 [2,] 1 0 ## Spectural decompsotion allows you to reconstruct a matrix using only the eigen ## values and vectors lambda[1] * ee[,1] %*% t(ee[,1]) + lambda[2] * ee[,2] %*% t(ee[,2]) [,1] [,2] [1,] 1 0 [2,] 0 2 ## Straight line distance (Euclidean) vs Statistical Distance ## Straight line distance to the origin using point(1, 1) sqrt((1 - 0)^2 + (1 - 0)^2) [1] 1.414214 # Statistical Distance to the origin sqrt((1 - 0)^2/1 + (1 - 0)^2/5) [1] 1.095445 ## Generating Multivariate Normal Data library(mvtnorm) ## set up parameters, 2 means use the covariance matrix from earlier mu = c(10, 20) ## generate a large dataset set.seed(1000) X = rmvnorm(1000, mu, Cov) ## Correlation of X, should be close to 0 cor(X) [,1] [,2] [1,] 1.0000000 -0.0217756 [2,] -0.0217756 1.0000000 ## Calculate the distance between each point and the means distance = c() for (i in 1:nrow(X)) { x = t(X[i, ] - colMeans(X)) %*% solve(cov(X)) %*% (X[i, ] - colMeans(X)) distance = c(distance, x) } ## distances head(distance) [1] 1.7095871 0.4531800 0.7819858 0.7832673 1.9309843 1.3017149 ## What is the distance that captures 95% of all points generated from the distribution? (critical.value = qchisq(.95, 2)) [1] 5.991465 ## What is the proportion of points that fall within this distance? ## As n increases the proportion should converge on 5% length(which((distance - critical.value) &gt; 0))/length(distance) [1] 0.051 ## plot a 95% confidence ellipse for the generated data library(plotrix) plot(x = c(7,13), y = c(15,24), type = &quot;n&quot;, xlab = expression(x[1]), ylab = expression(x[2]), main = &quot;95% Confidence Ellipsoid&quot;) points(X[, 1], X[, 2]) abline(h = 20, v = 10, lty = 2, lwd = 2, col = &quot;red&quot;) draw.ellipse(10, 20, sqrt(critical.value * lambda[1]), sqrt(critical.value * lambda[2]), ## convert radians to degrees angle = acos(abs(ee[1,1])) * 57.2957795, border = 1, lwd = 2) "],
["regression-methods.html", "Regression Methods ", " Regression Methods "],
["matrix-regression.html", "Matrix Regression", " Matrix Regression library(mvtnorm) ## Covariance matrix for random data A = matrix(c(3, 1.5, 1.5, 3), nrow = 2) ## number of observations to generate n = 10 ## Generate data set.seed(1123) (dta = rmvnorm(n = n, mean = c(10, 20), sigma = A)) [,1] [,2] [1,] 11.675350 22.16815 [2,] 11.164364 18.89734 [3,] 12.400876 20.94524 [4,] 10.357397 21.56407 [5,] 10.109067 18.25317 [6,] 10.444637 19.84917 [7,] 10.451464 21.36056 [8,] 8.763568 19.40245 [9,] 8.923162 22.68341 [10,] 7.600346 18.09957 ## Create Y vector from random data (Y = dta[, 1]) [1] 11.675350 11.164364 12.400876 10.357397 10.109067 10.444637 10.451464 [8] 8.763568 8.923162 7.600346 ## Create design matrix (X = as.matrix(data.frame(&quot;b&quot; = rep(1, n), &quot;x&quot; = dta[, 2]))) b x [1,] 1 22.16815 [2,] 1 18.89734 [3,] 1 20.94524 [4,] 1 21.56407 [5,] 1 18.25317 [6,] 1 19.84917 [7,] 1 21.36056 [8,] 1 19.40245 [9,] 1 22.68341 [10,] 1 18.09957 ## Beta B = solve(t(X) %*% X) %*% t(X) ## Solve coefficients (B %*% Y) [,1] b 4.0586986 x 0.3016549 ## Verify results (mdl = lm(dta[,1] ~ dta[,2])) Call: lm(formula = dta[, 1] ~ dta[, 2]) Coefficients: (Intercept) dta[, 2] 4.0587 0.3017 ## Hat Matrix ## Projected onto Y will give you Y-hat ## Diagonals of Hat Matrix are leverage H = X %*% B diag(H) [1] 0.2401398 0.1835194 0.1159607 0.1634227 0.2760978 0.1092077 0.1443377 [8] 0.1348034 0.3292979 0.3032130 ## Sum of squares X SXX = sum((dta[, 2] - colMeans(dta)[2])^2) ## calculate leverage manually ## values greater than 4/n are considered high leverage ## for multiple regression Hii &gt; 2 + (p + 1)/n are considered high leverage 1/n + (dta[, 2] - colMeans(dta)[2])^2 / SXX [1] 0.2401398 0.1835194 0.1159607 0.1634227 0.2760978 0.1092077 0.1443377 [8] 0.1348034 0.3292979 0.3032130 ## verify results hatvalues(mdl) 1 2 3 4 5 6 7 0.2401398 0.1835194 0.1159607 0.1634227 0.2760978 0.1092077 0.1443377 8 9 10 0.1348034 0.3292979 0.3032130 ## Project Hat Matrix on to Y to get Y-hat H %*% Y [,1] [1,] 10.745830 [2,] 9.759172 [3,] 10.376933 [4,] 10.563605 [5,] 9.564856 [6,] 10.046298 [7,] 10.502215 [8,] 9.911541 [9,] 10.901259 [10,] 9.518521 ## Verify results predict(mdl, data.frame(dta)) 1 2 3 4 5 6 7 10.745830 9.759172 10.376933 10.563605 9.564856 10.046298 10.502215 8 9 10 9.911541 10.901259 9.518521 ## MSE of estimate (mse = sqrt(diag(anova(mdl)[[3]][2] * solve(t(X) %*% X)))) b x 5.90855 0.28989 ## 95% confidence interval for X mdl$coefficients[1] + c(-1, 1) * mse[1] * qt(1 - .05/2, df = n - 2) [1] -9.566442 17.683839 mdl$coefficients[2] + c(-1, 1) * mse[2] * qt(1 - .05/2, df = n - 2) [1] -0.3668327 0.9701424 ## Check confidence interval confint(mdl) 2.5 % 97.5 % (Intercept) -9.5664418 17.6838390 dta[, 2] -0.3668327 0.9701424 ## Calculate R^2 ## SST is also SYY SST = sum((dta[, 1] - colMeans(dta)[1])^2) SSReg = sum((predict(mdl, data.frame(dta)) - colMeans(dta)[1])^2) ## R^2 SSReg / SST [1] 0.1192158 ## Adjusted R^2 1 - (sum(mdl$residuals^2)/(n - 2))/(SST/(n - 1)) [1] 0.009117763 summary(mdl) Call: lm(formula = dta[, 1] ~ dta[, 2]) Residuals: Min 1Q Median 3Q Max -1.9781 -0.9125 0.1738 0.8332 2.0239 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.0587 5.9085 0.687 0.512 dta[, 2] 0.3017 0.2899 1.041 0.328 Residual standard error: 1.429 on 8 degrees of freedom Multiple R-squared: 0.1192, Adjusted R-squared: 0.009118 F-statistic: 1.083 on 1 and 8 DF, p-value: 0.3285 "],
["logistic-regression.html", "Logistic Regression", " Logistic Regression library(popbio) library(Deducer) dta = read.csv(&quot;data/pros.csv&quot;) head(dta) ID CAPSULE AGE RACE DPROS DCAPS PSA VOL GLEASON 1 1 0 65 1 2 1 1.4 0.0 6 2 2 0 72 1 3 2 6.7 0.0 7 3 3 0 70 1 1 2 4.9 0.0 6 4 4 0 76 2 2 1 51.2 20.0 7 5 5 0 69 1 1 1 12.3 55.9 6 6 6 1 71 1 3 2 3.3 0.0 8 mdl = glm(CAPSULE ~ AGE + RACE + DPROS + DCAPS + PSA + VOL + GLEASON, family = binomial(), data = dta) summary(mdl) Call: glm(formula = CAPSULE ~ AGE + RACE + DPROS + DCAPS + PSA + VOL + GLEASON, family = binomial(), data = dta) Deviance Residuals: Min 1Q Median 3Q Max -2.4058 -0.7772 -0.4416 0.9019 2.3771 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -6.915650 1.728754 -4.000 6.32e-05 *** AGE -0.015825 0.019517 -0.811 0.41745 RACE -0.695258 0.476106 -1.460 0.14421 DPROS 0.551597 0.136545 4.040 5.35e-05 *** DCAPS 0.479916 0.463728 1.035 0.30071 PSA 0.027196 0.009922 2.741 0.00613 ** VOL -0.010840 0.007753 -1.398 0.16204 GLEASON 0.972442 0.167259 5.814 6.10e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 506.59 on 375 degrees of freedom Residual deviance: 377.31 on 368 degrees of freedom (4 observations deleted due to missingness) AIC: 393.31 Number of Fisher Scoring iterations: 5 logi.hist.plot(sqrt(dta$PSA), dta$CAPSULE, logi.mod = 1, boxp = FALSE, notch = FALSE, main = &quot;Logistic Regression plot for PSA&quot;, xlab = &quot;PSA&quot;) rocplot(mdl) "],
["multinomial-logistic-regression.html", "Multinomial Logistic Regression", " Multinomial Logistic Regression library(nnet) library(devtools) library(lattice) ## Using the iris dataset summary(iris) Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Species setosa :50 versicolor:50 virginica :50 ## Build a multinomial logistic model from the nnet package mdl = multinom(Species ~ ., data = iris) # weights: 18 (10 variable) initial value 164.791843 iter 10 value 16.177348 iter 20 value 7.111438 iter 30 value 6.182999 iter 40 value 5.984028 iter 50 value 5.961278 iter 60 value 5.954900 iter 70 value 5.951851 iter 80 value 5.950343 iter 90 value 5.949904 iter 100 value 5.949867 final value 5.949867 stopped after 100 iterations summary(mdl) Call: multinom(formula = Species ~ ., data = iris) Coefficients: (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width versicolor 18.69037 -5.458424 -8.707401 14.24477 -3.097684 virginica -23.83628 -7.923634 -15.370769 23.65978 15.135301 Std. Errors: (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width versicolor 34.97116 89.89215 157.0415 60.19170 45.48852 virginica 35.76649 89.91153 157.1196 60.46753 45.93406 Residual Deviance: 11.89973 AIC: 31.89973 ## Prediction based on Sepal and Petal values sample = data.frame(Sepal.Length = 5.5, Sepal.Width = 3, Petal.Length = 2.5, Petal.Width = 2) round(predict(mdl, sample, type = &quot;probs&quot;)) setosa versicolor virginica 1 0 0 ## Scatterplot Matrix splom(iris[, 1:4], col = 1:3, panel = function(x, y, i, j, groups, ...) { panel.points(x, y, col = iris$Species) panel.points(sample[1, j], sample[1, i], col = &#39;blue&#39;, pch = 16) }, auto.key = TRUE) "],
["poisson-regression.html", "Poisson Regression", " Poisson Regression library(MASS) dta = read.csv(&quot;data/pharynx.csv&quot;) head(dta) CASE INST SEX TX GRADE AGE COND SITE T_STAGE N_STAGE ENTRY_DT STATUS 1 1 2 2 1 1 51 1 2 3 1 2468 1 2 2 2 1 2 1 65 1 4 2 3 2968 1 3 3 2 1 1 2 64 2 1 3 3 3368 1 4 4 2 1 1 1 73 1 1 4 0 5768 1 5 5 5 1 2 2 64 1 1 4 3 9568 1 6 6 4 1 2 1 61 1 2 3 0 10668 0 TIME 1 631 2 270 3 327 4 243 5 916 6 1823 ## Poisson Regression Model for Survival Time mdl = glm(TIME ~ SEX + TX + AGE + COND + SITE + T_STAGE + N_STAGE + STATUS, family = poisson(), data = dta) ## Negative Binomial Model for Survival Time mdl2 = glm.nb(TIME ~ SEX + TX + AGE + COND + SITE + T_STAGE + N_STAGE + STATUS, maxit = 100, data = dta) ## Dispersion paramater &gt; 0 so poisson is not appropriate, negative binomial is a better model par(mfrow = c(2,2)) plot(mdl2) ## Remove the outlier observation and retrain dta = dta[-159, ] mdl.nb = glm.nb(TIME ~ SEX + TX + AGE + COND + SITE + T_STAGE + N_STAGE + STATUS, maxit = 100, data = dta) summary(mdl.nb) Call: glm.nb(formula = TIME ~ SEX + TX + AGE + COND + SITE + T_STAGE + N_STAGE + STATUS, data = dta, maxit = 100, init.theta = 3.121080189, link = log) Deviance Residuals: Min 1Q Median 3Q Max -3.3855 -0.8138 -0.2142 0.4034 2.2540 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 7.646448 0.369792 20.678 &lt;2e-16 *** SEX 0.142239 0.098324 1.447 0.1480 TX -0.140805 0.082706 -1.702 0.0887 . AGE 0.006148 0.003802 1.617 0.1059 COND -0.510610 0.076821 -6.647 3e-11 *** SITE -0.006501 0.034988 -0.186 0.8526 T_STAGE -0.119522 0.053337 -2.241 0.0250 * N_STAGE -0.058860 0.036701 -1.604 0.1088 STATUS -0.886395 0.094450 -9.385 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(3.1211) family taken to be 1) Null deviance: 385.38 on 193 degrees of freedom Residual deviance: 205.16 on 185 degrees of freedom AIC: 2704.2 Number of Fisher Scoring iterations: 1 Theta: 3.121 Std. Err.: 0.306 2 x log-likelihood: -2684.213 plot(mdl.nb) "],
["clustering-methods.html", "Clustering Methods ", " Clustering Methods "],
["kmeans-clustering.html", "Kmeans Clustering", " Kmeans Clustering ## The famous iris dataset data(iris) flower = iris head(flower) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa summary(flower) Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Species setosa :50 versicolor:50 virginica :50 ## Kmeans requires you to predetermine the number of clusters mdl = kmeans(flower[, 1:4], centers = 3) flower$cluster = mdl$cluster ## plot kmeans for a few of the variable combinations par(mfrow = c(2, 2)) plot(x = flower$Sepal.Length, y = flower$Sepal.Width, col = factor(flower$Species), main = &quot;Actual&quot;) plot(x = flower$Sepal.Length, y = flower$Sepal.Width, col = factor(flower$cluster), main = &quot;Model&quot;) plot(x = flower$Petal.Length, y = flower$Petal.Width, col = factor(flower$Species), main = &quot;Actual&quot;) plot(x = flower$Petal.Length, y = flower$Petal.Width, col = factor(flower$cluster), main = &quot;Model&quot;) ## Try using principal components to draw one picture that catures all of the variance pc = prcomp(flower[, 1:4], cor = TRUE) summary(pc) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 ## The first 2 principal components explains 97% of all variation flower.pc = as.data.frame(as.matrix(flower[, 1:4]) %*% pc$rotation) flower.pc$cluster = flower$cluster flower.pc$species = flower$Species ## How well did the model do? par(mfrow = c(2, 1)) plot(flower.pc$PC1, flower.pc$PC2, col = factor(flower.pc$cluster), main = &quot;Model&quot;) plot(flower.pc$PC1, flower.pc$PC2, col = factor(flower.pc$species), main = &quot;Actual&quot;) "],
["hierarchical-clustering.html", "Hierarchical Clustering", " Hierarchical Clustering library(cluster) ## Hierarchical clustering using the mtcars dataset ## ## Data description from R: ## The data was extracted from the 1974 Motor Trend US magazine, and comprises ## fuel consumption and 10 aspects of automobile design and performance for ## 32 automobiles (1973-74 models). ## ## Since the attributes have different scales we need to standardize them cars.dist = dist(scale(mtcars, center = TRUE, scale = TRUE)) ## Clustering by rows (cars) ## Method Average, average distance between all points in a cluster plot(hclust(cars.dist, method = &quot;average&quot;), xlab = &quot;&quot;, ylab =&quot;Distance&quot;) ## Method Single, shortest distance between each cluster plot(hclust(cars.dist, method = &quot;single&quot;), xlab = &quot;&quot;, ylab =&quot;Distance&quot;) ## Method Complete, longest distance between each cluster plot(hclust(cars.dist, method = &quot;complete&quot;), xlab = &quot;&quot;, ylab =&quot;Distance&quot;) ## Method Ward, minimizes the loss of exmplained variance plot(hclust(cars.dist, method = &quot;ward.D&quot;), xlab = &quot;&quot;, ylab =&quot;Distance&quot;) ## Attribute Descriptions: ## ## mpg Miles/(US) gallon ## cyl Number of cylinders ## disp Displacement (cu.in.) ## hp Gross horsepower ## drat Rear axle ratio ## wt Weight (lb/1000) ## qsec 1/4 mile time ## vs V/S ## am Transmission (0 = automatic, 1 = manual) ## gear Number of forward gears ## carb Number of carburetors ## Data and Summary mtcars; summary(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 mpg cyl disp hp Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 Median :19.20 Median :6.000 Median :196.3 Median :123.0 Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 drat wt qsec vs Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 Median :3.695 Median :3.325 Median :17.71 Median :0.0000 Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 am gear carb Min. :0.0000 Min. :3.000 Min. :1.000 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 Median :0.0000 Median :4.000 Median :2.000 Mean :0.4062 Mean :3.688 Mean :2.812 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 Max. :1.0000 Max. :5.000 Max. :8.000 "],
["forecasting.html", "Forecasting ", " Forecasting "],
["basic-graphical-methods-for-time-series.html", "Basic Graphical Methods for Time Series", " Basic Graphical Methods for Time Series library(forecast) library(lubridate) data(&quot;AirPassengers&quot;) ## Plot of Air Passengers with increasing variance plot(AirPassengers, main = &quot;Air Passengers 1949 to 1961&quot;, ylab = &quot;Passengers&quot;) ## Seasonal plot by year seasonplot(AirPassengers, s = 12, main = &quot;Seasonal Plot&quot;, ylab = &quot;Passengers&quot;, year.labels = TRUE, col = 1:12) ## Seasonal subseries plot monthplot(AirPassengers, ylab = &quot;Passengers&quot;, main = &quot;Seasonal Deviation&quot;, labels = month.abb) ## Autocorrelation acf(AirPassengers, main = &quot;Autocorrelation&quot;) Simple Forecasting Methods ## Average Method method.avg = meanf(AirPassengers, h = 36) ## Naive Method method.naive = naive(AirPassengers, h = 36) ## Seasonal Naive method.snaive = snaive(AirPassengers, h = 36) ## Random Walk with Drift method.rwf = rwf(AirPassengers, h = 36, drift = TRUE) ## Plot of the different methods plot(AirPassengers, main = &quot;Air Passengers 1949 to 1961&quot;, xlim = c(1949, 1964)) lines(method.avg$mean, col = 2) lines(method.naive$mean, col = 3) lines(method.snaive$mean, col = 4) lines(method.rwf$mean, col = 5) legend(&quot;topleft&quot;, lty = 1, col = c(2, 3, 4, 5), legend = c(&quot;Average&quot;, &quot;Naive&quot;, &quot;Seasonal Naive&quot;, &quot;Random Walk&quot;)) Stabalizing Variance ## Box Cox Transformation (lambda = BoxCox.lambda(AirPassengers)) [1] -0.2947156 ## Remove variation from months, create a data.frame first Air = data.frame(Passengers = AirPassengers) Air$Date = seq.Date(from = as.Date(&quot;1949-01-01&quot;), by = &quot;month&quot;, length.out = 144) ## Calculate days per month Air$Days.per.month = days_in_month(Air$Date) ## Calculate average number of passengers per month removing variation due to number of days Air$month.avg = with(Air, Passengers / Days.per.month) ## Plots of original data and monthly average under the Box Cox Transformation par(mfrow = c(2, 1)) plot(BoxCox(Air$Passengers, lambda), ylab = &quot;Passengers&quot;, main = &quot;Air Passengers (BCT)&quot;) plot(BoxCox(Air$month.avg, BoxCox.lambda(Air$month.avg)), ylab = &quot;Passengers&quot;, main = &quot;Passengers Per Month (BCT)&quot;) ## Decompose average monthly passengers plot(decompose(Air$month.avg)) ## Decompose average monthly passenegers under Box Cox ## Results better random error plot(decompose(BoxCox(Air$month.avg, BoxCox.lambda(Air$month.avg))), ylab = &quot;Passengers&quot;) "],
["generate-time-series-data.html", "Generate Time Series Data", " Generate Time Series Data 0.0.1 Manually Generated Series library(forecast) ## Set parameters n = 500 w = rnorm(n, sd = 1) x.t = 0 ## graphical parameters par(mfrow = c(1,2)) ## White Noise for (i in 2:n) x.t[i] = w[i] plot.ts(x.t, main = &quot;White Noise&quot;) Acf(x.t, main = &quot;ACF Plot&quot;) ## Random Walk, AR(1) for (i in 2:n) x.t[i] = x.t[i-1] + w[i] plot.ts(x.t, main = &quot;AR(1) Random Walk&quot;) ## Random Walk, AR(2) for (i in 3:n) x.t[i] = .5*x.t[i-1] - .4*x.t[i-2] + w[i] plot.ts(x.t, main = &quot;AR(2)&quot;) ## MA(1) for (i in 2:n) x.t[i] = w[i] + w[i-1] plot.ts(x.t, main = &quot;MA(1)&quot;) for (i in 3:n) x.t[i] = w[i] + w[i-1] + w[i-2] plot.ts(x.t, main = &quot;MA(2)&quot;) ## graphical parameters par(mfrow = c(1,1)) Auto Generated Series ## Generated Time Series plot(arima.sim(n = n, model = list(order = c(0, 0, 0))), main = &quot;White Noise&quot;, ylab = &quot;&quot;) par(mfrow = c(2, 2)) ## AR(1) (correlation between observations) p(h) = phi^h for (i in c(-.9, .01, .7, .9)) { plot(arima.sim(n = n, model = list(ar = i)), main = bquote(AR(1)~~~phi==.(i)), ylab = &quot;&quot;) } ## AR(2) plot(arima.sim(n = n, model = list(order = c(2, 0, 0), ar = c(.2, .1))), main = &quot;AR(2)&quot;, ylab = &quot;&quot;) ## AR(2) with drift plot(arima.sim(n = n, model = list(order = c(2, 0, 0), ar = c(.2, .1), d = 1.5)), main = &quot;AR(2) with drift&quot;, ylab = &quot;&quot;) ## MA(3) plot(arima.sim(n = n, model = list(order = c(0, 0, 3), ma = c(.3, .3, .3))), main = &quot;MA(3)&quot;, ylab = &quot;&quot;) ## MA(3) with drift plot(arima.sim(n = n, model = list(order = c(0, 0, 3), ma = c(.1, .8, .1), d = 2)), main = &quot;MA(3) with drift&quot;, ylab = &quot;&quot;) ## ARMA(2,3) plot(arima.sim(n = n, model = list(order = c(2, 0, 3), ar = c(.4, .2), ma = c(.1, .8, .1))), main = &quot;ARMA(2,3)&quot;, ylab = &quot;&quot;) ## ARIMA(2,1,3) plot(arima.sim(n = n, model = list(order = c(2, 1, 3), ar = c(.4, .2), ma = c(.1, .8, .1))), main = &quot;ARIMA(2,1,3)&quot;, ylab = &quot;&quot;) "],
["machine-learning.html", "Machine Learning ", " Machine Learning "],
["0-1-neural-network.html", "0.1 Neural Network", " 0.1 Neural Network ## Required Packages library(neuralnet) library(ggplot2) library(nnet) library(gridExtra) ## Set up parameters for the NN n = 10000 size = 50 maxit = 500 ## Generate Random Data fron the Uniform Distribution set.seed(10) x = runif(n, min = -1, max = 1) y = runif(n, min = -1, max = 1) dt = data.frame(x, y) ## Quick Plot of Data qplot(x = x, y = y, geom = &quot;point&quot;, data = dt, main = &quot;Data Set&quot;) ## Calculate the distance to the origin dt$dto = with(dt, sqrt(x^2 + y^2)) ## Create a bullseye basid on distance to origin dt$code = 0 dt$code[dt$dto &lt; .8] = 1 dt$code[dt$dto &lt; .5] = 0 dt$code[dt$dto &lt; .15] = 1 ## Code the indicator dt$code2 = class.ind(dt$code) ## Create a custom plot theme theme_jeb = function() { theme(panel.background = element_blank(), panel.border = element_rect(color = &quot;gray&quot;, fill = NA), panel.grid = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), plot.title = element_text(face = &quot;bold&quot;)) } ## Main plot of the testing set g1 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Coded Data Set&quot;) + theme_jeb() g1 ## Train the NN from 50 random samples of the data set samp = sample(n, 50, replace = FALSE) mdl = nnet(code2 ~ x + y, data = dt[samp,], size = size, softmax = TRUE, maxit = maxit, trace = FALSE) dt$out = factor(predict(mdl, dt, type = &quot;class&quot;)) g2 = ggplot(dt[samp, ], aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Training Set N=50&quot;) + theme_jeb() g3 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = out)) + scale_color_discrete(guide = FALSE) + ggtitle(paste(&quot;Model: N=50,&quot;, &quot;Error=&quot;, length(which(as.numeric(dt$out)-as.numeric(dt$code) == 0))/n)) + theme_jeb() grid.arrange(g2, g3, nrow = 1) ## 100 random samples samp = sample(n, 100, replace = FALSE) mdl = nnet(code2 ~ x + y, data = dt[samp,], size = size, softmax = TRUE, maxit = maxit, trace = FALSE) dt$out.100 = factor(predict(mdl, dt, type = &quot;class&quot;)) g4 = ggplot(dt[samp, ], aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Training Set N=100&quot;) + theme_jeb() g5 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = out.100)) + scale_color_discrete(guide = FALSE) + ggtitle(paste(&quot;Model: N=100,&quot;, &quot;Error=&quot;, length(which(as.numeric(dt$out.100)-as.numeric(dt$code) == 0))/n)) + theme_jeb() grid.arrange(g4, g5, nrow = 1) ## 500 random samples samp = sample(n, 500, replace = FALSE) mdl = nnet(code2 ~ x + y, data = dt[samp,], size = size, softmax = TRUE, maxit = maxit, trace = FALSE) dt$out.500 = factor(predict(mdl, dt, type = &quot;class&quot;)) g6 = ggplot(dt[samp, ], aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Training Set N=500&quot;) + theme_jeb() g7 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = out.500)) + scale_color_discrete(guide = FALSE) + ggtitle(paste(&quot;Model: N=500,&quot;, &quot;Error=&quot;, length(which(as.numeric(dt$out.500)-as.numeric(dt$code) == 0))/n)) + theme_jeb() grid.arrange(g6, g7, nrow = 1) ## 1000 random samples samp = sample(n, 1000, replace = FALSE) mdl = nnet(code2 ~ x + y, data = dt[samp,], size = size, softmax = TRUE, maxit = maxit, trace = FALSE) dt$out.1000 = factor(predict(mdl, dt, type = &quot;class&quot;)) g8 = ggplot(dt[samp, ], aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Training Set N=1000&quot;) + theme_jeb() g9 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = out.1000)) + scale_color_discrete(guide = FALSE) + ggtitle(paste(&quot;Model: N=1000,&quot;, &quot;Error=&quot;, length(which(as.numeric(dt$out.1000)-as.numeric(dt$code) == 0))/n)) + theme_jeb() grid.arrange(g8, g9, nrow = 1) ## 2500 random samples samp = sample(n, 2500, replace = FALSE) mdl = nnet(code2 ~ x + y, data = dt[samp,], size = size, softmax = TRUE, maxit = maxit, trace = FALSE) dt$out.2500 = factor(predict(mdl, dt, type = &quot;class&quot;)) g10 = ggplot(dt[samp, ], aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Training Set N=2500&quot;) + theme_jeb() g11 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = out.2500)) + scale_color_discrete(guide = FALSE) + ggtitle(paste(&quot;Model: N=2500,&quot;, &quot;Error=&quot;, length(which(as.numeric(dt$out.2500)-as.numeric(dt$code) == 0))/n)) + theme_jeb() grid.arrange(g10, g11, nrow = 1) ## 5000 random samples samp = sample(n, 5000, replace = FALSE) mdl = nnet(code2 ~ x + y, data = dt[samp,], size = size, softmax = TRUE, maxit = maxit, trace = FALSE) dt$out.5000 = factor(predict(mdl, dt, type = &quot;class&quot;)) g12 = ggplot(dt[samp, ], aes(x = x, y = y)) + geom_point(aes(col = factor(code))) + scale_color_discrete(guide = FALSE) + ggtitle(&quot;Training Set N=5000&quot;) + theme_jeb() g13 = ggplot(dt, aes(x = x, y = y)) + geom_point(aes(col = out.5000)) + scale_color_discrete(guide = FALSE) + ggtitle(paste(&quot;Model: N=5000,&quot;, &quot;Error=&quot;, length(which(as.numeric(dt$out.5000)-as.numeric(dt$code) == 0))/n)) + theme_jeb() grid.arrange(g12, g13, nrow = 1) "],
["random-forest.html", "Random Forest", " Random Forest Background The following dataset are career stats for over 1000 MLB baseball players. The data consists of the position of each player along with 19 numeric variables measure offense. The training set consists of 677 observations and the testing set has 339 observations. The objective is to build a model that will predict whether a player is in the Hall of Fame based on his career statistics. Since so few players make it to the Hall of Fame, the methodology for scoring the accuracy of models is based on the following calculation: \\((sensitivity + 3*specificity) / 4\\). The objective is to get as few incorrect predictions as possible, but having fewer false positives will affect the accuracy measure more than false negatives. ## required packages library(randomForest) ## Training and Testing Data hof.train = read.csv(&quot;data/HOF_tr.csv&quot;); hof.test = read.csv(&quot;data/HOF_te.csv&quot;) ## remove unwanted columns hof.train = hof.train[, -c(2:4)]; hof.test = hof.test[, -c(2:4)] head(hof.train) HOF POS ASG G AB R H DB TP HR RBI SB CS BB SO AVG 1 Y OF 0.75 2440 9288 1383 3141 543 85 135 1138 319 125 790 434 338 2 Y SS 0.56 2601 10230 1335 2677 394 92 83 791 506 136 736 742 262 3 Y OF 0.83 1783 7244 1071 2304 414 57 207 1085 134 76 450 965 318 4 Y 1B 0.38 3026 11336 1627 3255 560 35 504 1917 110 43 1333 1516 287 5 Y 1B 0.95 2469 9315 1424 3053 445 112 92 1015 353 187 1018 1028 328 6 Y 1B 0.41 2124 7927 1131 2351 417 27 379 1365 142 80 588 1169 297 SLG OBP 1 459 388 2 343 311 3 477 360 4 476 359 5 429 393 6 499 350 summary(hof.train) HOF POS ASG G AB N:644 1B: 77 Min. :0.0000 Min. : 253 Min. : 559 Y: 33 2B: 83 1st Qu.:0.0000 1st Qu.: 924 1st Qu.: 2626 3B: 71 Median :0.0000 Median :1294 Median : 4101 C :122 Mean :0.1022 Mean :1336 Mean : 4323 OF:244 3rd Qu.:0.1500 3rd Qu.:1666 3rd Qu.: 5536 SS: 80 Max. :0.9500 Max. :3081 Max. :11551 R H DB TP Min. : 37.0 Min. : 90 Min. : 14 Min. : 0.00 1st Qu.: 308.0 1st Qu.: 656 1st Qu.:109 1st Qu.: 12.00 Median : 522.0 Median :1064 Median :183 Median : 24.00 Mean : 574.9 Mean :1157 Mean :199 Mean : 30.19 3rd Qu.: 760.0 3rd Qu.:1502 3rd Qu.:260 3rd Qu.: 41.00 Max. :2295.0 Max. :3283 Max. :668 Max. :166.00 HR RBI SB CS Min. : 1.0 Min. : 44.0 Min. : 0.00 Min. : 0.00 1st Qu.: 34.0 1st Qu.: 268.0 1st Qu.: 14.00 1st Qu.: 15.00 Median : 80.0 Median : 443.0 Median : 42.00 Median : 31.00 Mean :113.9 Mean : 538.1 Mean : 84.64 Mean : 42.07 3rd Qu.:155.0 3rd Qu.: 708.0 3rd Qu.: 98.00 3rd Qu.: 57.00 Max. :660.0 Max. :1917.0 Max. :1406.00 Max. :335.00 BB SO AVG SLG Min. : 29 Min. : 71 Min. :161.0 Min. :222.0 1st Qu.: 221 1st Qu.: 362 1st Qu.:249.0 1st Qu.:350.0 Median : 364 Median : 565 Median :261.0 Median :392.0 Mean : 436 Mean : 643 Mean :261.5 Mean :392.7 3rd Qu.: 583 3rd Qu.: 842 3rd Qu.:273.0 3rd Qu.:433.0 Max. :2190 Max. :2597 Max. :338.0 Max. :557.0 OBP Min. :203.0 1st Qu.:311.0 Median :328.0 Mean :328.8 3rd Qu.:347.0 Max. :415.0 ## Simple Random Forest (mdl = randomForest(HOF ~ ., ntree = 1000, data = hof.train)) Call: randomForest(formula = HOF ~ ., data = hof.train, ntree = 1000) Type of random forest: classification Number of trees: 1000 No. of variables tried at each split: 4 OOB estimate of error rate: 1.48% Confusion matrix: N Y class.error N 641 3 0.004658385 Y 7 26 0.212121212 ## Function for testing accuracy metric = function(confusion) { sensitivity = confusion[4] / (confusion[2] + confusion[4]) specificity = confusion[1] / (confusion[1] + confusion[3]) score = (sensitivity + (3 * specificity)) / 4 return(score) } ## Plot of the model performance par(mfrow = c(1, 2)) plot(x = 1:1000, y = mdl$err.rate[,1], xlab = &quot;Trees&quot;, ylab = &quot;Error&quot;, type = &quot;l&quot;, main = &quot;Out of Sample Error Rate&quot;) varImpPlot(mdl, main = &quot;Variable Importance Plot&quot;) Testing Model Accuracy on New Data Now that we have a trained model, we will apply the model to data that was not used in the training set. We will calculate the same accuracy score and compare the two. If they are wildly different we may have a problem with overfitting. ## predict the probability of HOF estimate = data.frame(predict(mdl, hof.test, type = &quot;prob&quot;)) estimate$predict = predict(mdl, hof.test) estimate$actual = hof.test$HOF ## Generate a confusion matrix (confusion = table(estimate[, 3:4])) actual predict N Y N 320 1 Y 3 15 ## Final Accuracy Measure (test.metric = metric(confusion)) [1] 0.9559969 The random forest method is fairly robust to overfitting because it reserves some of the training data to use as test data which is called Out of Bag (OOB error). Because of this internal mechanism we could probably ues a larger portion of the overall data to train. The next sections tests this to see if accuracy is improved. hof = rbind(hof.train, hof.test) ## create a training and testing set by randomly sampling from all of the data set.seed(1002) x = sample(nrow(hof), replace = FALSE) ## lets train the model on about 90% of the data train = hof[x[1:900], ] test = hof[-x[1:900], ] ## build the model (mdl = randomForest(HOF ~ ., ntree = 1000, data = train)) Call: randomForest(formula = HOF ~ ., data = train, ntree = 1000) Type of random forest: classification Number of trees: 1000 No. of variables tried at each split: 4 OOB estimate of error rate: 1.56% Confusion matrix: N Y class.error N 858 3 0.003484321 Y 11 28 0.282051282 ## predict the probability of HOF estimate = data.frame(predict(mdl, test, type = &quot;prob&quot;)) estimate$predict = predict(mdl, test) estimate$actual = test$HOF ## confusion matrix (confusion = table(estimate[, 3:4])) actual predict N Y N 106 2 Y 0 8 (test.metric = metric(confusion)) [1] 0.9861111 ## model plots par(mfrow = c(1, 2)) plot(x = 1:1000, y = mdl$err.rate[,1], xlab = &quot;Trees&quot;, ylab = &quot;Error&quot;, type = &quot;l&quot;, main = &quot;Out of Sample Error Rate&quot;) varImpPlot(mdl, main = &quot;Variable Importance Plot&quot;) We get slightly better results from increasing the training set. The Random Forest model is predicting Yes to Hall of Fame if it measures the probability &gt; .5. Since it is is so rare that a player gets voted to the Hall of Fame how accurate is the model if we lower the threshold? Based on a review of some of the false negatives I will make the minimum threshold for predicting yes .33 ## predict the probability of HOF estimate = data.frame(predict(mdl, test, type = &quot;prob&quot;)) estimate$predict = &quot;N&quot; estimate$predict[which(estimate$Y &gt; .33)] = &quot;Y&quot; estimate$actual = test$HOF ## confusion matrix (confusion = table(estimate[, 3:4])) actual predict N Y N 106 0 Y 0 10 (test.metric = metric(confusion)) [1] 1 "],
["gradient-boosting.html", "Gradient Boosting", " Gradient Boosting ## required packages library(caret) library(gbm) ## Training and Testing Data hof.train = read.csv(&quot;data/HOF_tr.csv&quot;); hof.test = read.csv(&quot;data/HOF_te.csv&quot;) hof = rbind(hof.train, hof.test) hof$HOF = as.factor(as.numeric(hof$HOF) - 1) ## create a training and testing set by randomly sampling from all of the data ## using the same set as in the random forest example set.seed(1002) x = sample(nrow(hof), replace = FALSE) ## remove unwanted columns hof = hof[, -c(2:4)] ## lets train the model on about 90% of the data train = hof[x[1:900], ] test = hof[-x[1:900], ] head(train) HOF POS ASG G AB R H DB TP HR RBI SB CS BB SO AVG 453 0 1B 0.44 2071 7030 1105 1921 295 48 370 1274 63 31 943 1137 273 803 0 SS 0.00 568 1104 142 260 43 10 37 109 7 5 94 220 236 621 0 C 0.00 476 1125 89 267 41 5 18 108 1 0 43 159 237 230 0 OF 0.12 1912 6787 926 1884 334 69 164 824 312 134 468 1266 278 379 0 OF 0.17 1457 4843 737 1399 212 60 142 661 89 68 644 591 289 720 0 OF 0.09 1221 3895 540 1020 175 37 112 485 45 30 351 574 262 SLG OBP 453 487 359 803 393 304 621 331 268 230 420 325 379 445 371 720 412 323 summary(train) HOF POS ASG G AB 0:861 1B: 96 Min. :0.0000 Min. : 140.0 Min. : 252 1: 39 2B:105 1st Qu.:0.0000 1st Qu.: 937.5 1st Qu.: 2628 3B:100 Median :0.0000 Median :1286.0 Median : 4034 C :158 Mean :0.1028 Mean :1338.3 Mean : 4335 OF:331 3rd Qu.:0.1500 3rd Qu.:1666.2 3rd Qu.: 5607 SS:110 Max. :0.9500 Max. :3308.0 Max. :12364 R H DB TP Min. : 20.0 Min. : 48.0 Min. : 6.0 Min. : 0.00 1st Qu.: 308.5 1st Qu.: 660.2 1st Qu.:110.8 1st Qu.: 12.00 Median : 511.5 Median :1061.0 Median :180.0 Median : 24.00 Mean : 574.8 Mean :1159.5 Mean :199.4 Mean : 30.44 3rd Qu.: 756.0 3rd Qu.:1531.2 3rd Qu.:264.0 3rd Qu.: 41.00 Max. :2295.0 Max. :3771.0 Max. :725.0 Max. :177.00 HR RBI SB CS Min. : 0.00 Min. : 21.0 Min. : 0.00 Min. : 0.00 1st Qu.: 37.75 1st Qu.: 280.5 1st Qu.: 15.00 1st Qu.: 15.00 Median : 81.00 Median : 447.0 Median : 40.50 Median : 30.00 Mean :115.15 Mean : 540.9 Mean : 84.33 Mean : 41.81 3rd Qu.:155.00 3rd Qu.: 708.0 3rd Qu.: 101.25 3rd Qu.: 57.00 Max. :755.00 Max. :2297.0 Max. :1406.00 Max. :335.00 BB SO AVG SLG Min. : 17.0 Min. : 35.0 Min. :161.0 Min. :222.0 1st Qu.: 226.8 1st Qu.: 375.5 1st Qu.:248.0 1st Qu.:351.0 Median : 363.0 Median : 569.0 Median :262.0 Median :392.5 Mean : 435.3 Mean : 643.5 Mean :261.3 Mean :393.2 3rd Qu.: 567.0 3rd Qu.: 841.2 3rd Qu.:274.0 3rd Qu.:432.0 Max. :2190.0 Max. :2597.0 Max. :338.0 Max. :565.0 OBP Min. :203.0 1st Qu.:310.0 Median :327.0 Mean :328.2 3rd Qu.:347.0 Max. :417.0 ## build model fitControl = trainControl(method = &quot;repeatedCV&quot;, number = 5, repeats = 5) mdl = train(HOF ~ ., data = train, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE) ## Model Summary mdl; plot(mdl) Stochastic Gradient Boosting 900 samples 17 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (5 fold, repeated 5 times) Summary of sample sizes: 720, 720, 720, 719, 721, 720, ... Resampling results across tuning parameters: interaction.depth n.trees Accuracy Kappa 1 50 0.9842196 0.7910195 1 100 0.9839986 0.7934159 1 150 0.9835517 0.7899395 2 50 0.9859986 0.8179582 2 100 0.9846653 0.8038399 2 150 0.9840035 0.7949731 3 50 0.9833307 0.7767886 3 100 0.9824443 0.7725392 3 150 0.9831146 0.7844066 Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 Accuracy was used to select the optimal model using the largest value. The final values used for the model were n.trees = 50, interaction.depth = 2, shrinkage = 0.1 and n.minobsinnode = 10. x = predict(mdl, test, type = &quot;prob&quot;) ## compile results results = data.frame( Actual = test$HOF, Prob.N = x[, 1], Prob.Y = x[, 2] ) ## code 0/1 back to N/Y results$Actual = as.character(results$Actual) results$Actual[results$Actual == &#39;0&#39;] = &#39;N&#39; results$Actual[results$Actual == &#39;1&#39;] = &#39;Y&#39; results$Actual = factor(results$Actual) ## if probability of HOF is &gt; .5 then score a Y results$Prediction = &quot;N&quot; results$Prediction[results$Prob.Y &gt;= .5] = &quot;Y&quot; results$Prediction = factor(results$Prediction) ## accuracy calculation from the random forest example metric = function(confusion) { sensitivity = confusion[4] / (confusion[2] + confusion[4]) specificity = confusion[1] / (confusion[1] + confusion[3]) score = (sensitivity + (3 * specificity)) / 4 return(score) } ## confusion matrix and accuracy score (confusion = table(Prediction = results$Prediction, Actual = results$Actual)) Actual Prediction N Y N 106 2 Y 0 8 ## accuracy score for training set metric(confusion) [1] 0.9861111 ## look at the incorrect responses and see if we can lower the threshold with creating ## false positives summary(results); subset(results, Actual != Prediction) Actual Prob.N Prob.Y Prediction N:106 Min. :0.001712 Min. :0.001282 N:108 Y: 10 1st Qu.:0.998683 1st Qu.:0.001317 Y: 8 Median :0.998683 Median :0.001317 Mean :0.935894 Mean :0.064106 3rd Qu.:0.998683 3rd Qu.:0.001317 Max. :0.998718 Max. :0.998288 Actual Prob.N Prob.Y Prediction 1 Y 0.9691392 0.03086077 N 78 Y 0.7711650 0.22883501 N min.pred = min(subset(results, Actual != Prediction, &quot;Prob.Y&quot;)) ## it looks like there is no danger of lowering the threshold results$Prediction.new = &quot;N&quot; results$Prediction.new[results$Prob.Y &gt;= min.pred] = &quot;Y&quot; ## confusion matrix and accuracy score (confusion = table(Prediction = results$Prediction.new, Actual = results$Actual)) Actual Prediction N Y N 102 0 Y 4 10 ## accuracy score for training set metric(confusion) [1] 0.9285714 ## there are fewer incorrect answers, but the penalty for false positives are greater ## than false negatives so the accuracy score is actually lower "],
["0-2-support-vector-machines.html", "0.2 Support Vector Machines", " 0.2 Support Vector Machines # # ## required packages # library(e1071) # # ## Training and Testing Data # hof.train = read.csv(&quot;data/HOF_tr.csv&quot;); # hof.test = read.csv(&quot;data/HOF_te.csv&quot;) # # hof = rbind(hof.train, hof.test) # # ## create a training and testing set by randomly sampling from all of the data # ## using the same set as in the random forest example # set.seed(1002) # x = sample(nrow(hof), replace = FALSE) # # ## remove unwanted columns # hof = hof[, -c(2:4)] # # ## lets train the model on about 90% of the data # train = hof[x[1:900], ] # test = hof[-x[1:900], ] # # head(train) # summary(train) # # ## train the model # (tuneModel = tune(svm, HOF ~ ., data = train, # ranges = list( # cost = seq(.1, 5, .25), # gamma = seq(0, .5, .01)))) # # ## darker colors are better models # plot(tuneModel, main = &quot;Model Performance&quot;) # # ## store best parameters # cst = as.numeric(tuneModel$best.parameters[1]) # gma = as.numeric(tuneModel$best.parameters[2]) # # ## do an indepth search of the darker grid # (tuneModel = tune(svm, HOF ~ ., data = train, # ranges = list( # cost = seq(cst-1, cst+1, .01), # gamma = seq(0, gma+.1, .001)))) # # ## darker colors are better models # plot(tuneModel, main = &quot;Model Performance&quot;) # # ## store best parameters # cst = as.numeric(tuneModel$best.parameters[1]) # gma = as.numeric(tuneModel$best.parameters[2]) # # ## build model based on the tuned parameters # mdl = svm(HOF ~ ., data = train, probability = TRUE, cost = cst, gamma = gma) # x = predict(mdl, test, probability = TRUE) # # ## compile results # results = data.frame( # Prediction = x, # Actual = test$HOF, # Prob.N = attr(x, &quot;probabilities&quot;)[, 1], # Prob.Y = attr(x, &quot;probabilities&quot;)[, 2] # ) # # ## accuracy calculation from the random forest example # metric = function(confusion) { # sensitivity = confusion[4] / (confusion[2] + confusion[4]) # specificity = confusion[1] / (confusion[1] + confusion[3]) # score = (sensitivity + (3 * specificity)) / 4 # return(score) # } # # ## confusion matrix and accuracy score # (confusion = table(Prediction = results$Prediction, Actual = results$Actual)) # # ## accuracy score for training set # metric(confusion) # # ## look at the probability prediction for all of CV results to see if we should lower the # ## probability threshold for predicing Y to HOF # subset(results, Prediction != Actual) # min = min(subset(results, Prediction != Actual, &quot;Prob.Y&quot;)) # # ## lower the threshold # results$Prediction.new = &quot;N&quot; # results$Prediction.new[results$Prob.Y &gt;= min] = &quot;Y&quot; # # ## confusion matrix and accuracy score # (confusion = table(Prediction = results$Prediction.new, Actual = results$Actual)) # # ## accuracy score for training set # metric(confusion) "],
["overfitting.html", "Overfitting", " Overfitting library(pander) library(e1071) ## First 500 digits in pi dta = c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9, 3, 2, 3, 8, 4, 6, 2, 6, 4, 3, 3, 8, 3, 2, 7, 9, 5, 0, 2, 8, 8, 4, 1, 9, 7, 1, 6, 9, 3, 9, 9, 3, 7, 5, 1, 0, 5, 8, 2, 0, 9, 7, 4, 9, 4, 4, 5, 9, 2, 3, 0, 7, 8, 1, 6, 4, 0, 6, 2, 8, 6, 2, 0, 8, 9, 9, 8, 6, 2, 8, 0, 3, 4, 8, 2, 5, 3, 4, 2, 1, 1, 7, 0, 6, 7, 9, 8, 2, 1, 4, 8, 0, 8, 6, 5, 1, 3, 2, 8, 2, 3, 0, 6, 6, 4, 7, 0, 9, 3, 8, 4, 4, 6, 0, 9, 5, 5, 0, 5, 8, 2, 2, 3, 1, 7, 2, 5, 3, 5, 9, 4, 0, 8, 1, 2, 8, 4, 8, 1, 1, 1, 7, 4, 5, 0, 2, 8, 4, 1, 0, 2, 7, 0, 1, 9, 3, 8, 5, 2, 1, 1, 0, 5, 5, 5, 9, 6, 4, 4, 6, 2, 2, 9, 4, 8, 9, 5, 4, 9, 3, 0, 3, 8, 1, 9, 6, 4, 4, 2, 8, 8, 1, 0, 9, 7, 5, 6, 6, 5, 9, 3, 3, 4, 4, 6, 1, 2, 8, 4, 7, 5, 6, 4, 8, 2, 3, 3, 7, 8, 6, 7, 8, 3, 1, 6, 5, 2, 7, 1, 2, 0, 1, 9, 0, 9, 1, 4, 5, 6, 4, 8, 5, 6, 6, 9, 2, 3, 4, 6, 0, 3, 4, 8, 6, 1, 0, 4, 5, 4, 3, 2, 6, 6, 4, 8, 2, 1, 3, 3, 9, 3, 6, 0, 7, 2, 6, 0, 2, 4, 9, 1, 4, 1, 2, 7, 3, 7, 2, 4, 5, 8, 7, 0, 0, 6, 6, 0, 6, 3, 1, 5, 5, 8, 8, 1, 7, 4, 8, 8, 1, 5, 2, 0, 9, 2, 0, 9, 6, 2, 8, 2, 9, 2, 5, 4, 0, 9, 1, 7, 1, 5, 3, 6, 4, 3, 6, 7, 8, 9, 2, 5, 9, 0, 3, 6, 0, 0, 1, 1, 3, 3, 0, 5, 3, 0, 5, 4, 8, 8, 2, 0, 4, 6, 6, 5, 2, 1, 3, 8, 4, 1, 4, 6, 9, 5, 1, 9, 4, 1, 5, 1, 1, 6, 0, 9, 4, 3, 3, 0, 5, 7, 2, 7, 0, 3, 6, 5, 7, 5, 9, 5, 9, 1, 9, 5, 3, 0, 9, 2, 1, 8, 6, 1, 1, 7, 3, 8, 1, 9, 3, 2, 6, 1, 1, 7, 9, 3, 1, 0, 5, 1, 1, 8, 5, 4, 8, 0, 7, 4, 4, 6, 2, 3, 7, 9, 9, 6, 2, 7, 4, 9, 5, 6, 7, 3, 5, 1, 8, 8, 5, 7, 5, 2, 7, 2, 4, 8, 9, 1, 2, 2, 7, 9, 3, 8, 1, 8, 3, 0, 1, 1, 9, 4, 9, 1, 2) ## Create 5 variables to based on the lagged value of the ith digit dta = data.frame(y = dta) dta$x1 = NA; dta$x2 = NA; dta$x3 = NA; dta$x4 = NA; dta$x5 = NA for (i in 2:500) { dta$x1[i] = dta$y[i-1] } for (i in 3:500) { dta$x2[i] = dta$y[i-2] } for (i in 4:500) { dta$x3[i] = dta$y[i-3] } for (i in 5:500) { dta$x4[i] = dta$y[i-4] } for (i in 6:500) { dta$x5[i] = dta$y[i-5] } head(dta) y x1 x2 x3 x4 x5 1 3 NA NA NA NA NA 2 1 3 NA NA NA NA 3 4 1 3 NA NA NA 4 1 4 1 3 NA NA 5 5 1 4 1 3 NA 6 9 5 1 4 1 3 ## Remove NA dta = dta[6:500,] ## Create Factors out of the variables dta[] = lapply(dta, factor) ## Break up the data into the training and testing sets train = dta[1:475, ] test = dta[476:495, ] ## Tune an SVM Model mdl.svm = tune(svm, y ~ ., data = train, ranges = list( cost = seq(1, 20, 2), gamma = seq(0, 1, .1)) ) plot(mdl.svm) best.cost = mdl.svm$best.parameters[1] best.gamma = mdl.svm$best.parameters[2] mdl.svm = svm(y ~ ., data = train, cost = best.cost, gamma = best.gamma, probability = TRUE) ## Predict the testing set tmp = predict(mdl.svm, test, probability = TRUE) results.svm = data.frame(actual = test$y, predicted = tmp) results.svm$Result = FALSE results.svm$Result[which(results.svm$actual == results.svm$predicted)] = TRUE ## create Predict the training set train.results.svm = data.frame(actual = train$y, pred = predict(mdl.svm, train)) train.results.svm$Result = FALSE train.results.svm$Result[train.results.svm$actual == train.results.svm$pred] = TRUE ## Aggregate results results = list( Train.Incorrect = round(table(train.results.svm$Result)[[1]]/475, 3), Train.Correct = round(table(train.results.svm$Result)[[2]]/475, 3), Test.Incorrect = round(table(results.svm$Result)[[1]]/20, 3), Test.Correct = round(table(results.svm$Result)[[2]]/20, 3) ) pandoc.table(results, split.tables = Inf) ----------------------------------------------------------------- Train.Incorrect Train.Correct Test.Incorrect Test.Correct ----------------- --------------- ---------------- -------------- 0.008 0.992 0.85 0.15 ----------------------------------------------------------------- "],
["simulation.html", "Simulation ", " Simulation "],
["bootstrap-simulation.html", "Bootstrap Simulation", " Bootstrap Simulation This script creates a function for running a simple bootstrap simulation from a given input. In this example I am using the data set BJsales from R. Bootstrap simulations work by pulling a series of random samples from the past and stringing them together to produce a possible future outcome. You can specify how many months.back you want the simulation to pull samples from and how many months.forward you want the simulation to produce. If you want to use recent history make months.back a small number. If you want to project your simulation far out into the future, make months.forward a larger number. Specify the number of simulations (trials). The function produces 2 plots. The first plot shows the original data as well as each individual simulated trial. The second plot is a distribution of the return or total percent change from the last historical value and the last months.forward value for each simulation. The alpha parameter controls how transparent the lines are in the first graph. library(ggplot2) library(reshape2) library(scales) library(grid) library(gridExtra) data(&quot;BJsales&quot;) boot.plot = function(dt, trials, months.back = 24, months.forward = 12, alph = .1) { ## calculate the change history lookup = 1 + (diff(dt) / dt) ## run run simulation by picking random percent changes from history ## repeat for the number of trials sims = data.frame() for (i in 1:trials) { sims = rbind(sims, matrix( sample(lookup, size = months.forward , replace = TRUE), nrow = 1) ) } time = seq(151, 151 + months.forward - 1, by = 1) colnames(sims) = time sims[is.na(sims)] = 1 last.price = dt[length(dt)] # calculate the price change from the last known price p.chg = data.frame(sims[, 1] * last.price) for (i in 2:months.forward) {p.chg = cbind(p.chg, p.chg[, i - 1] * sims[, i])} colnames(p.chg) = time # calculate the cumulative percent change cuml.new = sims[,] - 1 cuml = data.frame(cuml.new[, 1]) for (i in 2:months.forward) {cuml = cbind(cuml, cuml[, i - 1] + cuml.new[, i])} colnames(cuml) = time cuml.chg = data.frame(&quot;Price Change&quot; = cuml[, length(cuml)]) cuml$trial = 1:nrow(cuml) cuml = melt(cuml, id.vars = &quot;trial&quot;) dt = data.frame(time = 1:length(dt), dt = dt) ## create graphics plot.price = dcast(dt, . ~ time) plot.price2 = cbind(plot.price, p.chg) plot.price2$trial = 1:nrow(plot.price2) plot.price2 = plot.price2[,-1] plot.price2 = melt(plot.price2, id.vars = c(&quot;trial&quot;)) cuml = data.frame(&quot;cuml.1qrt&quot; = quantile(cuml.chg$Price.Change)[2], &quot;cuml.3qrt&quot; = quantile(cuml.chg$Price.Change)[4], &quot;iqr.15&quot; = quantile(cuml.chg$Price.Change)[4] - quantile(cuml.chg$Price.Change)[2] * 1.5, &quot;med&quot; = quantile(cuml.chg$Price.Change)[3]) x1 = ggplot(cuml.chg) + geom_rect(aes(xmin = cuml.1qrt, xmax = cuml.3qrt, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;green&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = cuml.1qrt - iqr.15, xmax = cuml.1qrt, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;yellow&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = cuml.3qrt, xmax = cuml.3qrt + iqr.15, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;yellow&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = cuml.3qrt + iqr.15, xmax = Inf, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;red&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = -Inf, xmax = cuml.1qrt - iqr.15, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;red&quot;, color = &quot;gray&quot;, alpha = .1) + geom_bar(aes(x = Price.Change), stat = &quot;bin&quot;, binwidth = .005, fill = &quot;white&quot;, color = &quot;black&quot;, alpha = .15) + ggtitle(&quot;Distibution of Simulated Changes&quot;) + scale_x_continuous(&quot;&quot;, labels = percent, breaks = seq(-.5, .5, .05)) + scale_y_continuous(&quot;&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;, size = 12), axis.text.y = element_text(size = 0), axis.ticks = element_blank()) plot.price2$variable = as.numeric(plot.price2$variable) x3 = ggplot(plot.price2, aes(x = variable, y = value, group = trial)) + geom_line(color = &quot;blue&quot;, alpha = alph, se = FALSE) + scale_x_continuous(&quot;Time&quot;) + scale_y_continuous(&quot;Sales&quot;, labels = dollar) + theme(plot.title = element_text(face = &quot;bold&quot;, size = 12)) + ggtitle(&quot;Time Series with Bootstrap Simulation&quot;) return(grid.arrange(x3, x1, nrow = 2)) } boot.plot(BJsales, trials = 100, months.back = 12, months.forward = 12, alph = .01) ## Warning: `geom_bar()` no longer has a `binwidth` parameter. Please use ## `geom_histogram()` instead. ## Warning: Ignoring unknown parameters: se boot.plot(BJsales, trials = 500, months.back = 24, months.forward = 24, alph = .005) ## Warning: `geom_bar()` no longer has a `binwidth` parameter. Please use ## `geom_histogram()` instead. ## Warning: Ignoring unknown parameters: se boot.plot(BJsales, trials = 1500, months.back = 36, months.forward = 36, alph = .005) ## Warning: `geom_bar()` no longer has a `binwidth` parameter. Please use ## `geom_histogram()` instead. ## Warning: Ignoring unknown parameters: se boot.plot = function(dt, trials, months.back = 24, months.forward = 12, alph = .1) { ## calculate the change history lookup = 1 + (diff(dt) / dt) ## run run simulation by picking random percent changes from history ## repeat for the number of trials sims = data.frame() for (i in 1:trials) { sims = rbind(sims, matrix( sample(lookup, size = months.forward , replace = TRUE), nrow = 1) ) } time = seq(151, 151 + months.forward - 1, by = 1) colnames(sims) = time sims[is.na(sims)] = 1 last.price = dt[length(dt)] # calculate the price change from the last known price p.chg = data.frame(sims[, 1] * last.price) for (i in 2:months.forward) {p.chg = cbind(p.chg, p.chg[, i - 1] * sims[, i])} colnames(p.chg) = time # calculate the cumulative percent change cuml.new = sims[,] - 1 cuml = data.frame(cuml.new[, 1]) for (i in 2:months.forward) {cuml = cbind(cuml, cuml[, i - 1] + cuml.new[, i])} colnames(cuml) = time cuml.chg = data.frame(&quot;Price Change&quot; = cuml[, length(cuml)]) cuml$trial = 1:nrow(cuml) cuml = melt(cuml, id.vars = &quot;trial&quot;) dt = data.frame(time = 1:length(dt), dt = dt) ## create graphics plot.price = dcast(dt, . ~ time) plot.price2 = cbind(plot.price, p.chg) plot.price2$trial = 1:nrow(plot.price2) plot.price2 = plot.price2[,-1] plot.price2 = melt(plot.price2, id.vars = c(&quot;trial&quot;)) cuml = data.frame(&quot;cuml.1qrt&quot; = quantile(cuml.chg$Price.Change)[2], &quot;cuml.3qrt&quot; = quantile(cuml.chg$Price.Change)[4], &quot;iqr.15&quot; = quantile(cuml.chg$Price.Change)[4] - quantile(cuml.chg$Price.Change)[2] * 1.5, &quot;med&quot; = quantile(cuml.chg$Price.Change)[3]) x1 = ggplot(cuml.chg) + geom_rect(aes(xmin = cuml.1qrt, xmax = cuml.3qrt, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;green&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = cuml.1qrt - iqr.15, xmax = cuml.1qrt, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;yellow&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = cuml.3qrt, xmax = cuml.3qrt + iqr.15, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;yellow&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = cuml.3qrt + iqr.15, xmax = Inf, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;red&quot;, color = &quot;gray&quot;, alpha = .1) + geom_rect(aes(xmin = -Inf, xmax = cuml.1qrt - iqr.15, ymin = -Inf, ymax = Inf), data = cuml, fill = &quot;red&quot;, color = &quot;gray&quot;, alpha = .1) + geom_bar(aes(x = Price.Change), stat = &quot;bin&quot;, binwidth = .005, fill = &quot;white&quot;, color = &quot;black&quot;, alpha = .15) + ggtitle(&quot;Distibution of Simulated Changes&quot;) + scale_x_continuous(&quot;&quot;, labels = percent, breaks = seq(-.5, .5, .05)) + scale_y_continuous(&quot;&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;, size = 12), axis.text.y = element_text(size = 0), axis.ticks = element_blank()) plot.price2$variable = as.numeric(plot.price2$variable) x3 = ggplot(plot.price2, aes(x = variable, y = value, group = trial)) + geom_line(color = &quot;blue&quot;, alpha = alph, se = FALSE) + scale_x_continuous(&quot;Time&quot;) + scale_y_continuous(&quot;Sales&quot;, labels = dollar) + theme(plot.title = element_text(face = &quot;bold&quot;, size = 12)) + ggtitle(&quot;Time Series with Bootstrap Simulation&quot;) return(grid.arrange(x3, x1, nrow = 2)) } "],
["visualization.html", "Visualization ", " Visualization "],
["building-maps-with-choropleth-and-ggplot.html", "Building Maps with choropleth and ggplot", " Building Maps with choropleth and ggplot Data: Census 2000, number of jobs by industry by county # # library(choroplethr) # library(choroplethrMaps) # library(ggplot2) # library(gridExtra) # # employment = read.csv(&quot;data/census_2000_employment.csv&quot;) # crops = read.csv(&quot;data/econ_census_2012_farming.csv&quot;) # # head(employment) # # farming = data.frame( # region = employment$STCOU, # value = employment$Farming # ) # # county_choropleth(farming, title = &quot;Farming Employment&quot;, legend = &quot;Jobs&quot;) # # # choro = CountyChoropleth$new(farming) # choro$title = &quot;Southern Farmers&quot; # choro$set_num_colors(1) # choro$ggplot_scale = scale_fill_gradientn(&quot;Jobs Count&quot;, colors = c(&quot;#F6EAE1&quot;, &quot;darkgreen&quot;)) # choro$set_zoom(c(&quot;texas&quot;, &quot;louisiana&quot;, &quot;mississippi&quot;, &quot;alabama&quot;, &quot;georgia&quot;, &quot;arkansas&quot;, # &quot;tennessee&quot;, &quot;south carolina&quot;, &quot;florida&quot;, &quot;oklahoma&quot;, &quot;north carolina&quot;)) # choro$render() # # ## Also without the gradient # choro = CountyChoropleth$new(farming) # choro$title = &quot;Southern Farmers&quot; # choro$legend = &quot;Jobs Count&quot; # choro$set_zoom(c(&quot;texas&quot;, &quot;louisiana&quot;, &quot;mississippi&quot;, &quot;alabama&quot;, &quot;georgia&quot;, &quot;arkansas&quot;, # &quot;tennessee&quot;, &quot;south carolina&quot;, &quot;florida&quot;, &quot;oklahoma&quot;, &quot;north carolina&quot;)) # choro$render() # # ## fill in NA with 0 to prevent NULL counties from standing out # crops[is.na(crops)] = 0 # # ## create the individual data sets # corn = data.frame(region = crops$FIPS, value = crops$Corn) # wheat = data.frame(region = crops$FIPS, value = crops$Wheat) # cotton = data.frame(region = crops$FIPS, value = crops$Cotton) # peanuts = data.frame(region = crops$FIPS, value = crops$Peanuts) # peaches = data.frame(region = crops$FIPS, value = crops$Peaches) # oranges = data.frame(region = crops$FIPS, value = crops$Oranges) # # ## list of states we want on the map # states = c(&quot;texas&quot;, &quot;louisiana&quot;, &quot;mississippi&quot;, &quot;alabama&quot;, &quot;georgia&quot;, &quot;arkansas&quot;, # &quot;tennessee&quot;, &quot;south carolina&quot;, &quot;florida&quot;, &quot;oklahoma&quot;, &quot;north carolina&quot;) # # ## create each graph object # g1 = CountyChoropleth$new(corn); g1$title = &quot;Corn % of Farmable Land&quot; # g1$set_num_colors(1); g1$set_zoom(states) # g1$ggplot_scale = scale_fill_gradientn(&quot;%&quot;, colors = c(&quot;gray&quot;, &quot;darkgreen&quot;)) # # g2 = CountyChoropleth$new(wheat); g2$title = &quot;Wheat % of Farmable Land&quot; # g2$set_num_colors(1); g2$set_zoom(states) # g2$ggplot_scale = scale_fill_gradientn(&quot;%&quot;, colors = c(&quot;gray&quot;, &quot;darkgreen&quot;)) # # g3 = CountyChoropleth$new(cotton); g3$title = &quot;Cotton % of Farmable Land&quot; # g3$set_num_colors(1); g3$set_zoom(states) # g3$ggplot_scale = scale_fill_gradientn(&quot;%&quot;, colors = c(&quot;gray&quot;, &quot;darkgreen&quot;)) # # g4 = CountyChoropleth$new(peanuts); g4$title = &quot;Peanuts % of Farmable Land&quot; # g4$set_num_colors(1); g4$set_zoom(states) # g4$ggplot_scale = scale_fill_gradientn(&quot;%&quot;, colors = c(&quot;gray&quot;, &quot;darkgreen&quot;)) # # g5 = CountyChoropleth$new(peaches); g5$title = &quot;Peaches % of Farmable Land&quot; # g5$set_num_colors(1); g5$set_zoom(states) # g5$ggplot_scale = scale_fill_gradientn(&quot;%&quot;, colors = c(&quot;gray&quot;, &quot;darkgreen&quot;)) # # g6 = CountyChoropleth$new(oranges); g6$title = &quot;Oranges % of Farmable Land&quot; # g6$set_num_colors(1); g6$set_zoom(states) # g6$ggplot_scale = scale_fill_gradientn(&quot;%&quot;, colors = c(&quot;gray&quot;, &quot;darkgreen&quot;)) # # ## render each object in order to call as a group # g1 = g1$render(); g2 = g2$render(); g3 = g3$render() # g4 = g4$render(); g5 = g5$render(); g6 = g6$render() # # grid.arrange(g1, g2, g3, g4, g5, g6, nrow = 3, ncol = 2) "]
]
