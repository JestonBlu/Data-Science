[
["index.html", "Data Science Tasks Intro", " Data Science Tasks Intro This website is a work in progress and intended to be a personal reference for data science related tasks. It consists of reproducible programming examples for various analytic and visualization techniques. All code is available here. "],
["statistical-methods.html", "Statistical Methods Fitting Distributions Hypothesis Testing Parameter Estimation", " Statistical Methods Fitting Distributions Continuous Distributions Assessing Distributions Visually Formal Tests for Distribution Fit Maximum Likelihood calculation Normal Distribution library(nortest) library(MASS) ## Draw some random data set.seed(10) x1 = rnorm(20) ## Distribution plots par(mfrow = c(2, 2)) qqnorm(x1) qqline(x1) boxplot(x1, main = &quot;Boxplot&quot;) hist(x1, freq = FALSE, main = &quot;Histogram with Density Curve&quot;) lines(density(x1)) plot(ecdf(x1), main = &quot;Empiracle CDF&quot;) QQ plot indicates the data might be normal by remaining close to the line. The Box plot, histogram, and density curve all support this assumption. Formal tests all agree that the data are from the normal distribution. Shapiro Wilk is considered the best for test for testing normality. ## Are the data from a normal distribution? ## Shapiro-Wilk Test shapiro.test(x1) Shapiro-Wilk normality test data: x1 W = 0.95643, p-value = 0.4753 ## Anderson Darling Test ad.test(x1) Anderson-Darling normality test data: x1 A = 0.27523, p-value = 0.6216 ## Kolmogorov-Smirnoff Test ks.test(x1, &#39;pnorm&#39;) One-sample Kolmogorov-Smirnov test data: x1 D = 0.13528, p-value = 0.8109 alternative hypothesis: two-sided Chi-squared Distribution ## Draw some random data set.seed(10) x2 = rchisq(n = 20, 2) ## Estimate the DF parameter by maximum likelihood fitdistr(x = x2, dchisq, start = list(df = 2)) df 1.8140625 (0.3244201) ## Input the estimate from MLE ks.test(x = x2, y = pchisq, df = 1.8140625) One-sample Kolmogorov-Smirnov test data: x2 D = 0.15608, p-value = 0.6584 alternative hypothesis: two-sided ## Distribution plots par(mfrow = c(2, 2)) qqplot(qchisq(ppoints(20), df = 1.8140625), x2, main = &quot;QQ Plot&quot;) qqline(x2, distribution = function(p) qchisq(p, df = 1.8140625)) boxplot(x2, main = &quot;Boxplot&quot;) hist(x2, freq = FALSE, main = &quot;Histogram with Density Curve&quot;) lines(density(x2)) plot(ecdf(x2), main = &quot;Empiracle CDF&quot;) Calculating the MLE manually ## Generate data from the exponential distribution with mean = 1/5 set.seed(1000) X = rexp(n = 20, rate = 5) ## sample size and range of betas to test n = 20; beta = seq(.01, .5, by = .01) ## Liklihood function Likelihood = (1/beta)^n * exp(-1/beta * sum(X)) ## Maximum Likelihood (mle = max(Likelihood)) [1] 276565.8 (mle.beta = beta[which(Likelihood == mle)]) [1] 0.2 ## Statistical test for how well the specified distribution fits the data ks.test(x = X, y = &quot;pexp&quot;, rate = 1/mle.beta) One-sample Kolmogorov-Smirnov test data: X D = 0.1475, p-value = 0.7232 alternative hypothesis: two-sided par(mfrow = c(1, 2)) ## Plot the maximum likelihood plot(x = beta, y = Likelihood, type = &quot;l&quot;, main = &quot;Maximum Likelihood&quot;, lwd = 2) abline(h = mle, v = mle.beta, lty = 2) ## QQplot for assessing distribution fit visually qqplot(qexp(ppoints(10), rate = 1/mle.beta), X, xlab = &quot;QQ&quot;, main = &quot;QQ Plot&quot;) qqline(X, distribution = function(p) qexp(p, rate = 1/mle.beta)) Discrete Distributions Fitting a Binomial model Chi-squared goodness of fit test Producing a markdown table A rare but fatal disease of genetic origin occurring chiefly in infants and children is under investigation. An experiment was conducted on a 100 couples who are both carriers of the disease and have 5 children. A researcher recorded the number of children having the disease for each couple. library(knitr) ## Dataset (dta = data.frame( Diseased = c(0, 1, 2, 3, 4, 5), Count = c(21, 42, 24, 8, 4, 1) )) Diseased Count 1 0 21 2 1 42 3 2 24 4 3 8 5 4 4 6 5 1 ## Number of diseased children (d) and the total number of children (c) (d = sum(apply(X = dta, MARGIN = 1, FUN = function(p) dta$Diseased * dta$Count)[,1])) [1] 135 (c = 5 * sum(dta$Count)) [1] 500 ## MLE (mle = d / c) [1] 0.27 ## Calculate the expected probabilities and expected diseased children dta$Exp.Prob = round(dbinom(x = 0:5, size = 5, prob = mle), 4) dta$Exp.Diseased = with(dta, sum(Count) * Exp.Prob) dta Diseased Count Exp.Prob Exp.Diseased 1 0 21 0.2073 20.73 2 1 42 0.3834 38.34 3 2 24 0.2836 28.36 4 3 8 0.1049 10.49 5 4 4 0.0194 1.94 6 5 1 0.0014 0.14 ## Chi-square test requirements: ## 1) all Exp must be &gt; 1 ## 2) at most 20% of Exp may be less than 5 ## ## So we need to combine counts 4 and 5 and meet these requirements dta[5, 2:4] = dta[5, 2:4] + dta[6, 2:4] dta = dta[-6, ] dta$Diseased = as.character(dta$Diseased) dta$Diseased[5] = &#39;4 or 5&#39; ## Compute the Chi-Squared Statistic dta$X.2 = round(with(dta, (Count - Exp.Diseased)^2 / Exp.Diseased), 4) dta$Diseased = factor(dta$Diseased) dta Diseased Count Exp.Prob Exp.Diseased X.2 1 0 21 0.2073 20.73 0.0035 2 1 42 0.3834 38.34 0.3494 3 2 24 0.2836 28.36 0.6703 4 3 8 0.1049 10.49 0.5910 5 4 or 5 5 0.0208 2.08 4.0992 ## Compute the test statistic pvalue (TS = sum(dta$X.2)); 1 - pchisq(TS, df = 4) [1] 5.7134 [1] 0.2215985 ## Based on the table below we conclude the binomial model is a good fit for the data ## ## Assessment of Chi-squared GOF P-value ## • p-value &gt; .25 ⇒ Excellent fit ## • .15 ≤ p−value &lt; .25 ⇒ Good fit ## • .05 ≤ p−value &lt; .15 ⇒ Moderately Good fit ## • .01 ≤ p−value &lt; .05 ⇒ Poor fit ## • p-value &lt; .01 ⇒ Unacceptable fit ## ## Plot of the data vs model and create a markdown table from the data frame plot(x = dta$Diseased, y = NULL, xlab = &quot;Diseased Children per Couple&quot;, ylim = c(0, 50), ylab = &quot;Frequency&quot;, axes = FALSE, type = &quot;n&quot;, main = &quot;Binomial Model vs Data&quot;) axis(1, labels = dta$Diseased, at = dta$Diseased) axis(2, labels = seq(0, 50, 10), at = seq(0, 50, 10)) legend(&quot;topright&quot;, c(&quot;Data&quot;, &quot;Model&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 19, bty = &quot;n&quot;) points(x = dta$Diseased, y = dta$Count, col = &quot;blue&quot;, pch = 19, type = &quot;b&quot;) points(x = dta$Diseased, y = dta$Exp.Diseased, col = &quot;red&quot;, pch = 19, type = &quot;b&quot;) kable(dta) Diseased Count Exp.Prob Exp.Diseased X.2 0 21 0.2073 20.73 0.0035 1 42 0.3834 38.34 0.3494 2 24 0.2836 28.36 0.6703 3 8 0.1049 10.49 0.5910 4 or 5 5 0.0208 2.08 4.0992 ## Some Conclusions ## Since N is large we can use asymptotic confidence intervals ## A 95% confidence interval for whether a child will have the disease mle + c(-1, 1) * 1.96 * sqrt(.27 * (1 - .27))/sqrt(100) [1] 0.1829839 0.3570161 ## What is the probability that a couple will have at least 1 child with the disease? 1 - pbinom(q = 0, size = 5, prob = .27) [1] 0.7926928 Hypothesis Testing Shapiro Wilks One Sample T-Test Calculating Power Hypothesis Testing Sample Size Determination ## Data sample on chick weights data(chickwts) head(chickwts) weight feed 1 179 horsebean 2 160 horsebean 3 136 horsebean 4 227 horsebean 5 217 horsebean 6 168 horsebean ## Normal reference plot for height qqnorm(chickwts$weight) qqline(chickwts$weight) ## Are the data from a normal distribution? shapiro.test(chickwts$weight) Shapiro-Wilk normality test data: chickwts$weight W = 0.97674, p-value = 0.2101 mean(chickwts$weight) [1] 261.3099 How does the sample mean compare to a hypothesis test that the true mean is &lt; 260? What is the power of the test? \\[H_0: \\mu \\ge 260, H_a: \\mu \\lt 260\\] Population Fail to Reject Reject \\(H_0\\) \\(H_0\\) is True Correct Type I Error \\(H_a\\) is True Type II Error Correct ## What is the probability of a Type I error if we say the true mean is less than 250? t.test(chickwts$weight, mu = 250, alternative = &quot;less&quot;) One Sample t-test data: chickwts$weight t = 1.2206, df = 70, p-value = 0.8868 alternative hypothesis: true mean is less than 250 95 percent confidence interval: -Inf 276.7549 sample estimates: mean of x 261.3099 ## Verify the t statistic and p-value (ts = (mean(chickwts$weight) - 250) / (sd(chickwts$weight) / sqrt(length(chickwts$weight)))) [1] 1.220623 pt(ts, df = 70) [1] 0.8868377 ## What is the probability of a Type I error if we say the true mean is &gt; 245? t.test(chickwts$weight, mu = 245, alternative = &quot;greater&quot;) One Sample t-test data: chickwts$weight t = 1.7603, df = 70, p-value = 0.04137 alternative hypothesis: true mean is greater than 245 95 percent confidence interval: 245.8648 Inf sample estimates: mean of x 261.3099 ## Verify the t statistic and p-value (ts = (mean(chickwts$weight) - 245) / (sd(chickwts$weight) / sqrt(length(chickwts$weight)))) [1] 1.760251 1 - pt(ts, df = 70) [1] 0.04136678 ## We have rejected the null hypothesis and said under an alpha of .05 there is enough evidence ## to suppor that the true mean of Chick Weights is &gt; 245 ## What is the power of our test? power.t.test(n = length(chickwts$weight), delta = abs(mean(chickwts$weight) - 245), sd = sd(chickwts$weight), sig.level = .05, type = &quot;one.sample&quot;, alternative = &quot;one.sided&quot;, strict = TRUE) One-sample t test power calculation n = 71 delta = 16.30986 sd = 78.0737 sig.level = 0.05 power = 0.5391727 alternative = one.sided ## What sample size would we need to have a power of .8? power.t.test(delta = abs(mean(chickwts$weight) - 245), sd = sd(chickwts$weight), sig.level = .05, power = .8, type = &quot;one.sample&quot;, alternative = &quot;one.sided&quot;, strict = TRUE) One-sample t test power calculation n = 143.0323 delta = 16.30986 sd = 78.0737 sig.level = 0.05 power = 0.8 alternative = one.sided ## Verify manually (sd(chickwts$weight)^2 * (qnorm(p = .95) + qnorm(p = .8))^2) / abs(mean(chickwts$weight) - 245)^2 [1] 141.6698 Parameter Estimation Bootstrap Sampling ## Create a mixed Distribution set.seed(10) y1 = rnorm(10, 10, 2); y2 = rnorm(10, 15, 1); y3 = rnorm(10, 20, 2) Y = c(y1, y2, y3) par(mfrow = c(1, 2)) hist(Y) plot(density(Y)) ## Is Y part of a normal distribution? shapiro.test(Y) Shapiro-Wilk normality test data: Y W = 0.90712, p-value = 0.0126 ## Bootstrap simulation to estimate the mean X = c() ## Draw 10k random samples from Y and calculate the mean for (i in 1:10000) { x = sample(Y, length(Y), replace = TRUE) mu.x = mean(x) X = c(X, mu.x) } par(mfrow = c(1,1)) ## Draw a histogram of the bootstrap samples for the sample means hist(X, breaks = 50, main = &quot;Histogram of X_Bar&quot;) ## What is a 95% confidence interval for mu? sort(X)[c(250, 9750)] [1] 12.72430 15.58161 "],
["regression-methods.html", "Regression Methods", " Regression Methods "]
]
