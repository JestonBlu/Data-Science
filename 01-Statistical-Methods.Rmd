# Statistical Methods {-}

## Fitting Distributions {-}

### Continuous Distributions {-}
  * Assessing Distributions Visually
  * Formal Tests for Distribution Fit
  * Maximum Likelihood calculation

**Normal Distribution**

```{r a0, comment=NA, fig.width=8, fig.height=7, warning=FALSE, message=FALSE}
library(nortest)
library(MASS)

## Draw some random data
set.seed(10)
x1 = rnorm(20)

## Distribution plots
par(mfrow = c(2, 2))
qqnorm(x1)
qqline(x1)
boxplot(x1, main = "Boxplot")
hist(x1, freq = FALSE, main = "Histogram with Density Curve")
lines(density(x1))
plot(ecdf(x1), main = "Empiracle CDF")

```

QQ plot indicates the data might be normal by remaining close to the line. The Box plot, histogram, and density curve all support this assumption.

Formal tests all agree that the data are from the normal distribution. Shapiro Wilk is considered the best for test for testing normality.

```{r a1, comment=NA, warning=FALSE, message=FALSE}
## Are the data from a normal distribution?
## Shapiro-Wilk Test
shapiro.test(x1)
## Anderson Darling Test
ad.test(x1)
 ## Kolmogorov-Smirnoff Test
ks.test(x1, 'pnorm')

```

**Chi-squared Distribution**

```{r a2, comment=NA, warning=FALSE, message=FALSE, fig.width=8, fig.height=7}
## Draw some random data
set.seed(10)
x2 = rchisq(n = 20, 2)

## Estimate the DF parameter by maximum likelihood
fitdistr(x = x2, dchisq, start = list(df = 2))

## Input the estimate from MLE
ks.test(x = x2, y = pchisq, df = 1.8140625)

## Distribution plots
par(mfrow = c(2, 2))
qqplot(qchisq(ppoints(20), df = 1.8140625), x2, main = "QQ Plot")
qqline(x2, distribution = function(p) qchisq(p, df = 1.8140625))
boxplot(x2, main = "Boxplot")
hist(x2, freq = FALSE, main = "Histogram with Density Curve")
lines(density(x2))
plot(ecdf(x2), main = "Empiracle CDF")

```

**Calculating the MLE manually**

```{r a3, comment=NA, warning=FALSE, message=FALSE, fig.width=8, fig.height=5}

## Generate data from the exponential distribution with mean = 1/5
set.seed(1000)
X = rexp(n = 20, rate = 5)

## sample size and range of betas to test
n = 20; beta = seq(.01, .5, by = .01)

## Liklihood function
Likelihood = (1/beta)^n * exp(-1/beta * sum(X))

## Maximum Likelihood
(mle = max(Likelihood))
(mle.beta = beta[which(Likelihood == mle)])

## Statistical test for how well the specified distribution fits the data
ks.test(x = X, y = "pexp", rate = 1/mle.beta)

par(mfrow = c(1, 2))

## Plot the maximum likelihood
plot(x = beta, y = Likelihood, type = "l", main = "Maximum Likelihood", lwd = 2)
abline(h = mle, v = mle.beta, lty = 2)

## QQplot for assessing distribution fit visually
qqplot(qexp(ppoints(10), rate = 1/mle.beta), X, xlab = "QQ", main = "QQ Plot")
qqline(X, distribution = function(p) qexp(p, rate = 1/mle.beta))

```

---

### Discrete Distributions {-}
*  Fitting a Binomial model
*  Chi-squared goodness of fit test
*  Producing a markdown table

A rare but fatal disease of genetic origin occurring chiefly in infants and children is under investigation. An experiment was conducted on a 100 couples who are both carriers of the disease and have 5 children. A researcher recorded the number of children having the disease for each couple.

```{r a4, comment=NA, fig.width=8, fig.height=6, warning=FALSE, message=FALSE}
library(knitr)

## Dataset
(dta = data.frame(
  Diseased = c(0, 1, 2, 3, 4, 5),
  Count = c(21, 42, 24, 8, 4, 1)
))

## Number of diseased children (d) and the total number of children (c)
(d = sum(apply(X = dta, MARGIN = 1, FUN = function(p) dta$Diseased * dta$Count)[,1]))
(c = 5 * sum(dta$Count))

## MLE
(mle = d / c)

## Calculate the expected probabilities and expected diseased children
dta$Exp.Prob = round(dbinom(x = 0:5, size = 5, prob = mle), 4)
dta$Exp.Diseased = with(dta, sum(Count) * Exp.Prob)

dta

## Chi-square test requirements:
##  1) all Exp must be > 1
##  2) at most 20% of Exp may be less than 5
##
## So we need to combine counts 4 and 5 and meet these requirements

dta[5, 2:4] = dta[5, 2:4] + dta[6, 2:4]
dta = dta[-6, ]
dta$Diseased = as.character(dta$Diseased)
dta$Diseased[5] = '4 or 5'

## Compute the Chi-Squared Statistic
dta$X.2 = round(with(dta, (Count - Exp.Diseased)^2 / Exp.Diseased), 4)
dta$Diseased = factor(dta$Diseased)
dta

## Compute the test statistic pvalue
(TS = sum(dta$X.2)); 1 - pchisq(TS, df = 4)

## Based on the table below we conclude the binomial model is a good fit for the data
##
## Assessment of Chi-squared GOF P-value
##  •       p-value > .25 ⇒ Excellent fit
##  • .15 ≤ p−value < .25 ⇒ Good fit
##  • .05 ≤ p−value < .15 ⇒ Moderately Good fit
##  • .01 ≤ p−value < .05 ⇒ Poor fit
##  •       p-value < .01 ⇒ Unacceptable fit
##
## Plot of the data vs model and create a markdown table from the data frame
plot(x = dta$Diseased, y = NULL, xlab = "Diseased Children per Couple",
     ylim = c(0, 50), ylab = "Frequency",
     axes = FALSE, type = "n", main = "Binomial Model vs Data")
axis(1, labels = dta$Diseased, at = dta$Diseased)
axis(2, labels = seq(0, 50, 10), at = seq(0, 50, 10))
legend("topright", c("Data", "Model"), col = c("blue", "red"), pch = 19, bty = "n")
points(x = dta$Diseased, y = dta$Count, col = "blue", pch = 19, type = "b")
points(x = dta$Diseased, y = dta$Exp.Diseased, col = "red", pch = 19, type = "b")

kable(dta)

## Some Conclusions
## Since N is large we can use asymptotic confidence intervals
## A 95% confidence interval for whether a child will have the disease
mle + c(-1, 1) * 1.96 * sqrt(.27 * (1 - .27))/sqrt(100)

## What is the probability that a couple will have at least 1 child with the disease?
1 - pbinom(q = 0, size = 5, prob = .27)

```

## Hypothesis Testing {-}
  * Shapiro Wilks
  * One Sample T-Test
  * Calculating Power
  * Hypothesis Testing
  * Sample Size Determination


```{r a5, comment=NA, fig.width=8, fig.height=5}

## Data sample on chick weights
data(chickwts)
head(chickwts)

## Normal reference plot for height
qqnorm(chickwts$weight)
qqline(chickwts$weight)

## Are the data from a normal distribution?
shapiro.test(chickwts$weight)
mean(chickwts$weight)

```

How does the sample mean compare to a hypothesis test that the true mean is < 260? What is the power of the test?

$$H_0: \mu \ge 260, H_a: \mu \lt 260$$

|  Population   | Fail to Reject | Reject $H_0$ |
|:-------------:|:--------------:|:------------:|
| $H_0$ is True |    Correct     | Type I Error |
| $H_a$ is True | Type II Error  |   Correct    |

```{r a6, comment=NA, warning=FALSE, message=FALSE}

## What is the probability of a Type I error if we say the true mean is less than 250?
t.test(chickwts$weight, mu = 250, alternative = "less")

## Verify the t statistic and p-value
(ts = (mean(chickwts$weight) - 250) / (sd(chickwts$weight) / sqrt(length(chickwts$weight))))
pt(ts, df = 70)

## What is the probability of a Type I error if we say the true mean is > 245?
t.test(chickwts$weight, mu = 245, alternative = "greater")

## Verify the t statistic and p-value
(ts = (mean(chickwts$weight) - 245) / (sd(chickwts$weight) / sqrt(length(chickwts$weight))))
1 - pt(ts, df = 70)

## We have rejected the null hypothesis and said under an alpha of .05 there is enough evidence
## to suppor that the true mean of Chick Weights is > 245

## What is the power of our test?
power.t.test(n = length(chickwts$weight),
             delta = abs(mean(chickwts$weight) - 245),
             sd = sd(chickwts$weight),
             sig.level = .05,
             type = "one.sample",
             alternative = "one.sided", strict = TRUE)

## What sample size would we need to have a power of .8?
power.t.test(delta = abs(mean(chickwts$weight) - 245),
             sd = sd(chickwts$weight),
             sig.level = .05,
             power = .8,
             type = "one.sample",
             alternative = "one.sided", strict = TRUE)

## Verify manually
(sd(chickwts$weight)^2 * (qnorm(p = .95) + qnorm(p = .8))^2) / abs(mean(chickwts$weight) - 245)^2
```


## Parameter Estimation {-}
  * Bootstrap Sampling


```{r a7, warning=FALSE, message=FALSE, comment=NA, fig.width=7, fig.height=4}

## Create a mixed Distribution
set.seed(10)
y1 = rnorm(10, 10, 2); y2 = rnorm(10, 15, 1); y3 = rnorm(10, 20, 2)
Y = c(y1, y2, y3)

par(mfrow = c(1, 2))
hist(Y)
plot(density(Y))

## Is Y part of a normal distribution?
shapiro.test(Y)

## Bootstrap simulation to estimate the mean
X = c()

## Draw 10k random samples from Y and calculate the mean
for (i in 1:10000) {
  x = sample(Y, length(Y), replace = TRUE)
  mu.x = mean(x)
  X = c(X, mu.x)
}

par(mfrow = c(1,1))
## Draw a histogram of the bootstrap samples for the sample means
hist(X, breaks = 50, main = "Histogram of X_Bar")

## What is a 95% confidence interval for mu?
sort(X)[c(250, 9750)]

```


## Sample Size and Power {-}

---

### Proportions {-}

```{r a8, comment=NA}

library(pwr)

## Univariate Proportion
pwr.p.test(h = .25, sig.level = .05, power = .8, alternative = "greater")

## Two Proportions (equal n)
pwr.2p.test(h = .25, sig.level = .05, power = .8, alternative = "greater")

## Two Proportions (different n)
pwr.2p2n.test(h = .25, n1 = 100, n2 = 120, sig.level = .05, alternative = "greater")
```

### T-test {-}

```{r a9, comment=NA}
## Equal n
pwr.t.test(d = .5, sig.level = .05, power = .8, type = "one.sample")
pwr.t.test(d = .5, sig.level = .05, power = .8, type = "two.sample")
pwr.t.test(d = .5, sig.level = .05, power = .8, type = "paired")

## Different n
pwr.t2n.test(n1 = 10, n2 = 15, d = 1, sig.level = .05)
```

### Chi-square {-}

```{r a10, comment=NA}
pwr.chisq.test(w = .25, df = 4, sig.level = .05, power = .8)
```

### ANOVA {-}

```{r a11, comment=NA}
pwr.anova.test(k = 5, n = 10, f = .5, sig.level = .05)
```

## **Survival Analysis** {-}


```{r a12, comment=NA, fig.align='center'}

library(survival)

summary(lung)

mdl = survfit(Surv(time, status) ~ 1, data = lung)

plot(mdl, conf.int = FALSE,
     main="Kaplan-Meier Estimator of Survival Function",
     xlab="Survival Time",
     ylab="Survival Function")

summary(mdl)

mdl = survfit(Surv(time, status) ~ strata(sex), data = lung)

summary(mdl)

plot(mdl, conf.int = FALSE, col = c(2,4),
     main="Survival Function",
     xlab="Survival Time",
     ylab="Survival Function")

legend(800, 1, c("men", "women"), lty = c(1, 1), col = c(2, 4))


summary(survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), lung))
```

## Experimental Design {-}

### Completely Random Design {-}

The response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).

- Treatment Structure: 2 x 3 Factorial Treatment, both Fixed

- Model: $y_{ijk} = \mu + \alpha_i + \beta_j + \alpha \beta_{ij} + e_{ijk}$
- Treatments: $\alpha_i = \text{supp, } \beta_j = \text{dose}$
- Fixed Effects: $\alpha_1 = \beta_1 = \alpha \beta_{1j} = \alpha \beta_{i1} = 0$
- Random Effects: $e_{ijk} = N(0, \sigma^2_e)$

```{r a13, comment=NA, fig.width=8, fig.height=5, warning=FALSE, message=FALSE}
library(lsmeans)
library(reshape2)
library(car)
library(plyr)

data("ToothGrowth")

## Data is numeric, but we need to force it to be a factor for the model
ToothGrowth$dose.factor = as.factor(ToothGrowth$dose)
summary(ToothGrowth)

## Even observations per treatment group
table(ToothGrowth$supp, ToothGrowth$dose)

## ANOVA model
mdl = lm(len ~ supp * dose.factor, data = ToothGrowth)
anova(mdl)

## Least Squares Means
lsmeans(mdl, specs = c("supp", "dose.factor"))

## Differences in Means
TukeyHSD(aov(mdl))

```

Are the necessary conditions for hypothesis testing present?

- Normality: Residuals appear normally distributed per the residual normal reference plot and shapiro-wilks test
- Equal Variance: Brown-Forsythe-Levene test and residual plot supports equal variance
- Independence: No correlation in the residuals per the Durbin Watson test and plots of variables against residuals

Conditions for hypothesis testing appears to be satisfied

```{r a14, comment=NA, fig.width=8, fig.height=5, warning=FALSE, message=FALSE}

## Normality of Residuals
shapiro.test(mdl$residuals); qqnorm(mdl$residuals); qqline(mdl$residuals)

## Equal Variance of Residuals
leveneTest(mdl)
plot(x = mdl$fitted.values, y = (mdl$residual - mean(mdl$residuals))/sd(mdl$residuals),
     xlab = "Fitted Values", ylab = "Standardized Residuals",
     main = "Equal Variance Residual Plot")

## Independence of Residuals
par(mfrow = c(1, 2))
plot(ToothGrowth$dose, mdl$residuals, xlab = "Dose", ylab = "Residuals")
plot(ToothGrowth$supp, mdl$residuals, xlab = "Supp", ylab = "Residuals")

## Test for correlation in the residuals
durbinWatsonTest(mdl)

```

- Group doses so that each dose is not statistically different than any other dose in the group:
    + The interaction between dose and supp are significant so we need to assess the differences in dose per each level of supp.
    + OJ: {.5}, {1, 2}
    + VC: {.5}, {1}, {2}
- Group supps so that each supp is not statistically different than any other supp in the group:
    + The interaction is significant so we need to assess the supps at each level of dose
    + .5: {OJ}, {VC}
    + 1: {OJ}, {VC}
    + 2: {OJ, VC}


```{r a15, comment=NA, fig.width=8, fig.height=5, echo=FALSE}

## Calculate the mean and sd for each combination of supp and dose
x = ddply(ToothGrowth, .(supp, dose), summarize, mean = mean(len), sd = sd(len))

## add 95% confidence intervals
x$lwr = with(x, mean - qt(p = .975, df = 9) * sd/sqrt(10))
x$upr = with(x, mean + qt(p = .975, df = 9) * sd/sqrt(10))

x1 = subset(x, supp == "OJ")
x2 = subset(x, supp == "VC")

## Make a blank plot
plot(x = ToothGrowth$dose,
     y = ToothGrowth$len,
     xlab = "Dose",
     ylab = "Tooth Length",
     type = "n",
     ylim = c(6, 29),
     main = "Interaction Plot")

## Add points for the means
points(x = x1$dose, y = x1$mean, pch = 3, lwd = 4, col = "blue", cex = 1.1)
points(x = x2$dose, y = x2$mean, pch = 4, lwd = 4, col = "red", cex = 1.1)

## Add lines for interaction
lines(x = x1$dose, y = x1$mean, lty = 2)
lines(x = x2$dose, y = x2$mean, lty = 2)

## Add segments for confidence intervals
for (i in 1:6) {
  segments(x0 = x$dose[i], y0 = x$lwr[i], y1 = x$upr[i])
  segments(x0 = x$dose[i]-.05, x1 = x$dose[i]+.05, y0 = x$lwr[i])
  segments(x0 = x$dose[i]-.05, x1 = x$dose[i]+.05, y0 = x$upr[i])
}

## Add legend
legend("bottomright", c("Orange Juice", "Vitamin C"),
       pch = c(3, 4), bty = "n", cex = 1.1, col = c("blue", "red"))

```


### Random Complete Block Design {-}

An experiment was conducted to compare four different pre-planting treatments for soybeen seeds. A fifth treatment, consisting of not treating the seeds was used as a control. The experimental area consisted of four fields. There are notable differences in the fields. Each field was divided into five plots and one of the treatments was randomly assigned to a plot within each field.

- Treatment Structure: 1 Single Treatment with 5 levels
- Response: The number of plants that failed to emerge out of 100 seeds planted per plot.

- Model: $y_{ij} = \mu + \alpha_i + \beta_j + e_{ij}$
- Treatments: $\alpha_i = \text{Seed, } \beta_j = \text{Field, } \alpha_5 = \text{Control}$
- Fixed Effects: $\alpha_5 = \beta_1 = 0$
- Random Effects: $e_{ij} = N(0, \sigma^2_e)$

---

```{r a16, echo=FALSE, results='asis'}
library(knitr)
library(reshape2)
library(pander)

soy = data.frame(
  Treatment = c("Avasan", "Spergon", "Semaesan", "Fermate", "Control"),
  Field.1 = c(2, 4, 3, 9, 8),
  Field.2 = c(5, 10, 6, 3, 11),
  Field.3 = c(7, 9, 9, 5, 12),
  Field.4 = c(11, 8, 10, 5, 13)
)

pandoc.table(soy, caption = "Soy Bean Data")

soy = melt(soy, id.vars = "Treatment", variable.name = "Field", value.name = "Count")

```


```{r a17, comment=NA}
## Make the control treatment the default level
soy$Treatment = relevel(soy$Treatment, ref = "Control")

## We only have one rep per treatment so there are not enough DF to measure the interaction
mdl = lm(Count ~ Field + Treatment, data = soy)
anova(mdl)

```

---

#### Comparison of means vs control

Since we have a control variable we want to know if any of the treatment means are significantly lower than the control mean.

```{r a18, comment=NA, message=FALSE, warning=FALSE}
library(multcomp)

Dunnet = glht(mdl, linfct = mcp(Treatment = "Dunnet"), alternative = "less")
summary(Dunnet)

```

We have significant evidence that only Avasan and Fermate are significantly lower than the control. Are they significantly different from each other?

```{r a19, comment=NA, message=FALSE, warning=FALSE}

TukeyHSD(aov(mdl))

```

There is not significant evidence between the difference in means between any of the treatments.

## Contingency Tables {-}

```{r a20, comment=NA, echo=FALSE, results='asis'}
library(knitr)

dta = data.frame(
  Department = c(1, 2, 3, 4 ,5, 6),
  Male.Yes = c(512, 353, 120, 138, 53, 22),
  Male.No = c(313, 207, 205, 279, 138, 351),
  Female.Yes = c(89, 17, 202, 131, 94, 24),
  Female.No = c(19, 8, 391, 244, 299, 317)
)

kable(dta, caption = "Data for applicant entrance for 6 departments")

```

```{r a21, comment=NA, results='asis'}
dta$OR = with(dta, (Male.Yes * Female.No) / (Male.No * Female.Yes))
dta$se = with(dta, (sqrt(1/Male.Yes + 1/Female.No + 1/Male.No + 1/Female.Yes)))
dta$Conf.lwr = with(dta, OR - (1.96 * se))
dta$Conf.Upr = with(dta, OR + (1.96 * se))

kable(dta, split.tables = Inf)

```

```{r a22, comment=NA}

marginal = colSums(dta[, 2:5])
OR = (marginal[1] * marginal[4]) / (marginal[2] * marginal[3])
se = sqrt(1/marginal[1] + 1/marginal[4] + 1/marginal[2] + 1/marginal[3])

## Confidence Interval for the Marginal OR
OR + c(-1, 1) * 1.96 * se

```

```{r a23, comment=NA, warning=FALSE, message=FALSE}
library(DescTools)
library(lawstat)

dta = xtabs(freq ~ .,
            cbind(expand.grid(Gender = c("Male", "Female"),
                              Entrace = c("Yes", "No"),
                              Department = c("1", "2", "3", "4", "5", "6")),
                  freq =  c(512, 89, 313, 19, 353, 17, 207, 8, 120, 202, 205, 391,
                            138, 131, 279, 244, 53, 94, 138, 299, 22, 24, 351, 317)
                  )
            )

## Ho: OR = 1, Ha: OR > 1
BreslowDayTest(dta, OR = 1)

## Ho: OR_1 = OR_2 = OR_3 = OR_4 = OR_5 = OR_6, Ha: At least one set of OR are not equal
cmh.test(dta)

```

Based on the Breslow Day test we reject the null hypothesis that the odds ratios are equal to 1. The CMH test fails to reject that gender and entrance are independent.


## Principal Components {-}

---

```{r a24, comment=NA, warning=FALSE, message=FALSE, fig.width=8, fig.height=5}

## Generate some data
library(mvtnorm)

mu = rep(10, 6)
cov = matrix(nrow = 6, byrow = TRUE,
  c(1, 0, 1, 0, 1, 1,
    0, 2, 1, 2, 1, 1,
    1, 1, 2, 1, 1, 0,
    0, 2, 1, 3, 2, 1,
    1, 1, 1, 2, 4, 1,
    1, 1, 0, 1, 1, 10)
  )

set.seed(1000)
X = rmvnorm(100, mu, cov)

colnames(X) = paste("x", 1:6, sep = "")

## How do the data correlate?
round(cor(X), 3)
pairs(X)

summary(X)

## Build the principal components. We do not need to standardize the data
## since all of the variables are roughly the same scale.
(pr = prcomp(X))

## Principal components rotation matrix is actually the same as the eigen vectors
eigen(cov(X))$vectors

## Build the principal components from the coefficients
X.pca = X %*% pr$rotation

## The variance of the principal components is equal to the eigen values
eigen(cov(X))$values
diag(var(X.pca))

## PCA Summary
## The first two PCA account for 80% of all variation in the data
summary(pr)

X.pca = data.frame(X.pca)

## plot of the first two PCA
plot(X.pca$PC1, X.pca$PC2)

```

## Eigen Values and Statistical Distance {-}

```{r a25, warning=FALSE, message=FALSE, comment=NA}
## Eigen values and vectors are used to describe a positie definate covariance matrix.
(Cov = matrix(c(1, 0, 0, 2), nrow = 2))
eig = eigen(Cov)

## eigen values
(lambda = eig$values)

## eigen vectors
(ee = eig$vectors)

## Spectural decompsotion allows you to reconstruct a matrix using only the eigen
## values and vectors
lambda[1] * ee[,1] %*% t(ee[,1]) + lambda[2] * ee[,2] %*% t(ee[,2])

## Straight line distance (Euclidean) vs Statistical Distance
## Straight line distance to the origin using point(1, 1)
sqrt((1 - 0)^2 + (1 - 0)^2)

# Statistical Distance to the origin
sqrt((1 -  0)^2/1 + (1 - 0)^2/5)

## Generating Multivariate Normal Data
library(mvtnorm)

## set up parameters, 2 means use the covariance matrix from earlier
mu = c(10, 20)

## generate a large dataset
set.seed(1000)
X = rmvnorm(1000, mu, Cov)

## Correlation of X, should be close to 0
cor(X)

## Calculate the distance between each point and the means
distance = c()
for (i in 1:nrow(X)) {
  x = t(X[i, ] - colMeans(X)) %*% solve(cov(X)) %*% (X[i, ] - colMeans(X))
  distance = c(distance, x)
}
## distances
head(distance)

## What is the distance that captures 95% of all points generated from the distribution?
(critical.value = qchisq(.95, 2))

## What is the proportion of points that fall within this distance?
## As n increases the proportion should converge on 5%
length(which((distance - critical.value) > 0))/length(distance)


## plot a 95% confidence ellipse for the generated data
library(plotrix)
plot(x = c(7,13), y = c(15,24), type = "n",
  xlab = expression(x[1]), ylab = expression(x[2]),
  main = "95% Confidence Ellipsoid")
points(X[, 1], X[, 2])
abline(h = 20, v = 10, lty = 2, lwd = 2, col = "red")
draw.ellipse(10, 20,
  sqrt(critical.value * lambda[1]),
  sqrt(critical.value * lambda[2]),
  ## convert radians to degrees
  angle = acos(abs(ee[1,1])) * 57.2957795,
  border = 1, lwd = 2)

```
